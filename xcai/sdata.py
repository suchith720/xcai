# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/35_sdata.ipynb.

# %% auto 0
__all__ = ['identity_collate_fn', 'SMainXCDataset', 'SMetaXCDataset', 'SXCDataset', 'SBaseXCDataBlock', 'SXCDataBlock']

# %% ../nbs/35_sdata.ipynb 3
import torch, inspect, numpy as np
from typing import Callable, Optional, Union
from torch.utils.data import DataLoader
from transformers import BatchEncoding

from .data import MainXCData, MetaXCData
from .data import BaseXCDataset, MainXCDataset, MetaXCDataset, MetaXCDatasets
from .data import BaseXCDataBlock

from fastcore.utils import *
from fastcore.meta import *
from fastcore.dispatch import *

# %% ../nbs/35_sdata.ipynb 9
def identity_collate_fn(batch): return BatchEncoding(batch)

# %% ../nbs/35_sdata.ipynb 12
class SMainXCDataset(MainXCDataset):

    def __init__(
        self,
        n_slbl_samples:Optional[int]=1,
        main_oversample:Optional[bool]=False,
        **kwargs
    ):
        super().__init__(**kwargs)
        store_attr('n_slbl_samples,main_oversample')

    def __getitems__(self, idxs:List):
        x = {'data_idx': torch.tensor(idxs, dtype=torch.int64)}
        x.update(self.get_info('data', idxs, self.data_info, self.data_info_keys))
        if self.n_lbl is not None:
            prefix = 'lbl2data'
            o = self.extract_items(prefix, self.curr_data_lbl, idxs, self.n_lbl_samples, self.n_slbl_samples, self.main_oversample, 
                                   self.lbl_info, self.lbl_info_keys)
            x.update(o)
        return x

    @classmethod
    @delegates(MainXCData.from_file)
    def from_file(
        cls, 
        n_lbl_samples:Optional[int]=None,
        data_info_keys:Optional[List]=None,
        lbl_info_keys:Optional[List]=None,
        n_slbl_samples:Optional[int]=1,
        main_oversample:Optional[bool]=False,
        **kwargs
    ):
        return cls(**MainXCData.from_file(**kwargs), n_lbl_samples=n_lbl_samples, data_info_keys=data_info_keys, 
                   lbl_info_keys=lbl_info_keys, n_slbl_samples=n_slbl_samples, main_oversample=main_oversample)

    def _getitems(cls, idxs:List):
        return SMainXCDataset(
            {k:[v[idx] for idx in idxs] for k,v in cls.data_info.items()}, 
            data_lbl=cls.data_lbl[idxs] if cls.data_lbl is not None else None, 
            lbl_info=cls.lbl_info, 
            data_lbl_filterer=Filterer.sample(cls.data_lbl_filterer, sz=cls.data_lbl.shape, idx=idxs) if cls.data_lbl_filterer is not None else None,
            n_lbl_samples=cls.n_lbl_samples,
            data_info_keys=cls.data_info_keys,
            lbl_info_keys=cls.lbl_info_keys,
            n_slbl_samples=cls.n_slbl_samples,
            main_oversample=cls.main_oversample,
        )
    

# %% ../nbs/35_sdata.ipynb 24
class SMetaXCDataset(MetaXCDataset):

    def __init__(
        self,
        n_sdata_meta_samples:Optional[int]=1,
        n_slbl_meta_samples:Optional[int]=1,
        meta_oversample:Optional[bool]=False,
        **kwargs
    ):
        super().__init__(**kwargs)
        store_attr('n_sdata_meta_samples,n_slbl_meta_samples,meta_oversample')

    def _getitems(self, idxs:List):
        return SMetaXCDataset(prefix=self.prefix, data_meta=self.data_meta[idxs], lbl_meta=self.lbl_meta, meta_info=self.meta_info, 
                              n_data_meta_samples=self.n_data_meta_samples, n_lbl_meta_samples=self.n_lbl_meta_samples, 
                              meta_info_keys=self.meta_info_keys, n_sdata_meta_samples=self.n_sdata_meta_samples, 
                              n_slbl_meta_samples=self.n_slbl_meta_samples, meta_oversample=self.meta_oversample)

    def _sample_meta_items(self, idxs:List):
        assert max(idxs) < self.n_meta, f"indices should be less than {self.n_meta}"
        meta_info = {k: [v[i] for i in idxs] for k,v in self.meta_info.items()}
        return SMetaXCDataset(prefix=self.prefix, data_meta=self.data_meta[:, idxs], lbl_meta=self.lbl_meta[:, idxs], meta_info=meta_info, 
                              n_data_meta_samples=self.n_data_meta_samples, n_lbl_meta_samples=self.n_lbl_meta_samples,
                              meta_info_keys=self.meta_info_keys, n_sdata_meta_samples=self.n_sdata_meta_samples, 
                              n_slbl_meta_samples=self.n_slbl_meta_samples, meta_oversample=self.meta_oversample)
        
    @classmethod
    @delegates(MetaXCData.from_file)
    def from_file(
        cls, 
        n_data_meta_samples:Optional[int]=None, 
        n_lbl_meta_samples:Optional[int]=None, 
        meta_info_keys:Optional[List]=None,
        n_sdata_meta_samples:Optional[int]=1,
        n_slbl_meta_samples:Optional[int]=1,
        meta_oversample:Optional[bool]=False,
        **kwargs
    ):
        return cls(**MetaXCData.from_file(**kwargs), n_data_meta_samples=n_data_meta_samples, n_lbl_meta_samples=n_lbl_meta_samples, 
                   meta_info_keys=meta_info_keys, n_sdata_meta_samples=n_sdata_meta_samples, n_slbl_meta_samples=n_slbl_meta_samples,
                   meta_oversample=meta_oversample)

    def get_data_meta(self, idxs:List):
        x, prefix = dict(), f'{self.prefix}2data'
        o = self.extract_items(prefix, self.curr_data_meta, idxs, self.n_data_meta_samples, self.n_sdata_meta_samples, 
                               self.meta_oversample, self.meta_info, self.meta_info_keys)
        x.update(o)
        return x
        
    def get_lbl_meta(self, idxs:List):
        x, prefix = dict(), f'{self.prefix}2lbl'
        o = self.extract_items(prefix, self.curr_lbl_meta, idxs, self.n_lbl_meta_samples, self.n_slbl_meta_samples, 
                               self.meta_oversample, self.meta_info, self.meta_info_keys)
        x.update(o)
        return x

    def _verify_inputs(cls):
        cls.n_data,cls.n_meta = cls.data_meta.shape[0],cls.data_meta.shape[1]
        
        if cls.lbl_meta is not None:
            cls.n_lbl = cls.lbl_meta.shape[0]
            if cls.lbl_meta.shape[1] != cls.n_meta:
                raise ValueError(f'`lbl_meta`({cls.lbl_meta.shape[1]}) should have same number of columns as `data_meta`({cls.n_meta}).')
    
        if cls.meta_info is not None:
            n_meta = cls._verify_info(cls.meta_info)
            if n_meta != cls.n_meta:
                raise ValueError(f'`meta_info`({n_meta}) should have same number of entries as number of columns of `data_meta`({cls.n_meta})')
            if cls.meta_info_keys is None: cls.meta_info_keys = list(cls.meta_info.keys())
        

# %% ../nbs/35_sdata.ipynb 31
class SXCDataset(BaseXCDataset):

    def __init__(self, data:SMainXCDataset, **kwargs):
        super().__init__()
        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], SMetaXCDataset)})
        self._verify_inputs()

    def _getitems(self, idxs:List):
        return SXCDataset(self.data._getitems(idxs), **{k:meta._getitems(idxs) for k,meta in self.meta.items()})
        
    @classmethod
    @delegates(SMainXCDataset.from_file)
    def from_file(cls, **kwargs):
        data = SMainXCDataset.from_file(**kwargs)
        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}
        meta = {k:SMetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}
        return cls(data, **meta)

    @staticmethod
    def get_meta_args(**kwargs):
        return [k for k in kwargs if re.match(r'.*_meta$', k)]

    def _verify_inputs(self):
        self.n_data, self.n_lbl = self.data.n_data, self.data.n_lbl
        if len(self.meta):
            self.n_meta = len(self.meta)
            for meta in self.meta.values():
                if meta.n_data != self.n_data: 
                    raise ValueError(f'`meta`({meta.n_data}) and `data`({self.n_data}) should have the same number of datapoints.')
                if self.n_lbl is not None and meta.n_lbl != self.n_lbl: 
                    raise ValueError(f'`meta`({meta.n_lbl}) and `data`({self.n_lbl}) should have the same number of labels.')

    def __getitems__(self, idxs:List):
        x = self.data.__getitems__(idxs)
        if self.n_meta:
            for meta in self.meta.values():
                x.update(meta.get_data_meta(idxs))
                if self.n_lbl:
                    z = meta.get_lbl_meta(x['lbl2data_idx'])
                    z[f'{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])
                    z[f'p{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])
                    x.update(z)
        return x

    @property
    def lbl_info(self): return self.data.lbl_info

    @property
    def lbl_dset(self): return SMainXCDataset(data_info=self.data.lbl_info)

    @property
    def data_info(self): return self.data.data_info

    @property
    def data_dset(self): return SMainXCDataset(data_info=self.data.data_info) 

    def one_batch(self, bsz:Optional[int]=10, seed:Optional[int]=None):
        if seed is not None: torch.manual_seed(seed)
        idxs = list(torch.randperm(len(self)).numpy())[:bsz]
        return [self[idx] for idx in idxs]
       

# %% ../nbs/35_sdata.ipynb 39
class SBaseXCDataBlock(BaseXCDataBlock):

    @delegates(DataLoader.__init__)
    def __init__(
        self, 
        dset:SXCDataset, 
        collate_fn:Optional[Callable]=identity_collate_fn,
        **kwargs
    ):
        self.dset, self.dl_kwargs, self.collate_fn = dset, self._get_dl_kwargs(**kwargs), collate_fn
        self.dl = DataLoader(dset, collate_fn=collate_fn, **self.dl_kwargs) if collate_fn is not None else None

    def _get_dl_kwargs(self, **kwargs):
        dl_params = inspect.signature(DataLoader.__init__).parameters
        return {k:v for k,v in kwargs.items() if k in dl_params}

    @classmethod
    @delegates(SXCDataset.from_file)
    def from_file(cls, collate_fn:Callable=identity_collate_fn, **kwargs):
        return BaseXCDataBlock(SXCDataset.from_file(**kwargs), collate_fn, **kwargs)

    def __len__(self):
        return len(self.dset)    
    
    def _getitems(self, idxs:List):
        return SBaseXCDataBlock(self.dset._getitems(idxs), collate_fn=self.collate_fn, **self.dl_kwargs)

    @property
    def bsz(self): return self.dl.batch_size

    @bsz.setter
    def bsz(self, v):
        self.dl_kwargs['batch_size'] = v
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None

    @property
    def data_lbl_filterer(self): return self.dset.data.data_lbl_filterer

    @data_lbl_filterer.setter
    def data_lbl_filterer(self, val): self.dset.data.data_lbl_filterer = val

    @typedispatch
    def one_batch(self):
        return next(iter(self.dl))

    @typedispatch
    def one_batch(self, bsz:int):
        self.dl_kwargs['batch_size'] = bsz
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None
        return next(iter(self.dl))
        
    def filterer(cls, train:'SBaseXCDataBlock', valid:'SBaseXCDataBlock', fld:Optional[str]='identifier'):
        train_info, valid_info, lbl_info = train.dset.data.data_info, valid.dset.data.data_info, train.dset.data.lbl_info
        if fld not in train_info: raise ValueError(f'`{fld}` not in `data_info`')
            
        train.data_lbl_filterer, valid_filterer = Filterer.generate(train_info[fld], valid_info[fld], lbl_info[fld], 
                                                                    train.dset.data.data_lbl, valid.dset.data.data_lbl)
        _, valid_filterer, idx = Filterer.prune(valid.dset.data.data_lbl, valid_filterer)
        
        valid = valid._getitems(idx)
        valid.data_lbl_filterer = valid_filterer
        
        return train, valid
        
    def splitter(cls, valid_pct:Optional[float]=0.2, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(len(cls)).numpy())
        cut = int(valid_pct * len(cls))
        train, valid = cls._getitems(rnd_idx[cut:]), cls._getitems(rnd_idx[:cut])
        if cls.data_lbl_filterer is None: return train, valid
        else: return cls.filterer(train, valid)

    def sample(cls, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(len(cls)).numpy())
        cut = int(pct * len(cls)) if n is None else max(1, n)
        return cls._getitems(rnd_idx[:cut])
        

# %% ../nbs/35_sdata.ipynb 43
class SXCDataBlock:

    def __init__(self, train:SBaseXCDataBlock=None, valid:SBaseXCDataBlock=None, test:SBaseXCDataBlock=None):
        self.train, self.valid, self.test = train, valid, test

    @staticmethod
    def load_cfg(fname):
        with open(fname, 'r') as f: return json.load(f)

    def sample_info(self, info, idx):
        return {k: v[idx] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[i] for i in idx] for k,v in info.items()}

    def linker_dset(self, meta_name:str):
        if meta_name not in self.train.dset.meta:
            raise ValueError(f'Invalid metadata: {meta_name}')

        train_meta = self.train.dset.meta[meta_name].data_meta
        train_idx = np.where(train_meta.getnnz(axis=1) > 0)[0]
        meta_idx = np.where(train_meta.getnnz(axis=0) > 0)[0]
        train_meta = train_meta[train_idx][:, meta_idx].tocsr()
        
        train_info = self.train.dset.data.data_info
        meta_info = self.train.dset.meta[meta_name].meta_info
        
        train_info = self.sample_info(train_info, train_idx)
        meta_info = self.sample_info(meta_info, meta_idx)

        train_dset = SBaseXCDataBlock(SXCDataset(SMainXCDataset(data_info=train_info, data_lbl=train_meta, lbl_info=meta_info)))
        
        if self.test is not None:
            test_meta = block.test.dset.meta[meta_name].data_meta[:, meta_idx].tocsr()
            test_idx = np.where(test_meta.getnnz(axis=1) > 0)[0]
            test_meta = test_meta[test_idx].tocsr()
    
            test_info = block.test.dset.data.data_info
            test_info = self.sample_info(test_info, test_idx)
    
            test_dset = SBaseXCDataBlock(SXCDataset(SMainXCDataset(data_info=test_info, data_lbl=test_meta, lbl_info=meta_info)))
            return SXCDataBlock(train=train_dset, test=test_dset)
        
        return SXCDataBlock(train=train_dset)

    @property
    def lbl_info(self): return self.train.dset.data.lbl_info

    @property
    def lbl_dset(self): return SMainXCDataset(data_info=self.train.dset.data.lbl_info)

    @property
    def n_lbl(self): return self.train.dset.n_lbl

    @property
    def collator(self): return self.train.collate_fn
        
    @classmethod
    def from_cfg(cls, 
                 cfg:Union[str,Dict],
                 collate_fn:Optional[Callable]=identity_collate_fn,
                 valid_pct:Optional[float]=0.2,
                 seed=None):
        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)
        blks = {o:SBaseXCDataBlock.from_file(**cfg['path'][o], **cfg['parameters'], collate_fn=collate_fn) for o in ['train', 'valid', 'test'] if o in cfg['path']}
        return cls(**blks)
        
