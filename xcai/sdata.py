# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/35_sdata.ipynb.

# %% auto 0
__all__ = ['identity_collate_fn', 'Sampler', 'SMainXCDataset', 'SMetaXCDataset', 'SXCDataset', 'SBaseXCDataBlock', 'SXCDataBlock']

# %% ../nbs/35_sdata.ipynb 3
import torch, inspect, numpy as np, scipy.sparse as sp, inspect
from typing import Callable, Optional, Union, Dict
from torch.utils.data import DataLoader
from transformers import BatchEncoding
from itertools import chain

from .core import Filterer, Info
from .data import MainXCData, MetaXCData
from .data import BaseXCDataset, MainXCDataset, MetaXCDataset, XCDataset
from .data import MetaXCDatasets, BaseXCDataBlock, XCDataBlock
from .data import _read_sparse_file
from .graph.operations import *

from fastcore.utils import *
from fastcore.meta import *
from plum import dispatch

# %% ../nbs/35_sdata.ipynb 11
def identity_collate_fn(batch): return BatchEncoding(batch)

# %% ../nbs/35_sdata.ipynb 13
class Sampler:
    
    @staticmethod
    def dropout(idxs:List, remove:Optional[float]=None, replace:Optional[float]=None):
        remove_mask, replace_mask = list(), list()
        for idx in idxs:
            if remove is not None:
                if np.random.rand() < remove:
                    remove_mask.append([1]*len(idx))
                    if replace is not None:
                        replace_mask.append([0]*len(idx))
                else:
                    remove_mask.append([0]*len(idx))
                    if replace is not None: 
                        replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))
            elif replace is not None:
                replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))
        return remove_mask, replace_mask

    @staticmethod
    def prune_indices_and_scores(output:Dict, prefix:str, data_lbl_indices:List, data_lbl_scores:List, 
                                 indices:List, num_samples:Optional[int]=None, use_distribution:Optional[bool]=False,
                                 return_scores:Optional[bool]=False, dtype=torch.int64):
        entity = prefix.split('2')[-1]
        output[f'p{prefix}_idx'] = [data_lbl_indices[idx] for idx in indices]
        scores = [data_lbl_scores[idx] for idx in indices] if use_distribution or return_scores else None
        
        if num_samples:
            if scores is None:
                output[f'p{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:num_samples]] for o in output[f'p{prefix}_idx']]
            else:
                idxs, sc = list(), list()
                for p,q in zip(output[f'p{prefix}_idx'], scores):
                    assert len(p) == len(q)
                    rnd_idx = np.random.permutation(len(p))[:num_samples]
                    idxs.append([p[i] for i in rnd_idx])
                    sc.append([q[i] for i in rnd_idx])
                output[f'p{prefix}_idx'], scores = idxs, sc
                
        output[f'p{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'p{prefix}_idx']], dtype=dtype)
        return scores

    @staticmethod
    def sample_indices_and_scores(indices:List, scores:Optional[List]=None, num_samples:Optional[int]=1, 
                                  oversample:Optional[bool]=False, use_distribution:Optional[bool]=False, 
                                  return_scores:Optional[bool]=False):
        if use_distribution and scores is None:
            raise ValueError(f'`scores` cannot be empty when `use_distribution` is set.')
        
        s_indices, s_scores = [], []
        for k in range(len(indices)):
            probs = scores[k] if use_distribution else None
            size = num_samples if oversample else min(num_samples, len(indices[k]))
            
            rnd_idx = np.random.choice(len(indices[k]), size=size, p=probs, replace=oversample) if len(indices[k]) else []

            s_indices.append([indices[k][i] for i in rnd_idx])
            if return_scores:
                assert len(indices[k]) == len(scores[k]), f'Length of indices({len(indices[k])}) and scores({(len(scores[k]))}) should be equal.'
                s_scores.append([scores[k][i] for i in rnd_idx])

        return s_indices, s_scores

    @staticmethod
    def get_info(prefix:str, idxs:List, info:Dict, info_keys:List):
        output = dict()
        for k,v in info.items():
            if k in info_keys:
                if isinstance(v, np.ndarray) or isinstance(v, torch.Tensor):
                    o = v[idxs]
                    if isinstance(o, np.ndarray): o = torch.from_numpy(o)
                    output[f'{prefix}_{k}'] = o
                else:
                    output[f'{prefix}_{k}'] = [v[idx] for idx in idxs]
        return output

    @staticmethod
    def extract_items(
        prefix:str,
        data_lbl_indices:List,
        
        indices:List, 
        num_samples:int, 
        num_sampler_samples:int, 
        oversample:bool, 
                      
        info:Dict, 
        info_keys:List,
        
        use_distribution:Optional[bool]=False, 
        data_lbl_scores:Optional[List]=None, 
                      
        dropout_remove:Optional[float]=None, 
        dropout_replace:Optional[float]=None, 
        return_scores:Optional[bool]=False,
        dtype=torch.int64,
    ):
        output, entity = dict(), prefix.split('2')[-1]
            
        scores = Sampler.prune_indices_and_scores(output, prefix, data_lbl_indices, data_lbl_scores, indices, 
                                                  num_samples, use_distribution, return_scores, dtype=dtype)
        
        output[f'{prefix}_idx'], scores = Sampler.sample_indices_and_scores(output[f'p{prefix}_idx'], scores, 
                                                                            num_sampler_samples, oversample, 
                                                                            use_distribution, return_scores)
        if return_scores:
            output[f'{prefix}_scores'] = torch.tensor(list(chain(*scores)), dtype=torch.float32)

        output[f'{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'{prefix}_idx']], dtype=dtype)
        output[f'{prefix}_idx'] = torch.tensor(list(chain(*output[f'{prefix}_idx'])), dtype=dtype)
        output[f'p{prefix}_idx'] = torch.tensor(list(chain(*output[f'p{prefix}_idx'])), dtype=dtype)
        
        if info is not None:
            output.update(Sampler.get_info(prefix, output[f'{prefix}_idx'], info, info_keys))
            
        return output
        

# %% ../nbs/35_sdata.ipynb 16
class SMainXCDataset(MainXCDataset):

    def __init__(
        self,
        n_slbl_samples:Optional[int]=1,
        main_oversample:Optional[bool]=False,
        use_main_distribution:Optional[bool]=False,
        return_scores:Optional[bool]=False,
        **kwargs
    ):
        super().__init__(**kwargs)
        store_attr('n_slbl_samples,main_oversample,use_main_distribution,return_scores')
        
        self.data_lbl_scores = None
        if use_main_distribution or return_scores: self._store_scores()
        
    def _store_scores(self):
        if self.data_lbl is not None:
            if self.use_main_distribution:
                data_lbl = self.data_lbl / (self.data_lbl.sum(axis=1) + 1e-9)
                data_lbl = data_lbl.tocsr()
            else:
                data_lbl = self.data_lbl
            self.data_lbl_scores = [o.data.tolist() for o in data_lbl]
        
    def __getitems__(self, idxs:List):
        x = {'data_idx': torch.tensor(idxs, dtype=torch.int64)}
        x.update(self.get_info('data', idxs, self.data_info, self.data_info_keys))
        if self.data_lbl is not None:
            prefix = 'lbl2data'
            o = Sampler.extract_items(prefix, self.curr_data_lbl, idxs, self.n_lbl_samples, self.n_slbl_samples, 
                                      self.main_oversample, self.lbl_info, self.lbl_info_keys, self.use_main_distribution, 
                                      self.data_lbl_scores, return_scores=self.return_scores)
            x.update(o)
        return x
    

# %% ../nbs/35_sdata.ipynb 29
class SMetaXCDataset(MetaXCDataset):

    def __init__(
        self,
        n_sdata_meta_samples:Optional[int]=1,
        n_slbl_meta_samples:Optional[int]=1,
        meta_oversample:Optional[bool]=False,
        use_meta_distribution:Optional[bool]=False,
        meta_dropout_remove:Optional[float]=None,
        meta_dropout_replace:Optional[float]=None,
        return_scores:Optional[bool]=False,
        **kwargs
    ):
        super().__init__(**kwargs)
        store_attr('n_sdata_meta_samples,n_slbl_meta_samples,meta_oversample,use_meta_distribution')
        store_attr('meta_dropout_remove,meta_dropout_replace,return_scores')

        self.data_meta_scores, self.lbl_meta_scores = None, None
        if use_meta_distribution or return_scores: self._store_scores()

    def _store_scores(self):
        def get_scores(matrix:sp.csr_matrix, use_meta_distribution:bool):
            if matrix is not None:
                if use_meta_distribution:
                    matrix = matrix / (matrix.sum(axis=1) + 1e-9)
                    matrix = matrix.tocsr()
                return [o.data.tolist() for o in matrix]
                
        self.data_meta_scores = get_scores(self.data_meta, self.use_meta_distribution)
        self.lbl_meta_scores = get_scores(self.lbl_meta, self.use_meta_distribution)
        
    def get_data_meta(self, idxs:List):
        x, prefix = dict(), f'{self.prefix}2data'
        o = Sampler.extract_items(prefix, self.curr_data_meta, idxs, self.n_data_meta_samples, self.n_sdata_meta_samples, 
                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, 
                                  self.data_meta_scores, dropout_remove=self.meta_dropout_remove, 
                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)
        x.update(o)
        return x
        
    def get_lbl_meta(self, idxs:List):
        if self.curr_lbl_meta is None: return {}
        x, prefix = dict(), f'{self.prefix}2lbl'
        o = Sampler.extract_items(prefix, self.curr_lbl_meta, idxs, self.n_lbl_meta_samples, self.n_slbl_meta_samples, 
                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, 
                                  self.lbl_meta_scores, dropout_remove=self.meta_dropout_remove, 
                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)
        x.update(o)
        return x
        

# %% ../nbs/35_sdata.ipynb 36
class SXCDataset(XCDataset):

    def __init__(self, data:SMainXCDataset, **kwargs):
        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], SMetaXCDataset)})
        self._verify_inputs()
        
    @classmethod
    @delegates(SMainXCDataset.from_file)
    def from_file(cls, **kwargs):
        data = SMainXCDataset.from_file(**kwargs)
        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}

        meta = dict()
        for k,v in meta_kwargs.items():
            input_kwargs = {p:q.get(k,None) if isinstance(q, dict) else q for p,q in kwargs.items()}
            for o in v: input_kwargs.pop(o, None)
            meta[k] = SMetaXCDataset.from_file(**v, **input_kwargs)  
        # meta = {k:SMetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}
        
        return cls(data, **meta)
        
    def __getitems__(self, idxs:List):
        x = self.data.__getitems__(idxs)
        if self.n_meta:
            for meta in self.meta.values():
                x.update(meta.get_data_meta(idxs))
                if self.n_lbl:
                    z = meta.get_lbl_meta(x['lbl2data_idx'])
                    if len(z):
                        z[f'{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])
                        z[f'p{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])
                    x.update(z)
        return x

    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, 
                             topk:Optional[int]=10, **kwargs):
        data_lbl = Graph.threshold_on_degree(self.data.data_lbl, thresh=thresh)
        data_meta, lbl_meta = Graph.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk, 
                                                   do_normalize=True)
        
        self.meta['ohm_meta'] = SMetaXCDataset(prefix='ohm', data_meta=data_meta, lbl_meta=lbl_meta, 
                                               meta_info=dset.data.lbl_info, **kwargs)
        
    def get_data_lbl_random_walk_metadata(self, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, 
                                          prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, 
                                          degree_thresh=20, **kwargs):
        data_meta, lbl_meta = Operations.get_random_walk_matrix(self.data.data_lbl, batch_size, walk_to, 
                                                                prob_reset, topk_thresh, degree_thresh)
        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_meta, lbl_meta=lbl_meta,
                                               meta_info=self.data.lbl_info, **kwargs)
        

# %% ../nbs/35_sdata.ipynb 44
class SBaseXCDataBlock(BaseXCDataBlock):
    
    @classmethod
    @delegates(SXCDataset.from_file)
    def from_file(cls, collate_fn:Callable=identity_collate_fn, **kwargs):
        return cls(SXCDataset.from_file(**kwargs), collate_fn, **kwargs)
        

# %% ../nbs/35_sdata.ipynb 48
class SXCDataBlock(XCDataBlock):

    @staticmethod
    def inference_dset(data_info:Dict, data_lbl:sp.csr_matrix, lbl_info:Dict, data_lbl_filterer, 
                       **kwargs):
        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)
        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)
        data_lbl[x_idx, y_idx] = 1
        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])
    
        pred_dset = SXCDataset(SMainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,
                                              data_lbl_filterer=data_lbl_filterer, **kwargs))
        return pred_dset
    
    @classmethod
    def from_cfg(
        cls, 
        cfg:Union[str,Dict],
        collate_fn:Optional[Callable]=identity_collate_fn,
        valid_pct:Optional[float]=0.2,
        seed=None,
        **kwargs,
    ):
        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)

        blocks = dict()
        for o in ['train', 'valid', 'test']:
            if o in cfg['path']:
                params = cfg['parameters'].copy()
                params.update(kwargs)
                if o != 'train': 
                    params['meta_dropout_remove'], params['meta_dropout_replace'] = None, None
                blocks[o] = SBaseXCDataBlock.from_file(**cfg['path'][o], **params, collate_fn=collate_fn)
                
        return cls(**blocks)
        
