# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/09_generation.generate.ipynb.

# %% auto 0
__all__ = ['TriePtr', 'Hypothesis', 'pad_tensor', 'TrieBeam', 'TrieBeamSearch']

# %% ../../nbs/09_generation.generate.ipynb 3
import torch, math
import torch.multiprocessing as mp
from multiprocessing import Pool
import torch.nn.functional as F
from itertools import chain
from tqdm.auto import tqdm
from typing import Optional, Sequence, Any, Dict, List
from dataclasses import dataclass

from fastcore.utils import *
from fastcore.meta import *
from fastcore.parallel import *

from ..core import *
from ..transform import *
from .trie import *

# %% ../../nbs/09_generation.generate.ipynb 13
class TriePtr:

    def __init__(self, trie, max_info:Optional[int]=None):
        store_attr('trie,max_info')
        self.ptr, self.hyp = trie.root, [trie.root.tok]

    @property
    def tokens(self):
        return list(self.ptr.nxt_toks.keys())

    def next(self, val:int):
        if val not in self.tokens: raise ValueError(f'`{val}` not a valid next token.')
        self.ptr = self.ptr.nxt_toks[val]
        self.hyp.append(val)
        return self

    def suffixes(self):
        o = []
        Trie._search(self.ptr, self.hyp, o, self.max_info)
        return sorted(o, key=lambda x: x.cnt, reverse=True)

    @property
    def is_end(self):
        return self.ptr.is_end

    @property
    def value(self):
        info = list(self.ptr.info) if self.max_info is None else list(self.ptr.info)[:self.max_info]
        return TrieOutput(self.hyp, self.ptr.cnt, info)

    def copy(self):
        t = TriePtr(self.trie, self.max_info)
        t.ptr,t.hyp = self.ptr,self.hyp.copy()
        return t
        

# %% ../../nbs/09_generation.generate.ipynb 27
class Hypothesis:

    def __init__(self, n_bm:int, len_penalty:Optional[float]=1.0):
        store_attr('n_bm,len_penalty')
        self.worst_sc, self.beams = 1e9, []

    def __len__(self):
        return len(self.beams)

    def add(self, hyp, sum_logits:float, gen_len:Optional[int]=None):
        if gen_len is not None: sc = sum_logits/gen_len**self.len_penalty
        else: sc = sum_logits/len(hyp.s)**self.len_penalty

        if len(self) < self.n_bm or sc > self.worst_sc:
            self.beams.append((sc, hyp))
            if len(self) > self.n_bm:
                nxt_sc = sorted([(s,i) for i,(s,_) in enumerate(self.beams)])
                del self.beams[nxt_sc[0][1]]
                self.worst_sc = nxt_sc[1][0]
            else: self.worst_sc = min(sc, self.worst_sc)

    def is_done(self, best_sc:float, cur_len:int):
        if len(self) < self.n_bm: return False
        high_sc = best_sc/cur_len**self.len_penalty
        return self.worst_sc >= high_sc
        

# %% ../../nbs/09_generation.generate.ipynb 34
def pad_tensor(tensor, fill_value):
    max_len = max(len(t) for t in tensor)
    padded_tensor = torch.full((len(tensor), max_len), fill_value, dtype=tensor[0].dtype)
    mask = torch.zeros((len(tensor), max_len), dtype=torch.bool)
    for i, t in enumerate(tensor): padded_tensor[i, :len(t)], mask[i, :len(t)] = t, 1
    return padded_tensor, mask


# %% ../../nbs/09_generation.generate.ipynb 35
class TrieBeam:

    def __init__(self, trie:Trie, eos_tok:int, n_bm:Optional[int]=5, len_penalty:Optional[float]=1.0, 
                 max_info:Optional[int]=None, **kwargs):
        store_attr('trie,eos_tok,n_bm,len_penalty,max_info')
        self.tfm = XCPadOutputTfm(**kwargs)

    def valid(self, pointers:List, scores:torch.FloatTensor):
        all_tok, all_sc, all_idx = [], [], []
        for ptr,sc in zip(pointers, scores):
            batch_tok = [torch.tensor([], dtype=torch.long)]
            batch_sc = [torch.tensor([], dtype=scores.dtype)]
            batch_idx = [torch.tensor([], dtype=torch.long)] 
            for i,(p,s) in enumerate(zip(ptr,sc)):
                toks = torch.tensor(p.tokens)
                batch_tok.append(toks)
                batch_sc.append(s[toks])
                batch_idx.append(torch.full((len(toks),), i))
            all_tok.append(torch.concat(batch_tok))
            all_sc.append(torch.concat(batch_sc))
            all_idx.append(torch.concat(batch_idx))
        all_tok, mask = pad_tensor(all_tok, -100)
        all_sc, _ = pad_tensor(all_sc, -float('Inf'))
        all_idx,_ = pad_tensor(all_idx, -100)
        return all_tok, all_sc, all_idx, mask

    def topk(self, tok:torch.Tensor, sc:torch.Tensor, idx:torch.Tensor, mask:torch.Tensor):
        top_sc, top_i = (
            torch.topk(sc, 2*self.n_bm, dim=1) 
            if sc.shape[1] > 2*self.n_bm else torch.sort(sc, dim=1, descending=True)
        )
        top_idx, top_tok, mask = idx.gather(1,top_i), tok.gather(1,top_i), mask.gather(1, top_i)
        return top_tok, top_sc, top_idx, mask

    def next(self, pointers:List, tokens:torch.Tensor, scores:torch.Tensor, indices:torch.Tensor, masks:torch.Tensor):
        all_ptr, all_sc = [], []
        for hyp,ptr,tok,sc,idx,mask in zip(self.hyp, pointers, tokens, scores, indices, masks):
            batch_tok, batch_sc, batch_idx = [], [], []
            for t,s,i,m in zip(tok,sc,idx,mask):
                t,s,i,m = t.item(),s.item(),i.item(),m.item()
                if t == self.eos_tok and m: hyp.add(ptr[i].copy().next(t).value, s)
                elif m: batch_tok.append(t); batch_sc.append(s); batch_idx.append(i)
            all_sc.append(torch.tensor(batch_sc)[:self.n_bm])
            batch_ptr = [ptr[i].copy().next(t) for t,i in zip(batch_tok[:self.n_bm], batch_idx[:self.n_bm])]
            all_ptr.append(batch_ptr)
        all_sc, _ = pad_tensor(all_sc, -float('Inf'))
        return all_ptr, all_sc

    def finalize(self, pointers:List, scores:torch.Tensor):
        outputs = []
        for i,(hyp,ptr,sc) in enumerate(zip(self.hyp,pointers,scores)):
            if len(hyp) < self.n_bm:
                for p,s in zip(ptr, sc):
                    for o in p.suffixes(): hyp.add(o, s)
            if len(hyp) < self.n_bm: raise ValueError(f'`len(hyp)`({len(hyp)}) < `n_bm`({self.n_bm})')
            seq_sc, seq_ids, info, n_info = list(map(list, zip(*[(s,h.s,h.info,len(h.info)) for s,h in hyp.beams])))
            outputs.append({
                'seq2data_data2ptr':[self.n_bm],
                'seq2data_score':seq_sc, 
                'seq2data_output_ids':seq_ids, 
                'info2seq2data_idx':list(chain(*info)),
                'info2seq2data_seq2ptr':n_info,
                'info2seq2data_data2ptr':[sum(n_info)],
            })
        return outputs
    
    def proc(self, logits:torch.FloatTensor, n_bm:Optional[int]=None, len_penalty:Optional[float]=None, 
             max_info:Optional[int]=None):
        store_attr('n_bm,len_penalty,max_info', is_none=False)
        bsz,seq_len,cur_len = logits.shape[0],logits.shape[1],1
        
        self.hyp = [Hypothesis(self.n_bm, self.len_penalty) for _ in range(bsz)]
        sc, ptr = torch.zeros((bsz,1,1)), [[TriePtr(self.trie,self.max_info)] for _ in range(bsz)]
        
        while True:
            sc = logits[:, cur_len:cur_len+1] + sc
            v_tok, v_sc, v_idx, mask = self.valid(ptr, sc)
            top_tok, top_sc, top_idx, mask = self.topk(v_tok, v_sc, v_idx, mask)
            ptr, sc = self.next(ptr, top_tok, top_sc, top_idx, mask)
            sc = sc.unsqueeze(2)
            cur_len += 1
            
            if (cur_len == seq_len 
                or torch.all(torch.tensor([len(p) for p in ptr]) == 0) 
                or torch.all(torch.tensor([hyp.is_done(s.max().item(), cur_len) for hyp,s in zip(self.hyp,sc)]))):
                break
                
        outputs = self.finalize(ptr, sc.squeeze(2))
        outputs = self.tfm({k:list(chain(*[o[k] for o in outputs])) for k in outputs[0]})
        return outputs
    

# %% ../../nbs/09_generation.generate.ipynb 45
class TrieBeamSearch:

    @delegates(XCPadOutputTfm.__init__)
    def __init__(self, trie:Trie, eos_tok:int, n_bm:Optional[int]=5, len_penalty:Optional[float]=1.0, 
                 max_info:Optional[int]=None, **kwargs):
        store_attr('trie,eos_tok,n_bm,len_penalty,max_info')
        self.tb = TrieBeam(trie, eos_tok, n_bm=n_bm, len_penalty=len_penalty, max_info=max_info)
        
    def proc(self, model, inputs:Dict, n_bm:int=None, len_penalty:Optional[float]=None, 
             max_info:Optional[int]=None):
        store_attr('n_bm,len_penalty,max_info', is_none=False)
        
        logits = F.log_softmax(model(**inputs).logits, dim=2).cpu().detach()
        attention_mask = inputs['data_attention_mask'].bool().cpu().detach()
        mask = torch.logical_not(attention_mask.unsqueeze(2).expand(logits.size()))
        logits[mask] = 0
        
        outputs = self.tb.proc(logits, n_bm=self.n_bm, len_penalty=self.len_penalty, max_info=self.max_info)
        outputs['info2seq2data_score'] = torch.repeat_interleave(outputs['seq2data_score'], outputs['info2seq2data_seq2ptr'], dim=0)
        return outputs
        
