# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_transform.ipynb.

# %% auto 0
__all__ = ['PadTfm', 'CollapseTfm', 'CollateFeatTfm', 'PadFeatTfm', 'AlignInputIdsTfm', 'XCPadFeatTfm', 'XCPadOutputTfm',
           'SampleFeatTfm', 'XCSamplePadFeatTfm', 'RamenPadFeatTfm', 'RemoveColumnTfm', 'NGPadFeatTfm', 'TfmPipeline',
           'AugmentMetaInputIdsTfm', 'TriePruneInputIdsTfm']

# %% ../nbs/01_transform.ipynb 2
import torch, numpy as np, re, pickle
from tqdm.auto import tqdm
from scipy import sparse
from transformers import AutoTokenizer, BatchEncoding
from itertools import chain

from fastcore.utils import *
from fastcore.meta import *
from plum import dispatch
from fastprogress.fastprogress import master_bar, progress_bar

from .core import *
from .generation.trie import *
from .data import XCDataBlock, BaseXCDataBlock

# %% ../nbs/01_transform.ipynb 13
class PadTfm:

    def __init__(self, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]='right', 
                 ret_t:Optional[bool]=True,
                 in_place:Optional[bool]=True,
                 **kwargs):
        store_attr('pad_tok,pad_side,ret_t,in_place')

    def _sz_help(self, x:List, sz:List, lev:int):
        if len(x) and isinstance(x[0], list):
            l = max(len(o) for o in x)
            if len(sz) > lev: sz[lev] = max(sz[lev], l)
            else: sz.append(l)
            for o in x: self._sz_help(o, sz, lev+1)

    def get_sz(self, x:List):
        sz = [len(x)]
        self._sz_help(x, sz, len(sz))
        return sz

    def _pad_help(self, x:List, sz:List, pads:List, lev:int):
        if len(x) and isinstance(x[0], list):
            for i,o in enumerate(x): x[i] = self._pad_help(o, sz, pads, lev+1)
        rem = [pads[lev]]*(sz[lev] - len(x))
        return x+rem if self.pad_side == 'right' else rem+x

    def __call__(self, 
                 x:List, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]=None, 
                 ret_t:Optional[bool]=None, 
                 in_place:Optional[bool]=None):
        store_attr('pad_tok,pad_side,ret_t,in_place', is_none=False)
        if self.pad_tok is None: raise ValueError('`pad_tok` cannot be None.')
        
        sz = self.get_sz(x)
        pads = [self.pad_tok]
        for s in sz[:0:-1]: pads.insert(0, [pads[0]]*s)
        if not self.in_place: x = x.copy()
        x = self._pad_help(x, sz, pads, 0)
        try: return torch.tensor(x) if self.ret_t else x
        except: return x
        

# %% ../nbs/01_transform.ipynb 22
class CollapseTfm:

    def __init__(self, lev:int=0, use_ptr:int=True, **kwargs):
        store_attr('lev,use_ptr')

    def collapse(self, x:List, ptr:Dict, lev:int):
        if not isinstance(x, list): raise ValueError(f'`x` should be a list, check the `lev`({self.lev}).')
        if self.lev == lev:
            if lev in ptr: ptr[lev].append(len(x))
            else: ptr[lev] = [len(x)]
            return x
        x = list(chain(*[self.collapse(o, ptr, lev+1) for o in x]))
        if lev in ptr: ptr[lev].append(len(x))
        else: ptr[lev] = [len(x)]
        return x

    def _get_ptr(self, ptr):
        for v in ptr.values():
            for p,q in enumerate(v[1:]): v[p+1] = v[p] + q
        
    def __call__(self, x:List, lev:int=None, use_ptr:Optional[int]=None):
        store_attr('lev,use_ptr', is_none=False)
        
        ptr = dict()
        x = self.collapse(x, ptr, 0)
        if self.use_ptr: self._get_ptr(ptr)
        return x, ptr


# %% ../nbs/01_transform.ipynb 26
class CollateFeatTfm:

    def __init__(self, prefix:Optional[str]=None, drop:Optional[bool]=True, lev:Optional[int]=0, **kwargs):
        store_attr('prefix,drop,lev')
        self.colps_proc = CollapseTfm(lev, use_ptr=False)

    def proc(self, x:Union[Dict, List], prefix:Optional[str]=None, drop:Optional[bool]=True, lev:Optional[int]=0):
        if isinstance(x, list):
            name = [k for k in x[0] if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: [o.pop(k) if drop else o[k] for o in x] for k in name}
            if lev > 0:
                for k in name: 
                    feat[k], ptr = self.colps_proc(feat[k], lev)
                    for p,q in ptr.items(): 
                        if p != 0: feat[f'{k}_ptr-{p}'] = q
        elif isinstance(x, dict):
            name = [k for k in x if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: x.pop(k) if drop else x[k] for k in name}
        return feat

    def __call__(self, x:Union[Dict, List], prefix:Optional[str]=None, drop:Optional[bool]=None, lev:Optional[int]=None):
        store_attr('prefix,drop,lev', is_none=False)
        return self.proc(x, self.prefix, self.drop, self.lev)
        
        

# %% ../nbs/01_transform.ipynb 30
class PadFeatTfm:

    def __init__(self,
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=True, 
                 pad_tok:Optional[int]=0, 
                 pad_side:Optional[str]='right', 
                 ret_t:Optional[bool]=True,
                 in_place:Optional[bool]=True,
                 lev:Optional[int]=0,
                 **kwargs):
        store_attr('prefix,drop,pad_tok,pad_side,ret_t,in_place,lev')
        self.pad_proc, self.coll_proc = PadTfm(), CollateFeatTfm(prefix=prefix, drop=drop, lev=lev)

    def get_feat(self, 
                 x:Union[Dict, List], 
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=True, 
                 lev:Optional[int]=0):
        if isinstance(x, list):
            name = [k for k in x[0] if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: [o.pop(k) if drop else o[k] for o in x] for k in name}
            if lev > 0:
                for k in name: 
                    feat[k], ptr = self.coll_proc(feat[k], lev)
                    for p,q in ptr.items(): 
                        if p != 0: feat[f'{k}_ptr-{p}'] = q
        elif isinstance(x, dict):
            name = [k for k in x if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: x.pop(k) if drop else x[k] for k in name}
        return feat

    def proc(self, x):
        return BatchEncoding({
            k: (self.pad_proc(v, 0, self.pad_side, self.ret_t, self.in_place) 
                if re.match('(.*_attention_mask|.*_token_type_ids)', k) else 
                self.pad_proc(v, self.pad_tok, self.pad_side, self.ret_t, self.in_place)) 
            for k,v in x.items()
        })
        
    def __call__(self, x:Union[Dict, List], 
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=None, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]=None, 
                 ret_t:Optional[bool]=None, 
                 in_place:Optional[bool]=None,
                 lev:Optional[int]=0):
        store_attr('prefix,drop,pad_tok,pad_side,ret_t,in_place,lev', is_none=False)
        feat = self.coll_proc(x, self.prefix, self.drop, self.lev)
        return self.proc(feat)
        

# %% ../nbs/01_transform.ipynb 38
class AlignInputIdsTfm:

    def __init__(self,
                 inp:Optional[str]='data',
                 targ:Optional[str]='lbl2data',
                 ptr:Optional[str]='lbl2data_data2ptr',
                 sep_tok:Optional[int]=0, 
                 pad_tok:Optional[int]=0,
                 device:Union[str,torch.device]='cpu', 
                 **kwargs):
        store_attr('inp,targ,ptr,sep_tok,pad_tok,device')

    @dispatch
    def proc(self, inp_ids:List, targ_ids:List, sep_tok:int, targ_mask:Optional[List]=None, targ_tok:Optional[List]=None, **kwargs):
        for i,ids in enumerate(inp_ids):
            inp_len = len(ids)
            for j,t in enumerate(targ_ids[i]):
                if len(t) > inp_len: 
                    targ_ids[i][j] = t[:inp_len-1]+[self.sep_tok]
                    if targ_mask is not None: targ_mask[i][j] = targ_mask[i][j][:inp_len]
                    if targ_tok is not None: targ_tok[i][j] = targ_tok[i][j][:inp_len] 
        return targ_ids, targ_mask, targ_tok

    @dispatch
    def proc(self, inp_ids:torch.Tensor, targ_ids:torch.Tensor, ptr:torch.Tensor, sep_tok:int, pad_tok:int,
             targ_mask:Optional[torch.Tensor]=None, targ_tok:Optional[torch.Tensor]=None):
        inp_len = (inp_ids == sep_tok).cumsum(1).argmax(1) + 1
        inp_len = torch.repeat_interleave(inp_len, ptr)
        targ_len = (targ_ids == sep_tok).cumsum(1).argmax(1) + 1
        seq_len = torch.where(inp_len < targ_len, inp_len, targ_len)
        
        for i,(p,q) in enumerate(zip(seq_len, targ_len)):
            targ_ids[i,p-1] = sep_tok
            targ_ids[i,p:q] = pad_tok 
            if targ_mask is not None: targ_mask[i,p:q] = 0
            if targ_tok is not None: targ_tok[i,p:q] = 0
        return targ_ids, targ_mask, targ_tok
        
    def __call__(self, x:Dict, 
                 inp:Optional[str]=None, 
                 targ:Optional[str]=None,
                 ptr:Optional[str]=None, 
                 sep_tok:Optional[int]=None, 
                 pad_tok:Optional[int]=None):
        store_attr('inp,targ,ptr,sep_tok,pad_tok', is_none=False)

        def get_attr(x, keys, required=False):
            attr = []
            for k in keys.split(','):
                if k not in x: 
                    if required: raise ValueError(f'"{k}" not in `x`')
                    else: attr.append(None)
                else: attr.append(x[k])
            return attr
            
        inp_ids, targ_ids = get_attr(x, f'{self.inp}_input_ids,{self.targ}_input_ids')
        if inp_ids is None or targ_ids is None: return x
        targ_mask, targ_tok = get_attr(x, f'{self.targ}_attention_mask,{self.targ}_token_type_ids') 
        ptr = None if self.ptr is None else x[self.ptr]
        
        targ_ids, targ_mask, targ_tok = self.proc(inp_ids, targ_ids, ptr=ptr, targ_mask=targ_mask, targ_tok=targ_tok, 
                                                  sep_tok=self.sep_tok, pad_tok=self.pad_tok)
        def set_attr(x, keys, vals):
            for i,(k,v) in enumerate(zip(keys.split(','),vals)):
                if v is not None: x[k] = v
                    
        set_attr(x, f'{self.targ}_input_ids,{self.targ}_attention_mask,{self.targ}_token_type_ids', [targ_ids,targ_mask,targ_tok])
        
        return x
        

# %% ../nbs/01_transform.ipynb 51
class XCPadFeatTfm:

    @delegates(PadFeatTfm.__init__)
    def __init__(self, **kwargs):
        self.tfm = PadFeatTfm(**kwargs)

    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        ptr = [x.pop(k) for k in ptr_name]
        return ptr[0] if len(ptr) else None

    def __call__(self, x):
        meta_name = set([k.split('_',maxsplit=1)[0].split('2')[0] for k in x[0]]).difference(['lbl', 'data'])
        out = self.tfm(x, prefix='lbl2data', lev=1, in_place=True, drop=True)
        lbl2data_data2ptr = self.extract_ptr(out, 'ptr-1')
        if lbl2data_data2ptr is not None: out['lbl2data_data2ptr'] = lbl2data_data2ptr
        out.update(self.tfm(x, prefix='data', lev=0, in_place=True, drop=True))
        for k in meta_name:
            o = self.tfm(x, prefix=f'{k}2lbl2data', lev=2, in_place=True, drop=True)
            o[f'{k}2lbl2data_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            o[f'{k}2lbl2data_lbl2ptr'] = self.extract_ptr(o, 'ptr-2')
            out.update(o)
            o = self.tfm(x, prefix=f'{k}2data', lev=1, in_place=True, drop=True)
            o[f'{k}2data_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            out.update(o)
        return out
        

# %% ../nbs/01_transform.ipynb 57
class XCPadOutputTfm:

    @delegates(PadFeatTfm.__init__)
    def __init__(self, **kwargs):
        self.tfm = PadFeatTfm(**kwargs)

    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        return [x.pop(k) for k in ptr_name][0]

    def __call__(self, x):
        out = self.tfm(x, prefix='info2seq', lev=0, in_place=True, drop=True)
        out.update(self.tfm(x, prefix='seq', lev=0, in_place=True, drop=True))
        return out
        

# %% ../nbs/01_transform.ipynb 61
class SampleFeatTfm:

    def __init__(self, feat_type:Optional[str]=None, smp_prefix:Optional[str]='', **kwargs):
        store_attr('feat_type,smp_prefix')

    def _get_feat(self, x:Dict):
        if self.feat_type is None: raise ValueError('`feat_type` is None.')
        return [o for o in x if re.match(f'^({self.feat_type})_.*', o)]

    def proc(self, x:List, lev:int, n_samples:Optional[int]=1):
        feat = self._get_feat(x[0])
        out, coll_proc = {}, CollapseTfm()
        
        if len(feat) > 0:
            smp_prefix = self.smp_prefix if self.smp_prefix == '' else f'{self.smp_prefix}2'
            def _size(o, l): return len(coll_proc(o[feat[0]],l)[0])
            rnd_idx = [np.random.permutation(_size(o,lev))[:n_samples] if _size(o,lev) else [] for o in x]
            
            out = []
            for idx,o in zip(rnd_idx, x):
                d = {}
                for k in feat: c = coll_proc(o[k],lev)[0]; d.update({smp_prefix+k:[c[i] for i in idx] if len(idx) >= 0 else []})
                out.append(d) 
        return out

    def __call__(self, x:[List,Dict,BatchEncoding], lev:int, n_samples:Optional[int]=1, 
                 feat_type:Optional[str]=None, smp_prefix:Optional[str]=None, **kwargs):
        store_attr('feat_type,smp_prefix', is_none=False)
        return self.proc(x, lev, n_samples)
        

# %% ../nbs/01_transform.ipynb 68
class XCSamplePadFeatTfm:

    def __init__(self, smp_features:Optional[List]=None, **kwargs):
        store_attr('smp_features')
        self.smp_proc, self.pad_proc = SampleFeatTfm(**kwargs), PadFeatTfm(**kwargs)
        
    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        ptr = [x.pop(k) for k in ptr_name]
        return ptr[0] if len(ptr) else None
        
    def sample_feat(self, x:List, feat:str, lev:int, n_samples:Optional[int]=1):
        out = self.pad_proc(x, prefix=f'{feat}_idx', lev=lev, in_place=False, drop=False)
        
        if f'{feat}_idx' in out:
            
            out[f'p{feat}_idx'] = out.pop(f'{feat}_idx')
            out[f'p{feat}_data2ptr'] = out.pop(f'{feat}_idx_ptr-1')
            if f'{feat}_idx_ptr-2' in out: out.pop(f'{feat}_idx_ptr-2')
                
            o = self.pad_proc(self.smp_proc(x, lev=lev-1, n_samples=n_samples, feat_type=feat), 
                              prefix=feat, lev=1, in_place=True, drop=True)
            o[f'{feat}_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            self.extract_ptr(o, 'ptr-2')
            
            out.update(o)
        return out

    def __call__(self, x:List, smp_features:Optional[List]=None):
        store_attr('smp_features', is_none=False)
        
        out, smp_features = BatchEncoding({}), () 
        if self.smp_features is not None:
            for feat,lev,n in self.smp_features: out.update(self.sample_feat(x, feat, lev, n))
            smp_features = list(zip(*self.smp_features))[0]
            
        out.update(self.pad_proc(x, prefix='data', lev=0, in_place=True, drop=True))
        
        meta_names = set([o.split('_',maxsplit=1)[0] for o in x[0]]).difference(smp_features+('data',))
        if 'lbl2data' in meta_names:
            out.update(self.pad_proc(x, prefix='lbl2data', lev=1, in_place=True, drop=True))
            out['lbl2data_data2ptr'] = self.extract_ptr(out, 'ptr-1')
        
        for k in meta_names.difference(['lbl2data']):
            if k.endswith('2lbl2data'): 
                o = self.pad_proc(x, prefix=k, lev=2, in_place=True, drop=True)
                o[f'{k}_data2ptr'] = self.extract_ptr(o, 'ptr-1')
                if 'lbl2data' in meta_names: o[f'{k}_lbl2data2ptr'] = self.extract_ptr(o, 'ptr-2')
                else: o[f'{k}_plbl2data2ptr'] = self.extract_ptr(o, 'ptr-2')
            elif k.endswith('2data'): 
                o = self.pad_proc(x, prefix=k, lev=1, in_place=True, drop=True)
                o[f'{k}_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            else: raise ValueError(f'Invalid metadata name ({k})')
            out.update(o)
        return out
        

# %% ../nbs/01_transform.ipynb 79
class RamenPadFeatTfm:

    def __init__(self, smp_features:Optional[List]=None, **kwargs):
        store_attr('smp_features')
        self.smp_proc, self.pad_proc = SampleFeatTfm(**kwargs), PadFeatTfm(**kwargs)
        
    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        ptr = [x.pop(k) for k in ptr_name]
        return ptr[0] if len(ptr) else None
    
    def get_feat(self, x, feat_type): return [o for o in x[0] if o.startswith(f'{feat_type}_')]

    def smp_feat(self, x:List, feat_type:str, n_samples:Optional[int]=1):
        feat = self.get_feat(x, feat_type)
        rnd_idx = [[np.random.permutation(len(v))[:n_samples] if len(v) else [-1] for v in o[feat[0]]] for o in x]

        out = []
        for o,idx in zip(x, rnd_idx):
            out.append({k:[[v[i] for i in ii if i >= 0] for v,ii in zip(o[k], idx)] for k in feat})
        return out

        
    def sample_feat(self, x:List, feat:str, lev:int, n_samples:Optional[Union[int,List]]=1):
        f = feat.split("|")
        
        if isinstance(n_samples, int):
            n_samples = (n_samples,)*len(f)
        else:
            if len(n_samples) != len(f): 
                raise ValueError(f"Size of `n_samples`({len(n_samples)}) should be equal to number of features.")
        
        f,of = f[0], f[1:]
        n_sample, n_samples = n_samples[0], n_samples[1:]
        
        out = self.pad_proc(x, prefix=f'{f}_idx', lev=1, in_place=False, drop=False)
        if f'{f}_idx' in out:
            out[f'p{f}_idx'],out[f'p{f}_{f.split("2")[-1]}2ptr'] = out.pop(f'{f}_idx'), self.extract_ptr(out, 'ptr-1')
            self.extract_ptr(out, 'ptr-2')

            smp_out = self.smp_proc(x, lev=0, n_samples=n_sample, feat_type=feat)

            o = self.pad_proc(smp_out, prefix=f, lev=1, in_place=True, drop=True)
            o[f'{f}_{f.split("2")[-1]}2ptr'] = self.extract_ptr(o, 'ptr-1')
            self.extract_ptr(o, 'ptr-2')
            out.update(o)

            for f,n_sample in zip(of,n_samples):
                o = self.pad_proc(smp_out, prefix=f'{f}_idx', lev=2, in_place=False, drop=False)
                if f'{f}_idx' in o:
                    o[f'p{"2".join(f.split("2")[:-1])}_idx'] = o.pop(f'{f}_idx')
                    o[f'p{"2".join(f.split("2")[:-1])}_{"2".join(f.split("2")[-2:])}2ptr'] = self.extract_ptr(o, 'ptr-2')
                    o[f'p{"2".join(f.split("2")[:-1])}_{f.split("2")[-1]}2ptr'] = self.extract_ptr(o, 'ptr-1')
                    out.update(o)

                    o = self.pad_proc(self.smp_feat(smp_out, f, n_sample), prefix=f, lev=2, in_place=True, drop=True)
                    feat = list(o.keys())
                    for k in feat:
                        p,q = k.split('_', maxsplit=1)
                        o["_".join(["2".join(p.split("2")[:-1]),q])] = o.pop(k)
                    o[f'{"2".join(p.split("2")[:-1])}_{"2".join(f.split("2")[-2:])}2ptr'] = self.extract_ptr(o, 'ptr-2')
                    o[f'{"2".join(p.split("2")[:-1])}_{f.split("2")[-1]}2ptr'] = self.extract_ptr(o, 'ptr-1')
                    out.update(o)
        return out
    

    def __call__(self, x:List, smp_features:Optional[List]=None):
        store_attr('smp_features', is_none=False)
        
        out, smp_features = BatchEncoding({}), () 
        if self.smp_features is not None:
            for feat,lev,n in self.smp_features: out.update(self.sample_feat(x, feat, lev, n))
            smp_features = list(chain(*[o[0].split('|') for o in self.smp_features]))
            
        out.update(self.pad_proc(x, prefix='data', lev=0, in_place=True, drop=True))
        
        meta_names = set([o.split('_',maxsplit=1)[0] for o in x[0]]).difference(smp_features+['data'])
        if 'lbl2data' in meta_names:
            out.update(self.pad_proc(x, prefix='lbl2data', lev=1, in_place=True, drop=True))
            out['lbl2data_data2ptr'] = self.extract_ptr(out, 'ptr-1')
        
        for k in meta_names.difference(['lbl2data']):
            if k.endswith('2lbl2data'): 
                o = self.pad_proc(x, prefix=k, lev=2, in_place=True, drop=True)
                o[f'{k}_data2ptr'] = self.extract_ptr(o, 'ptr-1')
                if 'lbl2data' in meta_names: o[f'{k}_lbl2data2ptr'] = self.extract_ptr(o, 'ptr-2')
                else: o[f'{k}_plbl2data2ptr'] = self.extract_ptr(o, 'ptr-2')
            elif k.endswith('2data'): 
                o = self.pad_proc(x, prefix=k, lev=1, in_place=True, drop=True)
                o[f'{k}_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            else: raise ValueError(f'Invalid metadata name ({k})')
            out.update(o)
        return out
        

# %% ../nbs/01_transform.ipynb 92
class RemoveColumnTfm:
    
    def __init__(self, column:List, **kwargs):
        self.column = column
    
    def __call__(self, x:Dict):
        for k in self.column: 
            if k in x: x.pop(k)
        return x

# %% ../nbs/01_transform.ipynb 95
class NGPadFeatTfm:

    def __init__(self, **kwargs):
        self.smp_proc, self.pad_proc = SampleFeatTfm(**kwargs), PadFeatTfm(**kwargs)

    def __call__(self, x:Dict):
        out = self.pad_proc(x, prefix='lbl2data_idx', lev=1, in_place=False, drop=False)
        if 'lbl2data_idx' in out:
            out['plbl2data_idx'] = out['lbl2data_idx']
            out['plbl2data_data2ptr'] = out.pop('lbl2data_idx_ptr-1')
            out.update(self.pad_proc(self.smp_proc(x, lev=0, feat_type='lbl2data', ptr_type='data'), 
                                     prefix='lbl2data', lev=1, in_place=True, drop=True))
        out.update(self.pad_proc(x, prefix='data', lev=0, in_place=True, drop=True))
        return out
        

# %% ../nbs/01_transform.ipynb 104
class TfmPipeline:

    def __init__(self, tfms:List):
        self.tfms = tfms

    def __call__(self, x):
        for tfm in self.tfms: x = tfm(x)
        return x
        

# %% ../nbs/01_transform.ipynb 132
class AugmentMetaInputIdsTfm:

    def __init__(self, meta:str, max_len:Optional[int]=None, exclude_sep:Optional[bool]=False):
        self.meta, self.max_len, self.exclude_sep = meta, max_len, exclude_sep
    
    def augment(self, data_ids:List, data_meta:sparse.csr_matrix, meta_ids:List):
        meta2data_ids = []
        for d_ids, d_meta in progress_bar(zip(data_ids, data_meta), total=len(data_ids)):
            m2d_ids, sep_tok = d_ids[:-1].copy() if self.exclude_sep else d_ids.copy(), d_ids[-1:]
            for o in d_meta.indices[np.random.permutation(len(d_meta.indices))]:
                if self.exclude_sep: m2d_ids.extend(meta_ids[o][1:-1])
                else: m2d_ids.extend(meta_ids[o][1:])
                if self.max_len is not None and len(m2d_ids)>=self.max_len: m2d_ids = m2d_ids[:self.max_len-1]; break
            meta2data_ids.append(m2d_ids+sep_tok)
        return meta2data_ids

    def proc(self, block:XCDataBlock, split:str, fld:str, side:Optional[str]='data'):
        if side not in ['data', 'lbl']: 
            raise ValueError("Invalid `side`, it should be in ['data','lbl']")
            
        if fld in get_attr(block, f'{split}.dset.data.{side}_info'):
            data_ids = get_attr(block, f'{split}.dset.data.{side}_info')[fld]
            meta_ids = get_attr(block, f'{split}.dset.meta.{self.meta}.meta_info')[fld]
            data_meta = get_attr(block, f'{split}.dset.meta.{self.meta}.{side}_meta')
            get_attr(block, f'{split}.dset.data.{side}_info')[f'{fld}_aug_{self.meta.split("_")[0]}'] = self.augment(data_ids, data_meta, meta_ids)

    def __call__(self, block:XCDataBlock, meta:str, side:Optional[str]='data', max_len:Optional[int]=None, 
                 exclude_sep:Optional[bool]=None):
        store_attr('meta,max_len,exclude_sep', is_none=False)
        for split in master_bar(['train', 'valid', 'test']):
            if hasattr(block, split) and get_attr(block, split) is not None: 
                for fld in ['input_ids', 'attention_mask', 'token_type_ids']: self.proc(block, split, fld, side)
        return block
        
    @classmethod
    def apply(cls, block:XCDataBlock, meta:str, side:Optional[str]='data', max_len:Optional[int]=None, exclude_sep:Optional[bool]=False):
        self = cls(meta, max_len, exclude_sep)
        return self(block, meta, side, max_len, exclude_sep)
        

# %% ../nbs/01_transform.ipynb 150
class TriePruneInputIdsTfm:

    def prune(self, block:XCDataBlock, loc:str, fld:str):
        x = get_attr(block, loc)
        if fld in x:
            trie = Trie.from_list(x[fld], None)
            trie.prune()
            x[f'{fld}_prn_tre'] = [trie.prefix(o) for o in x[fld]]

    def align(self, block:XCDataBlock, loc:str, inp:str, targ:str):
        x = get_attr(block, loc)
        if inp in x and targ in x:
            x[f'{targ}_prn_tre'] = [q[:len(p)] for i,(p,q) in enumerate(zip(x[inp],x[targ]))]
        
    def proc(self, block:XCDataBlock, loc:str):
        self.prune(block, loc, 'input_ids')
        self.align(block, loc, 'input_ids_prn_tre', 'attention_mask')
        self.align(block, loc, 'input_ids_prn_tre', 'token_type_ids')
        return block

    def __call__(self, block:XCDataBlock, loc:str):
        return self.proc(block, loc)

    @classmethod
    def apply(cls, block:XCDataBlock, loc:str):
        self = cls()
        return self(block, loc)
        
