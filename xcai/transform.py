# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_transform.ipynb.

# %% auto 0
__all__ = ['PadTfm', 'CollapseTfm', 'CollateFeatTfm', 'PadFeatTfm', 'AlignInputIdsTfm', 'XCPadFeatTfm', 'XCPadOutputTfm',
           'NGSampleFeatTfm', 'NGPadFeatTfm', 'TfmPipeline', 'AugmentMetaInputIdsTfm', 'TriePruneInputIdsTfm']

# %% ../nbs/01_transform.ipynb 2
import torch, numpy as np, re
from tqdm.auto import tqdm
from scipy import sparse
from transformers import AutoTokenizer, BatchEncoding
from itertools import chain

from fastcore.utils import *
from fastcore.meta import *
from fastcore.dispatch import *
from fastprogress.fastprogress import master_bar, progress_bar

from .core import *
from .generation.trie import *
from .data import XCDataBlock, BaseXCDataBlock

# %% ../nbs/01_transform.ipynb 9
class PadTfm:

    def __init__(self, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]='right', 
                 ret_t:Optional[bool]=True,
                 in_place:Optional[bool]=True,
                 **kwargs):
        store_attr('pad_tok,pad_side,ret_t,in_place')

    def _sz_help(self, x:List, sz:List, lev:int):
        if isinstance(x[0], list):
            l = max(len(o) for o in x)
            if len(sz) > lev: sz[lev] = max(sz[lev], l)
            else: sz.append(l)
            for o in x: self._sz_help(o, sz, lev+1)

    def get_sz(self, x:List):
        sz = [len(x)]
        self._sz_help(x, sz, len(sz))
        return sz

    def _pad_help(self, x:List, sz:List, pads:List, lev:int):
        if len(x) and isinstance(x[0], list):
            for i,o in enumerate(x): x[i] = self._pad_help(o, sz, pads, lev+1)
        rem = [pads[lev]]*(sz[lev] - len(x))
        return x+rem if self.pad_side == 'right' else rem+x

    def __call__(self, 
                 x:List, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]=None, 
                 ret_t:Optional[bool]=None, 
                 in_place:Optional[bool]=None):
        store_attr('pad_tok,pad_side,ret_t,in_place', is_none=False)
        if self.pad_tok is None: raise ValueError('`pad_tok` cannot be None.')
        
        sz = self.get_sz(x)
        pads = [self.pad_tok]
        for s in sz[:0:-1]: pads.insert(0, [pads[0]]*s)
        if not self.in_place: x = x.copy()
        x = self._pad_help(x, sz, pads, 0)
        try: return torch.tensor(x) if self.ret_t else x
        except: return x
        

# %% ../nbs/01_transform.ipynb 13
class CollapseTfm:

    def __init__(self, lev:int=0, use_ptr:int=True, **kwargs):
        store_attr('lev,use_ptr')

    def collapse(self, x:List, ptr:Dict, lev:int):
        if not isinstance(x, list): raise ValueError(f'`x` should be a list, check the `lev`({self.lev}).')
        if self.lev == lev:
            if lev in ptr: ptr[lev].append(len(x))
            else: ptr[lev] = [len(x)]
            return x
        x = list(chain(*[self.collapse(o, ptr, lev+1) for o in x]))
        if lev in ptr: ptr[lev].append(len(x))
        else: ptr[lev] = [len(x)]
        return x

    def _get_ptr(self, ptr):
        for v in ptr.values():
            for p,q in enumerate(v[1:]): v[p+1] = v[p] + q
        
    def __call__(self, x:List, lev:int=None, use_ptr:Optional[int]=None):
        store_attr('lev,use_ptr', is_none=False)
        
        ptr = dict()
        x = self.collapse(x, ptr, 0)
        if self.use_ptr: self._get_ptr(ptr)
        return x, ptr


# %% ../nbs/01_transform.ipynb 17
class CollateFeatTfm:

    def __init__(self, prefix:Optional[str]=None, drop:Optional[bool]=True, lev:Optional[int]=0, **kwargs):
        store_attr('prefix,drop,lev')
        self.colps_proc = CollapseTfm(lev, use_ptr=False)

    def proc(self, x:Union[Dict, List], prefix:Optional[str]=None, drop:Optional[bool]=True, lev:Optional[int]=0):
        if isinstance(x, list):
            name = [k for k in x[0] if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: [o.pop(k) if drop else o[k] for o in x] for k in name}
            if lev > 0:
                for k in name: 
                    feat[k], ptr = self.colps_proc(feat[k], lev)
                    for p,q in ptr.items(): 
                        if p != 0: feat[f'{k}_ptr-{p}'] = q
        elif isinstance(x, dict):
            name = [k for k in x if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: x.pop(k) if drop else x[k] for k in name}
        return feat

    def __call__(self, x:Union[Dict, List], prefix:Optional[str]=None, drop:Optional[bool]=None, lev:Optional[int]=None):
        store_attr('prefix,drop,lev', is_none=False)
        return self.proc(x, self.prefix, self.drop, self.lev)
        
        

# %% ../nbs/01_transform.ipynb 21
class PadFeatTfm:

    def __init__(self,
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=True, 
                 pad_tok:Optional[int]=0, 
                 pad_side:Optional[str]='right', 
                 ret_t:Optional[bool]=True,
                 in_place:Optional[bool]=True,
                 lev:Optional[int]=0,
                 **kwargs):
        store_attr('prefix,drop,pad_tok,pad_side,ret_t,in_place,lev')
        self.pad_proc, self.coll_proc = PadTfm(), CollateFeatTfm(prefix=prefix, drop=drop, lev=lev)

    def get_feat(self, 
                 x:Union[Dict, List], 
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=True, 
                 lev:Optional[int]=0):
        if isinstance(x, list):
            name = [k for k in x[0] if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: [o.pop(k) if drop else o[k] for o in x] for k in name}
            if lev > 0:
                for k in name: 
                    feat[k], ptr = self.colps_proc(feat[k], lev)
                    for p,q in ptr.items(): 
                        if p != 0: feat[f'{k}_ptr-{p}'] = q
        elif isinstance(x, dict):
            name = [k for k in x if prefix is None or re.match(f'^{prefix}',k)]
            feat = {k: x.pop(k) if drop else x[k] for k in name}
        return feat

    def proc(self, x):
        return BatchEncoding({
            k: (self.pad_proc(v, 0, self.pad_side, self.ret_t, self.in_place) 
                if re.match('(.*_attention_mask|.*_token_type_ids)', k) else 
                self.pad_proc(v, self.pad_tok, self.pad_side, self.ret_t, self.in_place)) 
            for k,v in x.items()
        })
        
    def __call__(self, x:Union[Dict, List], 
                 prefix:Optional[str]=None, 
                 drop:Optional[bool]=None, 
                 pad_tok:Optional[int]=None, 
                 pad_side:Optional[str]=None, 
                 ret_t:Optional[bool]=None, 
                 in_place:Optional[bool]=None,
                 lev:Optional[int]=0):
        store_attr('prefix,drop,pad_tok,pad_side,ret_t,in_place,lev', is_none=False)
        feat = self.coll_proc(x, self.prefix, self.drop, self.lev)
        return self.proc(feat)
        

# %% ../nbs/01_transform.ipynb 29
class AlignInputIdsTfm:

    def __init__(self,
                 inp:Optional[str]='data',
                 targ:Optional[str]='lbl2data',
                 ptr:Optional[str]='lbl2data_data2ptr',
                 sep_tok:Optional[int]=0, 
                 pad_tok:Optional[int]=0,
                 device:Union[str,torch.device]='cpu', 
                 **kwargs):
        store_attr('inp,targ,ptr,sep_tok,pad_tok,device')

    @typedispatch
    def proc(self, inp_ids:List, targ_ids:List, sep_tok:int, targ_mask:Optional[List]=None, targ_tok:Optional[List]=None, **kwargs):
        for i,ids in enumerate(inp_ids):
            inp_len = len(ids)
            for j,t in enumerate(targ_ids[i]):
                if len(t) > inp_len: 
                    targ_ids[i][j] = t[:inp_len-1]+[self.sep_tok]
                    if targ_mask is not None: targ_mask[i][j] = targ_mask[i][j][:inp_len]
                    if targ_tok is not None: targ_tok[i][j] = targ_tok[i][j][:inp_len] 
        return targ_ids, targ_mask, targ_tok

    @typedispatch
    def proc(self, inp_ids:torch.Tensor, targ_ids:torch.Tensor, ptr:torch.Tensor, sep_tok:int, pad_tok:int,
             targ_mask:Optional[torch.Tensor]=None, targ_tok:Optional[torch.Tensor]=None):
        inp_len = (inp_ids == sep_tok).cumsum(1).argmax(1) + 1
        inp_len = torch.repeat_interleave(inp_len, ptr)
        targ_len = (targ_ids == sep_tok).cumsum(1).argmax(1) + 1
        seq_len = torch.where(inp_len < targ_len, inp_len, targ_len)
        
        for i,(p,q) in enumerate(zip(seq_len, targ_len)):
            targ_ids[i,p-1] = sep_tok
            targ_ids[i,p:q] = pad_tok 
            if targ_mask is not None: targ_mask[i,p:q] = 0
            if targ_tok is not None: targ_tok[i,p:q] = 0
        return targ_ids, targ_mask, targ_tok
        
    def __call__(self, x:Dict, 
                 inp:Optional[str]=None, 
                 targ:Optional[str]=None,
                 ptr:Optional[str]=None, 
                 sep_tok:Optional[int]=None, 
                 pad_tok:Optional[int]=None):
        store_attr('inp,targ,ptr,sep_tok,pad_tok', is_none=False)

        def get_attr(x, keys, required=False):
            attr = []
            for k in keys.split(','):
                if k not in x: 
                    if required: raise ValueError(f'"{k}" not in `x`')
                    else: attr.append(None)
                else: attr.append(x[k])
            return attr
            
        inp_ids, targ_ids = get_attr(x, f'{self.inp}_input_ids,{self.targ}_input_ids')
        if inp_ids is None or targ_ids is None: return x
        targ_mask, targ_tok = get_attr(x, f'{self.targ}_attention_mask,{self.targ}_token_type_ids') 
        ptr = None if self.ptr is None else x[self.ptr]
        
        targ_ids, targ_mask, targ_tok = self.proc(inp_ids, targ_ids, ptr=ptr, targ_mask=targ_mask, targ_tok=targ_tok, 
                                                  sep_tok=self.sep_tok, pad_tok=self.pad_tok)
        def set_attr(x, keys, vals):
            for i,(k,v) in enumerate(zip(keys.split(','),vals)):
                if v is not None: x[k] = v
                    
        set_attr(x, f'{self.targ}_input_ids,{self.targ}_attention_mask,{self.targ}_token_type_ids', [targ_ids,targ_mask,targ_tok])
        
        return x
        

# %% ../nbs/01_transform.ipynb 42
class XCPadFeatTfm:

    @delegates(PadFeatTfm.__init__)
    def __init__(self, **kwargs):
        self.tfm = PadFeatTfm(**kwargs)

    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        ptr = [x.pop(k) for k in ptr_name]
        return ptr[0] if len(ptr) else None

    def __call__(self, x):
        meta_name = set([k.split('_',maxsplit=1)[0].split('2')[0] for k in x[0]]).difference(['lbl', 'data'])
        out = self.tfm(x, prefix='lbl2data', lev=1, in_place=True, drop=True)
        lbl2data_data2ptr = self.extract_ptr(out, 'ptr-1')
        if lbl2data_data2ptr is not None: out['lbl2data_data2ptr'] = lbl2data_data2ptr
        out.update(self.tfm(x, prefix='data', lev=0, in_place=True, drop=True))
        for k in meta_name:
            o = self.tfm(x, prefix=f'{k}2lbl2data', lev=2, in_place=True, drop=True)
            o[f'{k}2lbl2data_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            o[f'{k}2lbl2data_lbl2ptr'] = self.extract_ptr(o, 'ptr-2')
            out.update(o)
            o = self.tfm(x, prefix=f'{k}2data', lev=1, in_place=True, drop=True)
            o[f'{k}2data_data2ptr'] = self.extract_ptr(o, 'ptr-1')
            out.update(o)
        return out
        

# %% ../nbs/01_transform.ipynb 48
class XCPadOutputTfm:

    @delegates(PadFeatTfm.__init__)
    def __init__(self, **kwargs):
        self.tfm = PadFeatTfm(**kwargs)

    def extract_ptr(self, x:Dict, suffix:str):
        ptr_name = [k for k in x if re.match(f'.*{suffix}$',k)]
        return [x.pop(k) for k in ptr_name][0]

    def __call__(self, x):
        out = self.tfm(x, prefix='info2seq', lev=0, in_place=True, drop=True)
        out.update(self.tfm(x, prefix='seq', lev=0, in_place=True, drop=True))
        return out
        

# %% ../nbs/01_transform.ipynb 52
class NGSampleFeatTfm:

    def __init__(self, prefix:Optional[str]=None, smp_prefix:Optional[str]='', **kwargs):
        store_attr('prefix,smp_prefix')

    def _get_feat(self, x:Dict):
        if self.prefix is None: raise ValueError('`prefix` is None.')
        feat, ptr = [o for o in x if re.match(f'^{self.prefix}_.*', o)], None
        if len(feat) > 0:
            for i,o in enumerate(feat):
                if re.match(f'.*2ptr$', o): ptr=o; feat.pop(i); break
        return feat, ptr

    @typedispatch
    def proc(self, x:Dict):
        feat, pn = self._get_feat(x)
        smp_prefix = self.smp_prefix if self.smp_prefix == '' else f'{self.smp_prefix}2'
        if len(feat) > 0:
            if pn is None: raise ValueError(f'`ptr` is empty for `prefix`({self.prefix})')
            smp_idx = torch.cat([torch.randint(high=o, size=(1,)) for o in x[pn]])
            ptr = x[pn].cumsum(0)
            smp_idx[1:] = smp_idx[1:]+ptr[:-1]
            for o in feat:
                if isinstance(x[o], torch.Tensor): x[smp_prefix+o] = x[o][smp_idx]
                else: x[smp_prefix+o] = [x[o][i] for i in smp_idx]
            x[smp_prefix+o] = torch.ones(len(smp_idx), dtype=torch.int64)
        return x

    @typedispatch
    def proc(self, x:List):
        out, (feat, _) = {}, self._get_feat(x[0])
        if len(feat) > 0:
            smp_prefix = self.smp_prefix if self.smp_prefix == '' else f'{self.smp_prefix}2'
            rnd_idx = [np.random.randint(len(o[feat[0]])) for o in x]
            out = [{smp_prefix+k:o[k][i] for k in feat if isinstance(o[k], list)} for i,o in zip(rnd_idx, x)]
            for o in out: o[smp_prefix+self.prefix+'_data2ptr'] = 1
        return out

    def __call__(self, x:[List,Dict], prefix:Optional[str]=None, smp_prefix:Optional[str]=None, **kwargs):
        store_attr('prefix,smp_prefix', is_none=False)
        return self.proc(x)
        

# %% ../nbs/01_transform.ipynb 67
class NGPadFeatTfm:

    def __init__(self, **kwargs):
        self.smp_proc, self.pad_proc = NGSampleFeatTfm(**kwargs), PadFeatTfm(**kwargs)

    def __call__(self, x:Dict):
        out = self.pad_proc(x, prefix='lbl2data_idx', lev=1, in_place=False, drop=False)
        if 'lbl2data_idx' in out:
            out['plbl2data_idx'] = out['lbl2data_idx']
            out['plbl2data_data2ptr'] = out.pop('lbl2data_idx_ptr-1')
            out.update(self.pad_proc(self.smp_proc(x, prefix='lbl2data'), prefix='lbl2data', lev=0, in_place=True, drop=True))
        out.update(self.pad_proc(x, prefix='data', lev=0, in_place=True, drop=True))
        return out
        

# %% ../nbs/01_transform.ipynb 76
class TfmPipeline:

    def __init__(self, tfms:List):
        self.tfms = tfms

    def __call__(self, x):
        for tfm in self.tfms: x = tfm(x)
        return x
        

# %% ../nbs/01_transform.ipynb 104
class AugmentMetaInputIdsTfm:

    def __init__(self, meta:str, max_len:Optional[int]=None, exclude_sep:Optional[bool]=False):
        self.meta, self.max_len, self.exclude_sep = meta, max_len, exclude_sep
    
    def augment(self, data_ids:List, data_meta:sparse.csr_matrix, meta_ids:List):
        meta2data_ids = []
        for d_ids, d_meta in progress_bar(zip(data_ids, data_meta), total=len(data_ids)):
            m2d_ids, sep_tok = d_ids[:-1].copy() if self.exclude_sep else d_ids.copy(), d_ids[-1:]
            for o in d_meta.indices:
                if self.exclude_sep: m2d_ids.extend(meta_ids[o][1:-1])
                else: m2d_ids.extend(meta_ids[o][1:])
                if self.max_len is not None and len(m2d_ids)>=self.max_len: m2d_ids = m2d_ids[:self.max_len-1]; break
            meta2data_ids.append(m2d_ids+sep_tok)
        return meta2data_ids

    def proc(self, block:XCDataBlock, split:str, fld:str):
        if fld in get_attr(block, f'{split}.dset.data.data_info'):
            data_ids = get_attr(block, f'{split}.dset.data.data_info')[fld]
            meta_ids = get_attr(block, f'{split}.dset.meta.{self.meta}.meta_info')[fld]
            data_meta = get_attr(block, f'{split}.dset.meta.{self.meta}.data_meta')
            get_attr(block, f'{split}.dset.data.data_info')[f'{fld}_aug_{self.meta.split("_")[0]}'] = self.augment(data_ids, data_meta, meta_ids)

    def __call__(self, block:XCDataBlock, meta:str, max_len:Optional[int]=None, exclude_sep:Optional[bool]=None):
        store_attr('meta,max_len,exclude_sep', is_none=False)
        for split in master_bar(['train', 'valid', 'test']):
            if hasattr(block, split) and get_attr(block, split) is not None: 
                for fld in ['input_ids', 'attention_mask', 'token_type_ids']: self.proc(block, split, fld)
        return block
        
    @classmethod
    def apply(cls, block:XCDataBlock, meta:str, max_len:Optional[int]=None, exclude_sep:Optional[bool]=False):
        self = cls(meta, max_len, exclude_sep)
        return self(block, meta, max_len, exclude_sep)
        

# %% ../nbs/01_transform.ipynb 120
class TriePruneInputIdsTfm:

    def prune(self, block:XCDataBlock, loc:str, fld:str):
        x = get_attr(block, loc)
        if fld in x:
            trie = Trie.from_list(x[fld])
            trie.prune()
            x[f'{fld}_prn_tre'] = [trie.prefix(o) for o in x[fld]]

    def align(self, block:XCDataBlock, loc:str, inp:str, targ:str):
        x = get_attr(block, loc)
        x[f'{targ}_prn_tre'] = [q[:len(p)] for i,(p,q) in enumerate(zip(x[inp],x[targ]))]
        
    def proc(self, block:XCDataBlock, loc:str):
        self.prune(block, loc, 'input_ids')
        self.align(block, loc, 'input_ids_prn_tre', 'attention_mask')
        self.align(block, loc, 'input_ids_prn_tre', 'token_type_ids')
        return block

    def __call__(self, block:XCDataBlock, loc:str):
        return self.proc(block, loc)

    @classmethod
    def apply(cls, block:XCDataBlock, loc:str):
        self = cls()
        return self(block, loc)
        
