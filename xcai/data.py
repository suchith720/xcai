"""Datasets and collators for Extreme Classification"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_data.ipynb.

# %% auto 0
__all__ = ['MainXCData', 'MetaXCData', 'BaseXCDataset', 'MainXCDataset', 'MetaXCDataset', 'Operations', 'MetaXCDatasets',
           'XCDataset', 'XCCollator', 'BaseXCDataBlock', 'XCDataBlock']

# %% ../nbs/02_data.ipynb 3
import torch, inspect, numpy as np, pandas as pd, torch.nn.functional as F, random, scipy.sparse as sp

from scipy import sparse
from itertools import chain
from tqdm.auto import tqdm

from IPython.display import display
from typing import Dict, Optional, Callable, Union
from torch.utils.data import Dataset,DataLoader
from xclib.data import data_utils as du
from xclib.utils.sparse import retain_topk
from transformers import PreTrainedTokenizerBase, AutoTokenizer

from fastcore.utils import *
from fastcore.meta import *
from plum import dispatch

from .core import *
from .graph.operations import *

# %% ../nbs/02_data.ipynb 8
def _read_sparse_file(fname:Optional[str]=None):
    if fname is None: return
    elif fname.endswith('.txt'): return du.read_sparse_file(fname)
    elif fname.endswith('.npz'): return sparse.load_npz(fname)
    else: raise ValueError(f'Invalid file extension : {fname}')
    

# %% ../nbs/02_data.ipynb 9
class MainXCData:
    
    @classmethod
    @delegates(Info.from_txt)
    def from_file(cls, data_info:str, lbl_info:Optional[Union[str, Dict]]=None, data_lbl:Optional[str]=None, data_lbl_filterer:Optional[str]=None, 
                  main_max_data_sequence_length:Optional[int]=None, main_max_lbl_sequence_length:Optional[int]=None, **kwargs):
        return {
            'data_lbl': _read_sparse_file(data_lbl),
            'data_info': Info.from_txt(data_info, max_sequence_length=main_max_data_sequence_length, **kwargs),
            'lbl_info': Info.from_txt(lbl_info, max_sequence_length=main_max_lbl_sequence_length, **kwargs) if isinstance(lbl_info, str) else lbl_info,
            'data_lbl_filterer': Filterer.load_filter(data_lbl_filterer),
        }
    

# %% ../nbs/02_data.ipynb 11
class MetaXCData:
    
    @classmethod
    @delegates(Info.from_txt)
    def from_file(cls, prefix:str, data_meta:str, lbl_meta:Optional[str]=None, meta_info:Optional[str]=None, 
                  meta_max_sequence_length:Optional[int]=None, **kwargs):
        return {
            'prefix': prefix,
            'data_meta': _read_sparse_file(data_meta),
            'lbl_meta': _read_sparse_file(lbl_meta),
            'meta_info': Info.from_txt(meta_info, max_sequence_length=meta_max_sequence_length, **kwargs) if isinstance(meta_info, str) else meta_info,
        }
    

# %% ../nbs/02_data.ipynb 17
class BaseXCDataset(Dataset):
    def __init__(self):
        self.n_data, self.n_lbl, self.n_meta, self.n_samples = None, None, None, None
        
    def __len__(self):
        return self.n_data if self.n_data is not None else 0

    def splitter(self, valid_pct:Optional[float]=0.2, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(self.n_data).numpy())
        cut = int(valid_pct * self.n_data)
        train, valid = self._getitems(rnd_idx[cut:]), self._getitems(rnd_idx[:cut])
        return train, valid

    def sample(self, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(self.n_data).numpy())
        cut = int(pct * self.n_data) if n is None else max(1, n)
        return self._getitems(rnd_idx[:cut])
        
    def _verify_info(self, info:Dict):
        if info is None: raise ValueError('`info` cannot be empty.')
        n_info = [len(v) for k,v in info.items()]
        if len(n_info) == 0 or n_info[0] == 0: raise ValueError('`info` cannot be empty.')
        if np.all([n_info[0] == o for o in n_info]) == False: 
            raise ValueError('All `data_info` fields should have equal number of elements.')
        return n_info[0]
        
    def show_data(self, n:Optional[int]=10, seed:Optional[int]=None):
        if n < 1: return
        if seed: np.random.seed(seed)
        idx = np.random.permutation(self.n_data)[:n]
        d = [self[i] for i in idx]
        df = pd.DataFrame({k:[o[k] for o in d] for k in d[0]})
        with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):
            display(df)

    def prune_data_lbl(self, data_lbl:sparse.csr_matrix, data_repr:torch.Tensor, lbl_repr:torch.Tensor, batch_size:Optional[int]=64, 
                       thresh:Optional[float]=0.1, topk:Optional[int]=None):
        data_repr,lbl_repr = F.normalize(data_repr, dim=1), F.normalize(lbl_repr, dim=1)
        curr_data_lbl = data_lbl.copy()
        rows, cols = data_lbl.nonzero()
        dl = DataLoader(list(zip(rows, cols)), batch_size=batch_size, shuffle=False)
        score = None
        for b in tqdm(dl, total=len(dl)): 
            sc = data_repr[b[0]].unsqueeze(1)@lbl_repr[b[1]].unsqueeze(2)
            sc = sc.squeeze()
            sc = torch.where(sc < thresh, 0, sc)
            score = sc if score is None else torch.hstack([score, sc])
        curr_data_lbl.data[:] = score
        curr_data_lbl.eliminate_zeros()
        if topk is not None: 
            curr_data_lbl = retain_topk(curr_data_lbl, k=topk)
        return curr_data_lbl

    @staticmethod
    def get_info(prefix:str, idxs:List, info:Dict, info_keys:List):
        output = dict()
        for k,v in info.items():
            if k in info_keys:
                if isinstance(v, np.ndarray) or isinstance(v, torch.Tensor):
                    o = v[idxs]
                    if isinstance(o, np.ndarray): o = torch.from_numpy(o)
                    output[f'{prefix}_{k}'] = o
                else:
                    output[f'{prefix}_{k}'] = [v[idx] for idx in idxs]
        return output

    @classmethod
    def _initialize(cls, dset):
        kwargs = {k: getattr(dset, k) for k in [o for o in vars(dset).keys() if not o.startswith('__')]}
        return cls(**kwargs)
        

# %% ../nbs/02_data.ipynb 19
class MainXCDataset(BaseXCDataset):
    def __init__(
        self,
        data_info:Dict,
        data_lbl:Optional[sparse.csr_matrix]=None,
        lbl_info:Optional[Dict]=None,
        data_lbl_filterer:Optional[Union[sparse.csr_matrix,np.array]]=None,
        n_lbl_samples:Optional[int]=None,
        data_info_keys:Optional[List]=None,
        lbl_info_keys:Optional[List]=None,
        enable_delayed_indexing:Optional[bool]=False,
        **kwargs
    ):
        super().__init__()
        store_attr('data_info,data_lbl,lbl_info,data_lbl_filterer,n_lbl_samples,data_info_keys')
        store_attr('lbl_info_keys,enable_delayed_indexing')
        self.curr_data_lbl = None
        
        self._verify_inputs()
        if not enable_delayed_indexing: self._store_indices()
        
    @classmethod
    @delegates(MainXCData.from_file)
    def from_file(cls, **kwargs):
        data_args = {o: kwargs.pop(o) for o in list(inspect.signature(MainXCData.from_file).parameters) if o in kwargs}
        return cls(**MainXCData.from_file(**data_args), **kwargs)

    def _verify_inputs(cls):
        cls.n_data = cls._verify_info(cls.data_info)
        if cls.data_info_keys is None: cls.data_info_keys = list(cls.data_info.keys())
        if cls.data_lbl is not None:
            if cls.n_data != cls.data_lbl.shape[0]:
                raise ValueError(f'`data_info`({cls.n_data}) should have same number of datapoints as `data_lbl`({cls.data_lbl.shape[0]})')
            cls.n_lbl = cls.data_lbl.shape[1]
            if cls.lbl_info is not None:
                n_lbl = cls._verify_info(cls.lbl_info)
                if n_lbl != cls.data_lbl.shape[1]:
                    raise ValueError(f'`lbl_info`({n_lbl}) should have same number of labels as `data_lbl`({cls.data_lbl.shape[1]})')
                if cls.lbl_info_keys is None: cls.lbl_info_keys = list(cls.lbl_info.keys())
        elif cls.lbl_info is not None:
            cls.n_lbl = cls._verify_info(cls.lbl_info)
            if cls.lbl_info_keys is None: cls.lbl_info_keys = list(cls.lbl_info.keys())

    def _store_indices(self):
        if self.data_lbl is not None: self.curr_data_lbl = [o.indices.tolist() for o in self.data_lbl]

    def enable_indexing(self):
        self.enable_delayed_indexing = True
        self._store_indices()

    def _get_dataset(
        self, 
        data_info:Dict,  
        **kwargs
    ):
        args = [o for o in vars(self).keys() if not o.startswith('__')]
        kwargs = {k: kwargs.get(k, getattr(self, k)) for k in args}
        kwargs['data_info'] = data_info
        return type(self)(**kwargs)

    def _getitems(self, idxs:List):
        kwargs = {k:getattr(self, k) for k in [o for o in vars(self).keys() if not o.startswith('__')]}
        kwargs['data_info'] = {k:v[idxs] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[idx] for idx in idxs] for k,v in self.data_info.items()}
        kwargs['data_lbl'] = self.data_lbl[idxs] if self.data_lbl is not None else None
        kwargs['data_lbl_filterer'] = Filterer.sample_x(self.data_lbl_filterer, sz=self.data_lbl.shape, idx=idxs) if self.data_lbl_filterer is not None else None
        return type(self)(**kwargs)

    def _getlabels(self, idxs:List):
        kwargs = {k:getattr(self, k) for k in [o for o in vars(self).keys() if not o.startswith('__')]}
        kwargs['lbl_info'] = {k:v[idxs] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[idx] for idx in idxs] for k,v in self.lbl_info.items()}
        kwargs['data_lbl'] = self.data_lbl[:, idxs] if self.data_lbl is not None else None
        kwargs['data_lbl_filterer'] = Filterer.sample_y(self.data_lbl_filterer, sz=self.data_lbl.shape, idx=idxs) if self.data_lbl_filterer is not None else None
        return type(self)(**kwargs)
        

# %% ../nbs/02_data.ipynb 20
@patch
def __getitem__(cls:MainXCDataset, idx:int):
    x = {f'data_{k}': v[idx] for k,v in cls.data_info.items() if k in cls.data_info_keys}
    x['data_idx'] = idx
    if cls.data_lbl is not None:
        prefix = 'lbl2data'
        x[f'{prefix}_idx'] = cls.curr_data_lbl[idx]
        if cls.n_lbl_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:cls.n_lbl_samples]]
        if cls.lbl_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in cls.lbl_info.items() if k in cls.lbl_info_keys})
    return x
    

# %% ../nbs/02_data.ipynb 30
class MetaXCDataset(BaseXCDataset):

    def __init__(self,
                 prefix:str,
                 data_meta:sparse.csr_matrix, 
                 lbl_meta:Optional[sparse.csr_matrix]=None, 
                 meta_info:Optional[Dict]=None, 
                 n_data_meta_samples:Optional[int]=None,
                 n_lbl_meta_samples:Optional[int]=None,
                 meta_info_keys:Optional[List]=None,
                 enable_delayed_indexing:Optional[bool]=False,
                 **kwargs):
        store_attr('prefix,data_meta,lbl_meta,meta_info,n_data_meta_samples')
        store_attr('n_lbl_meta_samples,meta_info_keys,enable_delayed_indexing')
        
        self.curr_data_meta,self.curr_lbl_meta = None,None
        self._verify_inputs()
        
        if not enable_delayed_indexing: self._store_indices()

    @classmethod
    @delegates(MetaXCData.from_file)
    def from_file(cls, **kwargs):
        data_args = {o: kwargs.pop(o) for o in list(inspect.signature(MetaXCData.from_file).parameters) if o in kwargs}
        return cls(**MetaXCData.from_file(**data_args), **kwargs)

    def _verify_inputs(cls):
        cls.n_data,cls.n_meta = cls.data_meta.shape[0],cls.data_meta.shape[1]

        cls.n_lbl = None
        if cls.lbl_meta is not None:
            cls.n_lbl = cls.lbl_meta.shape[0]
            if cls.lbl_meta.shape[1] != cls.n_meta:
                raise ValueError(f'`lbl_meta`({cls.lbl_meta.shape[1]}) should have same number of columns as `data_meta`({cls.n_meta}).')
    
        if cls.meta_info is not None:
            n_meta = cls._verify_info(cls.meta_info)
            if n_meta != cls.n_meta:
                raise ValueError(f'`meta_info`({n_meta}) should have same number of entries as number of columns of `data_meta`({cls.n_meta})')
            if cls.meta_info_keys is None: cls.meta_info_keys = list(cls.meta_info.keys())
                
    def _store_indices(self):
        if self.data_meta is not None: self.curr_data_meta = [o.indices.tolist() for o in self.data_meta]
        if self.lbl_meta is not None: self.curr_lbl_meta = [o.indices.tolist() for o in self.lbl_meta]

    def enable_indexing(self):
        self.enable_delayed_indexing = True
        self._store_indices()

    def prune_data_meta(self, data_repr:torch.Tensor, meta_repr:torch.Tensor, batch_size:Optional[int]=64, thresh:Optional[float]=0.0, 
                        topk:Optional[int]=None):
        assert self.data_meta is not None, "`self.data_meta` is empty."
        data_meta = self.prune_data_lbl(self.data_meta, data_repr, meta_repr, batch_size, thresh, topk)
        self.curr_data_meta = [o.indices.tolist() for o in data_meta]

    def prune_lbl_meta(self, lbl_repr:torch.Tensor, meta_repr:torch.Tensor, batch_size:Optional[int]=64, thresh:Optional[float]=0.0, 
                       topk:Optional[int]=None):
        assert self.lbl_meta is not None, "`self.lbl_meta` is empty."
        lbl_meta = self.prune_data_lbl(self.lbl_meta, lbl_repr, meta_repr, batch_size, thresh, topk)
        self.curr_lbl_meta = [o.indices.tolist() for o in lbl_meta]

    def update_meta_matrix(self, data_meta:sparse.csr_matrix, lbl_meta:Optional[sparse.csr_matrix]=None):
        self.data_meta, self.lbl_meta = data_meta, lbl_meta
        if not self.enable_delayed_indexing: self._store_indices()

    def _getitems(self, idxs:List):
        kwargs = {k:getattr(self, k) for k in [o for o in vars(self).keys() if not o.startswith('__')]}
        kwargs['data_meta'] = self.data_meta[idxs]
        return type(self)(**kwargs)

    def _sample_meta_items(self, idxs:List):
        assert max(idxs) < self.n_meta, f"indices should be less than {self.n_meta}"
        kwargs = {k:getattr(self, k) for k in [o for o in vars(self).keys() if not o.startswith('__')]}
        kwargs['meta_info'] = {k:v[idxs] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[idx] for idx in idxs] for k,v in self.meta_info.items()}
        kwargs['data_meta'] = self.data_meta[:, idxs]
        kwargs['lbl_meta'] = None if self.lbl_meta is None else self.lbl_meta[:, idxs]
        return type(self)(**kwargs)

    def _getlabels(self, idxs:List): return self._sample_meta_items(idxs)
        
    @dispatch
    def get_lbl_meta(self, idx:int):
        if self.curr_lbl_meta is None: return {}
        prefix = f'{self.prefix}2lbl2data'
        x = {f'{prefix}_idx': self.curr_lbl_meta[idx]}
        if self.n_lbl_meta_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:self.n_lbl_meta_samples]]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x
    
    @dispatch
    def get_lbl_meta(self, idxs:List):
        if self.curr_lbl_meta is None: return {}
        prefix = f'{self.prefix}2lbl2data'
        x = {f'{prefix}_idx': [self.curr_lbl_meta[idx] for idx in idxs]}
        if self.n_lbl_meta_samples: x[f'{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:self.n_lbl_meta_samples]] for o in x[f'{prefix}_idx']]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[[v[i] for i in o] for o in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x
        
    def get_data_meta(self, idx:int):
        prefix = f'{self.prefix}2data'
        x = {f'{prefix}_idx': self.curr_data_meta[idx]}
        if self.n_data_meta_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:self.n_data_meta_samples]]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x

    def shape(self): return (self.n_data, self.n_lbl, self.n_meta)
        
    def show_data(self, is_lbl:Optional[bool]=False, n:Optional[int]=10, seed:Optional[int]=None):
        if n < 1: return
        if seed: np.random.seed(seed)
        idx = np.random.permutation(self.n_lbl if is_lbl else self.n_data)[:n]
        d = [self.get_lbl_meta(int(i)) for i in idx] if is_lbl else [self.get_data_meta(i) for i in idx]
        df = pd.DataFrame({k:[o[k] for o in d] for k in d[0]})
        with pd.option_context('display.max_colwidth', None):
            display(df)
    

# %% ../nbs/02_data.ipynb 44
class Operations:

    @staticmethod
    def combined_lbl_and_meta(dset, meta_name:str, pad_token:Optional[int]=0, p_data:Optional[float]=0.5, **kwargs):
        assert meta_name in dset.meta, f'Invalid metadata: {meta_name}'
            
        data_lbl = dset.data.data_lbl
        data_lbl = data_lbl.multiply(1/(data_lbl.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_lbl = data_lbl.tocsr() * p_data
        
        data_meta = dset.meta[meta_name].data_meta
        data_meta = data_meta.multiply(1/(data_meta.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_meta = data_meta.tocsr() * (1 - p_data)
    
        data_info, lbl_info, meta_info = dset.data.data_info, dset.data.lbl_info, dset.meta[meta_name].meta_info 
        
        combined_info = dict()
        for k,v in lbl_info.items():
            if isinstance(v, tuple) or isinstance(v, list): 
                combined_info[k] = v + meta_info[k]
            elif isinstance(v, torch.Tensor):
                n_data = v.shape[0] + meta_info[k].shape[0]
                seq_len = max(v.shape[1], meta_info[k].shape[1]) 
                
                if k == 'input_ids': 
                    mat = torch.full((n_data, seq_len), pad_token, dtype=v.dtype)
                elif k == 'attention_mask': 
                    mat = torch.full((n_data, seq_len), 0, dtype=v.dtype)
                else:
                    raise ValueError(f'Invalid tensor present in info: {k}')
                    
                mat[:v.shape[0], :v.shape[1]] = v
                mat[v.shape[0]:, :meta_info[k].shape[1]] = meta_info[k]
                combined_info[k] = mat
                
        args = [o for o in vars(dset.data).keys() if not o.startswith('__')]
        args = {k: kwargs.get(k, getattr(dset.data, k)) for k in args}
        
        data_dset = type(dset.data)(data_info, sp.hstack([data_lbl, data_meta]), combined_info, 
                                    dset.data.data_lbl_filterer, **kwargs)
        return type(dset)(data_dset)

    @staticmethod
    def retain_randomk(matrix:sparse.csr_matrix, topk:Optional[int]=3):
        data, indices, indptr = [], [], np.zeros_like(matrix.indptr)
        for i,row in tqdm(enumerate(matrix), total=matrix.shape[0]):
            if row.nnz > 0:
                idxs = np.random.randint(row.nnz, size=topk)
                ind, d = row.indices[idxs], row.data[idxs]    
            else:
                ind, d = np.arange(topk), np.zeros(topk)
                
            indptr[i+1] = indptr[i] + topk
            indices.append(ind); data.append(d)
            
        data, indices = np.hstack(data), np.hstack(indices)
        output = sparse.csr_matrix((data, indices, indptr), shape=matrix.shape, dtype=matrix.dtype)
        output.eliminate_zeros()
        output.sort_indices()
        return output

    @staticmethod
    def remove_data(mat_a:sparse.csr_matrix, mat_b:sparse.csr_matrix, pct:Optional[float]=0.3):
        n_data = min(len(mat_a.data),len(mat_b.data))
        n = int(n_data * pct)
        idx = np.random.permutation(n_data)
        idx_a, idx_b = idx[:n], idx[n:]
        mat_a.data[idx_a] = 0; mat_a.eliminate_zeros()
        mat_b.data[idx_b] = 0; mat_b.eliminate_zeros()

    @staticmethod
    def mix_matrix(mat_a:sparse.csr_matrix, mat_b:sparse.csr_matrix, pct:Optional[float]=0.3, 
                   k:Optional[int]=3):
        mat_a = Operations.retain_randomk(mat_a, topk=k)
        mat_b = Operations.retain_randomk(mat_b, topk=k)
        Operations.remove_data(mat_a, mat_b, pct)
        return mat_a + mat_b

    @staticmethod
    def mix_meta_dataset(dset, meta_name_a:str, meta_name_b:str, pct:Optional[float]=0.3, 
                         k:Optional[int]=3, **kwargs):
        meta_info = dset.meta[meta_name_a].meta_info
        
        if pct < 1:
            mat_a, mat_b = dset.meta[meta_name_a].data_meta, dset.meta[meta_name_b].data_meta
            data_meta = None if mat_a is None or mat_b is None else Operations.mix_matrix(mat_a, mat_b, pct=pct, k=k)

            mat_a, mat_b = dset.meta[meta_name_a].lbl_meta, dset.meta[meta_name_b].lbl_meta
            lbl_meta = None if mat_a is None or mat_b is None else Operations.mix_matrix(mat_a, mat_b, pct=pct, k=k)
        else:
            data_meta, lbl_meta = dset.meta[meta_name_b].data_meta, dset.meta[meta_name_b].lbl_meta

        args = [o for o in vars(dset.meta[meta_name_b]).keys() if not o.startswith('__')]
        args = {k: kwargs.get(k, getattr(dset.meta[meta_name_b], k)) for k in args}
        
        dset.meta['hyb_meta'] = type(dset.meta[meta_name_b])(prefix='hyb', data_meta=data_meta, 
                                                             lbl_meta=lbl_meta, meta_info=meta_info, **args)
        
    @staticmethod
    def get_random_walk_matrix(data_lbl:sp.csr_matrix, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, 
                               prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, degree_thresh=20):
        data_meta = perform_random_walk(data_lbl, batch_size=batch_size, walk_to=walk_to, 
                                        prob_reset=prob_reset, n_hops=1, thresh=degree_thresh, 
                                        topk=topk_thresh, do_normalize=True)
        lbl_meta = perform_random_walk(data_lbl.transpose().tocsr(), batch_size=batch_size, walk_to=walk_to, 
                                       prob_reset=prob_reset, n_hops=2, thresh=degree_thresh, topk=topk_thresh, 
                                       do_normalize=True)
        return data_meta, lbl_meta
        
    @staticmethod
    def get_random_walk_with_matrices(data_meta:sp.csr_matrix, lbl_meta:sp.csr_matrix, batch_size:Optional[int]=1024, 
                                      walk_to:Optional[int]=100, prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, 
                                      data_degree_thresh=20, lbl_degree_thresh=20):
        data_rnw = perform_random_walk_with_matrices(data_meta, lbl_meta, batch_size=batch_size, walk_to=walk_to, 
                                                     prob_reset=prob_reset, n_hops=2, data_thresh=data_degree_thresh, 
                                                     lbl_thresh=lbl_degree_thresh, topk=topk_thresh, do_normalize=True)
        lbl_rnw = perform_random_walk_with_matrices(lbl_meta, data_meta, batch_size=batch_size, walk_to=walk_to, 
                                                    prob_reset=prob_reset, n_hops=3, data_thresh=data_degree_thresh, 
                                                    lbl_thresh=lbl_degree_thresh, topk=topk_thresh, do_normalize=True)
        return data_rnw, lbl_rnw
        
    @staticmethod
    def combine_info(info_a:Dict, info_b:Dict, pad_token:Optional[int]=0):
        combined_info = dict()
        for k,v in info_a.items():
            if isinstance(v, tuple) or isinstance(v, list): 
                combined_info[k] = v + info_b[k]
            elif isinstance(v, torch.Tensor):
                n_data = v.shape[0] + info_b[k].shape[0]
                seq_len = max(v.shape[1], info_b[k].shape[1]) 
                
                if k == 'input_ids': 
                    mat = torch.full((n_data, seq_len), pad_token, dtype=v.dtype)
                elif k == 'attention_mask': 
                    mat = torch.full((n_data, seq_len), 0, dtype=v.dtype)
                    
                mat[:v.shape[0], :v.shape[1]] = v
                mat[v.shape[0]:, :info_b[k].shape[1]] = info_b[k]
                combined_info[k] = mat
                
        return combined_info

    @staticmethod
    def combine_lbl_and_meta(dset, meta_name:str, pad_token:Optional[int]=0, p_data:Optional[float]=0.5, **kwargs):
        assert meta_name in dset.meta, f'Invalid metadata: {meta_name}'
            
        data_lbl = dset.data.data_lbl
        data_lbl = data_lbl.multiply(1/(data_lbl.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_lbl = data_lbl.tocsr() * p_data
        
        data_meta = dset.meta[meta_name].data_meta
        data_meta = data_meta.multiply(1/(data_meta.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_meta = data_meta.tocsr() * (1 - p_data)
        
        lbl_info, meta_info = dset.data.lbl_info, self.meta[meta_name].meta_info
        combined_info = Operations.combine_info(lbl_info, meta_info, pad_token)
        
        return dset._get_main_dataset(dset.data.data_info, sp.hstack([data_lbl, data_meta]), combined_info, 
                                      dset.data.data_lbl_filterer, **kwargs)
        
    @staticmethod
    def combine_data_and_meta(dset, meta_name:str, pad_token:Optional[int]=0, **kwargs):
        assert meta_name in dset.meta, f'Invalid metadata: {meta_name}'
        
        data_lbl, meta_lbl = dset.data.data_lbl, dset.meta[meta_name].lbl_meta.transpose().tocsr()
        assert data_lbl.shape[1] == meta_lbl.shape[1], f"Incompatible metadata shape: {meta_lbl.shape}"

        data_info, meta_info = dset.data.data_info, dset.meta[meta_name].meta_info
        combined_info = Operations.combine_info(data_info, meta_info, pad_token)

        dset = dset._get_main_dataset(combined_info, sp.vstack([data_lbl, meta_lbl]), dset.data.lbl_info, 
                                      dset.data.data_lbl_filterer, **kwargs)
        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]
        return dset._getitems(valid_idx)

    @staticmethod
    def get_combined_data_and_meta(dset, meta_lbl:sp.csr_matrix, meta_info:Dict, pad_token:Optional[int]=0, **kwargs):    
        data_lbl = dset.data.data_lbl
        assert data_lbl.shape[1] == meta_lbl.shape[1], f"Incompatible metadata shape: {meta_lbl.shape}"
        
        data_info = dset.data.data_info
        combined_info = dset.combine_info(data_info, meta_info, pad_token)
        
        dset = dset._get_main_dataset(combined_info, sp.vstack([data_lbl, meta_lbl]), dset.data.lbl_info, 
                                      dset.data.data_lbl_filterer, **kwargs)
        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]
        return dset._getitems(valid_idx)
        

# %% ../nbs/02_data.ipynb 46
class MetaXCDatasets(dict):

    def __init__(self, meta:Dict):
        super().__init__(meta)
        for o in meta: setattr(self, o, meta[o])

    def __setitem__(self, key, value):
        super().__setitem__(key, value)
        setattr(self, key, value)

    def __delitem__(self, key):
        super().__delitem__(key)
        delattr(self, key)
        

# %% ../nbs/02_data.ipynb 47
class XCDataset(BaseXCDataset):

    def __init__(self, data:MainXCDataset, **kwargs):
        super().__init__()
        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], MetaXCDataset)})
        self._verify_inputs()

    @staticmethod
    def get_meta_args(**kwargs): return [k for k in kwargs if re.match(r'.*_meta$', k)]

    @classmethod
    @delegates(MainXCDataset.from_file)
    def from_file(cls, **kwargs):
        data = MainXCDataset.from_file(**kwargs)
        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}
        meta = {k:MetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}
        return cls(data, **meta)

    def _verify_inputs(self):
        self.n_data, self.n_lbl = self.data.n_data, self.data.n_lbl
        if len(self.meta):
            self.n_meta = len(self.meta)
            for meta in self.meta.values():
                if meta.n_data != self.n_data: 
                    raise ValueError(f'`meta`({meta.n_data}) and `data`({self.n_data}) should have the same number of datapoints.')
                if self.n_lbl is not None and meta.n_lbl is not None and meta.n_lbl != self.n_lbl: 
                    raise ValueError(f'`meta`({meta.n_lbl}) and `data`({self.n_lbl}) should have the same number of labels.')

    def enable_indexing(self):
        self.data.enable_indexing()
        for meta_name in self.meta: self.meta[meta_name].enable_indexing()

    @classmethod
    def _initialize(cls, dset):
        return cls(MainXCDataset._initialize(dset.data), **{k:MetaXCDataset._initialize(meta) for k,meta in dset.meta.items()})

    def _getitems(self, idxs:List):
        return type(self)(self.data._getitems(idxs), **{k:meta._getitems(idxs) for k,meta in self.meta.items()})

    def _getlabels(self, idxs:List):
        return type(self)(self.data._getlabels(idxs), **{k:meta._getlabels(idxs) for k,meta in self.meta.items()})

    def get_valid_dset(self):
        idxs = np.where(self.data.data_lbl.getnnz(axis=1) > 0)[0]
        return self._getitems(idxs)

    def _get_main_dataset(self, data_info:Dict, data_lbl:Optional[sp.csr_matrix]=None, lbl_info:Optional[Dict]=None, 
                          data_lbl_filterer:Optional[Union[sp.csr_matrix,np.array]]=None, **kwargs):
        dset = self.data._get_dataset(data_info, data_lbl, lbl_info, data_lbl_filterer, **kwargs)
        return type(self)(dset)

    @property
    def lbl_info(self): return self.data.lbl_info

    @property
    def lbl_dset(self):
        kwargs = {k:getattr(self.data, k) for k in [o for o in vars(self.data).keys() if not o.startswith('__')]}
        for k in ['data_lbl', 'lbl_info', 'data_lbl_filterer']: kwargs.pop(k, None)
        kwargs['data_info'] = self.data.lbl_info
        return type(self.data)(**kwargs)

    def lbl_meta_dset(self, meta_name):
        meta_dset = self.meta[f'{meta_name}_meta']
        kwargs = {k:getattr(meta_dset, k) for k in [o for o in vars(meta_dset).keys() if not o.startswith('__')]}
        kwargs['data_meta'] = meta_dset.lbl_meta
        kwargs['lbl_meta'] = None
        return type(meta_dset)(**kwargs)

    @property
    def data_info(self): return self.data.data_info

    @property
    def data_dset(self):
        kwargs = {k:getattr(self.data, k) for k in [o for o in vars(self.data).keys() if not o.startswith('__')]}
        for k in ['data_lbl', 'lbl_info', 'data_lbl_filterer']: kwargs.pop(k, None)
        return type(self.data)(**kwargs)

    def data_meta_dset(self, meta_name):
        return self.meta[f'{meta_name}_meta']
        
    def __getitem__(self, idx:int):
        x = self.data[idx]
        if self.n_meta:
            for m in self.meta.values():
                x.update(m.get_data_meta(idx))
                if self.n_lbl: x.update(m.get_lbl_meta(x['lbl2data_idx']))
        return x

    def one_batch(self, bsz:Optional[int]=10, seed:Optional[int]=None):
        if seed is not None: torch.manual_seed(seed)
        idxs = list(torch.randperm(len(self)).numpy())[:bsz]
        return [self[idx] for idx in idxs]

    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, 
                             topk:Optional[int]=10, **kwargs):
        data_lbl = Graph.threshold_on_degree(self.data.data_lbl, thresh=thresh)
        data_meta, lbl_meta = Graph.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk, 
                                                   do_normalize=True)
        
        self.meta['ohm_meta'] = MetaXCDataset(prefix='ohm', data_meta=data_meta, lbl_meta=lbl_meta, 
                                              meta_info=dset.data.lbl_info, **kwargs)

    def get_data_lbl_random_walk_metadata(self, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, 
                                          prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, 
                                          degree_thresh=20, **kwargs):
        data_meta, lbl_meta = Operations.get_random_walk_matrix(self.data.data_lbl, batch_size, walk_to, 
                                                                prob_reset, topk_thresh, degree_thresh)
        self.meta['rnw_meta'] = MetaXCDataset(prefix='rnw', data_meta=data_meta, lbl_meta=lbl_meta,
                                              meta_info=self.data.lbl_info, **kwargs)

    def get_data_meta_random_walk_metadata(self, meta_name:str, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, 
                                           prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, 
                                           data_degree_thresh:Optional[int]=20, lbl_degree_thresh:Optional[int]=20, 
                                           **kwargs):
        assert meta_name in self.meta, f'Invalid metadata: {meta_name}'
        data_meta, lbl_meta = self.meta[meta_name].data_meta, self.meta[meta_name].lbl_meta
        data_rnw, lbl_rnw = Operations.get_random_walk_with_matrices(data_meta, lbl_meta, batch_size, walk_to, prob_reset, 
                                                                     topk_thresh, data_degree_thresh, lbl_degree_thresh)
        
        args = [o for o in vars(self.meta[meta_name]).keys() if not o.startswith('__')]
        kwargs = {k: kwargs.get(k, getattr(self, k)) for k in args}
        
        self.meta['rnw_meta'] = type(self.meta[meta_name])(prefix='rnw', data_meta=data_rnw, lbl_meta=lbl_rnw, 
                                                           meta_info=self.data.lbl_info, **kwargs)


# %% ../nbs/02_data.ipynb 59
class XCCollator:

    def __init__(self, tfms):
        self.tfms = tfms

    def __call__(self, x):
        return self.tfms(x)
        

# %% ../nbs/02_data.ipynb 76
class BaseXCDataBlock:

    @delegates(DataLoader.__init__)
    def __init__(
        self, 
        dset, 
        collate_fn:Optional[Callable]=None,
        **kwargs
    ):
        self.dset, self.dl_kwargs, self.collate_fn = dset, self.get_dl_kwargs(**kwargs), collate_fn
        self.dl = DataLoader(dset, collate_fn=collate_fn, **self.dl_kwargs) if collate_fn is not None else None

    @staticmethod
    def get_dl_kwargs(**kwargs):
        dl_params = inspect.signature(DataLoader.__init__).parameters
        return {k:v for k,v in kwargs.items() if k in dl_params}
        
    @classmethod
    @delegates(XCDataset.from_file)
    def from_file(cls, collate_fn:Callable=None, **kwargs):
        return cls(XCDataset.from_file(**kwargs), collate_fn, **kwargs)

    def enable_indexing(self):
        self.dset.enable_indexing()

    @classmethod
    def _initialize(cls, dset):
        return cls(XCDataset._initialize(dset.dset), collate_fn=dset.collate_fn, **dset.dl_kwargs) if dset is not None else None

    def _getitems(self, idxs:List):
        return type(self)(self.dset._getitems(idxs), collate_fn=self.collate_fn, **self.dl_kwargs)

    def _getlabels(self, idxs:List):
        return type(self)(self.dset._getlabels(idxs), collate_fn=self.collate_fn, **self.dl_kwargs)

    def get_valid_dset(self):
        return BaseXCDataBlock(self.dset.get_valid_dset(), collate_fn=self.collate_fn, **self.dl_kwargs)
        
    def __len__(self):
        return len(self.dset)

    @property
    def bsz(self): return self.dl.batch_size

    @bsz.setter
    def bsz(self, v):
        self.dl_kwargs['batch_size'] = v
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None

    @property
    def data_lbl_filterer(self): return self.dset.data.data_lbl_filterer

    @data_lbl_filterer.setter
    def data_lbl_filterer(self, val): self.dset.data.data_lbl_filterer = val

    @dispatch
    def one_batch(self):
        return next(iter(self.dl))

    @dispatch
    def one_batch(self, bsz:int):
        self.dl_kwargs['batch_size'] = bsz
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None
        return next(iter(self.dl))

    def filterer(cls, train, valid, fld:Optional[str]='identifier'):
        train_info, valid_info, lbl_info = train.dset.data.data_info, valid.dset.data.data_info, train.dset.data.lbl_info
        if fld not in train_info: raise ValueError(f'`{fld}` not in `data_info`')
            
        train.data_lbl_filterer, valid_filterer = Filterer.generate(train_info[fld], valid_info[fld], lbl_info[fld], 
                                                                    train.dset.data.data_lbl, valid.dset.data.data_lbl)
        _, valid_filterer, idx = Filterer.prune(valid.dset.data.data_lbl, valid_filterer)
        
        valid = valid._getitems(idx)
        valid.data_lbl_filterer = valid_filterer
        
        return train, valid
        
    def splitter(cls, valid_pct:Optional[float]=0.2, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(len(cls)).numpy())
        cut = int(valid_pct * len(cls))
        train, valid = cls._getitems(rnd_idx[cut:]), cls._getitems(rnd_idx[:cut])
        if cls.data_lbl_filterer is None: return train, valid
        else: return cls.filterer(train, valid)

    def sample(cls, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(len(cls)).numpy())
        cut = int(pct * len(cls)) if n is None else max(1, n)
        return cls._getitems(rnd_idx[:cut])
        

# %% ../nbs/02_data.ipynb 86
class XCDataBlock:

    def __init__(self, train=None, valid=None, test=None):
        self.train, self.valid, self.test = train, valid, test

    def enable_indexing(self):
        if self.train: self.train.enable_indexing()
        if self.valid: self.valid.enable_indexing()
        if self.test: self.test.enable_indexing()

    @classmethod
    def _initialize(cls, dset):
        return cls(BaseXCDataBlock._initialize(dset.train), BaseXCDataBlock._initialize(dset.valid), BaseXCDataBlock._initialize(dset.test))

    def get_valid_data_block(self):
        if self.train: self.train = self.train.get_valid_dset()
        if self.valid: self.valid = self.valid.get_valid_dset()
        if self.test: self.test = self.test.get_valid_dset()
            
    def get_valid_label_block(self):
        if self.train:
            valid_idx = np.where(self.train.dset.data.data_lbl.getnnz(axis=0) > 0)[0]
            self.train = self.train._getlabels(valid_idx)
            if self.valid: self.valid = self.valid._getlabels(valid_idx)
            if self.test: self.test = self.test._getlabels(valid_idx)

    def get_valid_block(self):
        self.get_valid_data_block()
        self.get_valid_label_block()
        
    @staticmethod
    def load_cfg(fname):
        with open(fname, 'r') as f: return json.load(f)

    @property
    def lbl_info(self): return self.test.dset.data.lbl_info if self.train is None else self.train.dset.data.lbl_info

    @property
    def lbl_dset(self): return type(self.train.dset.data)(data_info=self.train.dset.data.lbl_info)

    @property
    def n_lbl(self): return self.test.dset.n_lbl if self.train is None else self.train.dset.n_lbl

    @property
    def collator(self): return self.test.collate_fn if self.train is None else self.train.collate_fn

    def sample_info(self, info, idx):
        return {k: v[idx] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[i] for i in idx] for k,v in info.items()}

    def linker_dset(self, meta_name:str, remove_empty:Optional[bool]=True):
        if meta_name not in self.train.dset.meta:
            raise ValueError(f'Invalid metadata: {meta_name}')

        train_meta = self.train.dset.meta[meta_name].data_meta
        if remove_empty:
            train_idx = np.where(train_meta.getnnz(axis=1) > 0)[0]
            meta_idx = np.where(train_meta.getnnz(axis=0) > 0)[0]
            train_meta = train_meta[train_idx][:, meta_idx].tocsr()
        
        train_info = self.train.dset.data.data_info
        meta_info = self.train.dset.meta[meta_name].meta_info

        if remove_empty:
            train_info = self.sample_info(train_info, train_idx)
            meta_info = self.sample_info(meta_info, meta_idx)

        collate_fn = self.train.collate_fn
        train_dset = type(self.train)(type(self.train.dset)(type(self.train.dset.data)(data_info=train_info, data_lbl=train_meta, lbl_info=meta_info)), 
                                      collate_fn=collate_fn)
        
        if self.test is not None:
            test_meta = self.test.dset.meta[meta_name].data_meta
            if remove_empty:
                test_meta = test_meta[:, meta_idx].tocsr()
                test_idx = np.where(test_meta.getnnz(axis=1) > 0)[0]
                test_meta = test_meta[test_idx].tocsr()
    
            test_info = self.test.dset.data.data_info
            if remove_empty: test_info = self.sample_info(test_info, test_idx)
            test_dset = type(self.test)(type(self.test.dset)(type(self.test.dset.data)(data_info=test_info, data_lbl=test_meta, lbl_info=meta_info)), 
                                         collate_fn=collate_fn)
            return type(self)(train=train_dset, test=test_dset)

        return type(self)(train=train_dset)

    @staticmethod
    def inference_dset(data_info:Dict, data_lbl:sp.csr_matrix, lbl_info:Dict, data_lbl_filterer, 
                       **kwargs):
        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)
        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)
        data_lbl[x_idx, y_idx] = 1
        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])
    
        pred_dset = XCDataset(MainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,
                                            data_lbl_filterer=data_lbl_filterer, **kwargs))
        return pred_dset
        
    @classmethod
    def from_cfg(cls, 
                 cfg:Union[str,Dict],
                 collate_fn:Optional[Callable]=None,
                 valid_pct:Optional[float]=0.2,
                 seed=None):
        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)

        blks = dict()
        for split in ['train', 'valid', 'test']:

            if split in cfg['path']:
                
                if split != 'train' and 'train' in blks:
                    if 'lbl_info' not in cfg['path'][split]:
                        cfg['path'][split]['lbl_info'] = blks['train'].dset.data.lbl_info
    
                    if blks['train'].dset.meta is not None:
                        for meta_name in blks['train'].dset.meta:
                            if 'meta_info' not in cfg['path'][split][meta_name]:
                                cfg['path'][split][meta_name]['meta_info'] = blks['train'].dset.meta[meta_name].meta_info
                                
                blks[split] = BaseXCDataBlock.from_file(**cfg['path'][split], **cfg['parameters'], collate_fn=collate_fn)
                
        # blks = {o:BaseXCDataBlock.from_file(**cfg['path'][o], **cfg['parameters'], collate_fn=collate_fn) for o in ['train', 'valid', 'test'] if o in cfg['path']}
        # if 'valid' not in blks: blks['train'], blks['valid'] = blks['train'].splitter(valid_pct, seed=seed)
        return cls(**blks)
        
