"""Datasets and collators for Extreme Classification"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_data.ipynb.

# %% auto 0
__all__ = ['MainXCData', 'MetaXCData', 'BaseXCDataset', 'MainXCDataset', 'MetaXCDataset', 'MetaXCDatasets', 'XCDataset',
           'XCCollator', 'BaseXCDataBlock', 'XCDataBlock']

# %% ../nbs/02_data.ipynb 3
import torch, inspect, numpy as np, pandas as pd, torch.nn.functional as F, random, scipy.sparse as sp

from scipy import sparse
from itertools import chain
from tqdm.auto import tqdm

from IPython.display import display
from typing import Dict, Optional, Callable
from torch.utils.data import Dataset,DataLoader
from xclib.data import data_utils as du
from xclib.utils.sparse import retain_topk
from transformers import PreTrainedTokenizerBase, AutoTokenizer

from fastcore.utils import *
from fastcore.meta import *
from fastcore.dispatch import *

from .core import *

# %% ../nbs/02_data.ipynb 8
def _read_sparse_file(fname:Optional[str]=None):
    if fname is None: return
    elif fname.endswith('.txt'): return du.read_sparse_file(fname)
    elif fname.endswith('.npz'): return sparse.load_npz(fname)
    else: raise ValueError(f'Invalid file extension : {fname}')
    

# %% ../nbs/02_data.ipynb 9
class MainXCData:
    
    @classmethod
    @delegates(Info.from_txt)
    def from_file(cls, data_info:str, lbl_info:str, data_lbl:Optional[str]=None, data_lbl_filterer:Optional[str]=None, 
                  main_max_data_sequence_length:Optional[int]=None, main_max_lbl_sequence_length:Optional[int]=None, **kwargs):
        return {
            'data_lbl': _read_sparse_file(data_lbl),
            'data_info': Info.from_txt(data_info, max_sequence_length=main_max_data_sequence_length, **kwargs),
            'lbl_info': Info.from_txt(lbl_info, max_sequence_length=main_max_lbl_sequence_length, **kwargs),
            'data_lbl_filterer': Filterer.load_filter(data_lbl_filterer),
        }
    

# %% ../nbs/02_data.ipynb 11
class MetaXCData:
    
    @classmethod
    @delegates(Info.from_txt)
    def from_file(cls, data_meta:str, lbl_meta:str, meta_info:str, prefix:str, meta_max_sequence_length:Optional[int]=None, **kwargs):
        return {
            'prefix': prefix,
            'data_meta': _read_sparse_file(data_meta),
            'lbl_meta': _read_sparse_file(lbl_meta),
            'meta_info': Info.from_txt(meta_info, max_sequence_length=meta_max_sequence_length, **kwargs),
        }
    

# %% ../nbs/02_data.ipynb 17
class BaseXCDataset(Dataset):
    def __init__(self):
        self.n_data, self.n_lbl, self.n_meta, self.n_samples = None, None, None, None
        
    def __len__(self):
        return self.n_data if self.n_data is not None else 0

    def splitter(self, valid_pct:Optional[float]=0.2, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(self.n_data).numpy())
        cut = int(valid_pct * self.n_data)
        train, valid = self._getitems(rnd_idx[cut:]), self._getitems(rnd_idx[:cut])
        return train, valid

    def sample(self, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):
        if seed is not None: torch.manual_seed(seed)
        rnd_idx = list(torch.randperm(self.n_data).numpy())
        cut = int(pct * self.n_data) if n is None else max(1, n)
        return self._getitems(rnd_idx[:cut])
        
    def _verify_info(self, info:Dict):
        if info is None: raise ValueError('`info` cannot be empty.')
        n_info = [len(v) for k,v in info.items()]
        if len(n_info) == 0 or n_info[0] == 0: raise ValueError('`info` cannot be empty.')
        if np.all([n_info[0] == o for o in n_info]) == False: 
            raise ValueError('All `data_info` fields should have equal number of elements.')
        return n_info[0]
        
    def show_data(self, n:Optional[int]=10, seed:Optional[int]=None):
        if n < 1: return
        if seed: np.random.seed(seed)
        idx = np.random.permutation(self.n_data)[:n]
        d = [self[i] for i in idx]
        df = pd.DataFrame({k:[o[k] for o in d] for k in d[0]})
        with pd.option_context('display.max_colwidth', None, 'display.max_columns', None):
            display(df)

    def prune_data_lbl(self, data_lbl:sparse.csr_matrix, data_repr:torch.Tensor, lbl_repr:torch.Tensor, batch_size:Optional[int]=64, 
                       thresh:Optional[float]=0.1, topk:Optional[int]=None):
        data_repr,lbl_repr = F.normalize(data_repr, dim=1), F.normalize(lbl_repr, dim=1)
        curr_data_lbl = data_lbl.copy()
        rows, cols = data_lbl.nonzero()
        dl = DataLoader(list(zip(rows, cols)), batch_size=batch_size, shuffle=False)
        score = None
        for b in tqdm(dl, total=len(dl)): 
            sc = data_repr[b[0]].unsqueeze(1)@lbl_repr[b[1]].unsqueeze(2)
            sc = sc.squeeze()
            sc = torch.where(sc < thresh, 0, sc)
            score = sc if score is None else torch.hstack([score, sc])
        curr_data_lbl.data[:] = score
        curr_data_lbl.eliminate_zeros()
        if topk is not None: 
            curr_data_lbl = retain_topk(curr_data_lbl, k=topk)
        return curr_data_lbl

    def get_info(self, prefix, idxs, info, info_keys):
        x = dict()
        for k,v in info.items():
            if k in info_keys:
                if isinstance(v, np.ndarray) or isinstance(v, torch.Tensor):
                    o = v[idxs]
                    if isinstance(o, np.ndarray): o = torch.from_numpy(o)
                    x[f'{prefix}_{k}'] = o
                else:
                    x[f'{prefix}_{k}'] = [v[idx] for idx in idxs]
        return x

    def extract_items(self, prefix:str, data_lbl:List, idxs:List, n_samples:int, n_s_samples:int, oversample:bool, 
                      info:Dict, info_keys:List, use_distribution:Optional[bool]=False, data_lbl_scores:Optional[List]=None):
        x, entity = dict(), prefix.split('2')[-1]
        
        x[f'p{prefix}_idx'] = [data_lbl[idx] for idx in idxs]
        if n_samples: 
            x[f'p{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:n_samples]] for o in x[f'p{prefix}_idx']]
        x[f'p{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in x[f'p{prefix}_idx']], dtype=torch.int64)

        if oversample:
            if use_distribution:
                probs = [data_lbl_scores[idx] for idx in idxs]
                x[f'{prefix}_idx'] = [np.random.choice(o, size=n_s_samples, p=p) if len(o) else [] for o,p in zip(x[f'p{prefix}_idx'],probs)]
            else:
                x[f'{prefix}_idx'] = [np.random.choice(o, size=n_s_samples) if len(o) else [] for o in x[f'p{prefix}_idx']]
        else:
            x[f'{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:n_s_samples]] for o in x[f'p{prefix}_idx']]
        x[f'{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in x[f'{prefix}_idx']], dtype=torch.int64)
        
        x[f'{prefix}_idx'] = torch.tensor(list(chain(*x[f'{prefix}_idx'])), dtype=torch.int64)
        x[f'p{prefix}_idx'] = torch.tensor(list(chain(*x[f'p{prefix}_idx'])), dtype=torch.int64)
        
        if info is not None:
            x.update(self.get_info(prefix, x[f'{prefix}_idx'], info, info_keys))
            
        return x

    @staticmethod
    def one_hop_matrix(data_lbl:sp.csr_matrix, batch_size:int=1024, topk:Optional[int]=None):
        data_lbl_t = data_lbl.transpose().tocsr()
        lbl_lbl = sp.vstack([data_lbl_t[i:i+batch_size]@data_lbl for i in tqdm(range(0, data_lbl_t.shape[0], batch_size))])
        data_lbl = sp.vstack([data_lbl[i:i+batch_size]@lbl_lbl for i in tqdm(range(0, data_lbl.shape[0], batch_size))])
        lbl_lbl.sort_indices()
        data_lbl.sort_indices()
        if topk is not None:
            data_lbl, lbl_lbl = retain_topk(data_lbl, k=topk), retain_topk(lbl_lbl, k=topk)
        return data_lbl, lbl_lbl

    @staticmethod
    def threshold_on_degree(data_lbl:sp.csr_matrix, thresh:int=10):
        data_lbl = data_lbl.copy()
        idx = np.where(data_lbl.getnnz(axis=1) > thresh)[0]
        for i in idx:
            p,q = data_lbl.indptr[i],data_lbl.indptr[i+1]
            data_lbl.data[p:q] = 0
        data_lbl.eliminate_zeros()
        return data_lbl
            

# %% ../nbs/02_data.ipynb 19
class MainXCDataset(BaseXCDataset):
    def __init__(self,
                 data_info:Dict,
                 data_lbl:Optional[sparse.csr_matrix]=None,
                 lbl_info:Optional[Dict]=None,
                 data_lbl_filterer:Optional[Union[sparse.csr_matrix,np.array]]=None,
                 n_lbl_samples:Optional[int]=None,
                 data_info_keys:Optional[List]=None,
                 lbl_info_keys:Optional[List]=None,
                 **kwargs):
        super().__init__()
        store_attr('data_info,data_lbl,lbl_info,data_lbl_filterer,n_lbl_samples,data_info_keys,lbl_info_keys')
        self.curr_data_lbl = None
        
        self._verify_inputs()
        self._store_indices()
        
    @classmethod
    @delegates(MainXCData.from_file)
    def from_file(cls, n_lbl_samples:Optional[int]=None, data_info_keys:Optional[List]=None, lbl_info_keys:Optional[List]=None, **kwargs):
        return cls(**MainXCData.from_file(**kwargs), n_lbl_samples=n_lbl_samples, 
                   data_info_keys=data_info_keys, lbl_info_keys=lbl_info_keys)

    def _store_indices(cls):
        if cls.data_lbl is not None: cls.curr_data_lbl = [o.indices.tolist() for o in cls.data_lbl]

    def _verify_inputs(cls):
        cls.n_data = cls._verify_info(cls.data_info)
        if cls.data_info_keys is None: cls.data_info_keys = list(cls.data_info.keys())
        if cls.data_lbl is not None:
            if cls.n_data != cls.data_lbl.shape[0]:
                raise ValueError(f'`data_info`({cls.n_data}) should have same number of datapoints as `data_lbl`({cls.data_lbl.shape[0]})')
            cls.n_lbl = cls.data_lbl.shape[1]
            if cls.lbl_info is not None:
                n_lbl = cls._verify_info(cls.lbl_info)
                if n_lbl != cls.data_lbl.shape[1]:
                    raise ValueError(f'`lbl_info`({n_lbl}) should have same number of labels as `data_lbl`({cls.data_lbl.shape[1]})')
                if cls.lbl_info_keys is None: cls.lbl_info_keys = list(cls.lbl_info.keys())
        elif cls.lbl_info is not None:
            cls.n_lbl = cls._verify_info(cls.lbl_info)
            if cls.lbl_info_keys is None: cls.lbl_info_keys = list(cls.lbl_info.keys())

    @classmethod
    def _initialize(cls, dset):
        return cls(dset.data_info, data_lbl=dset.data_lbl, lbl_info=dset.lbl_info, data_lbl_filterer=dset.data_lbl_filterer,
                   n_lbl_samples=dset.n_lbl_samples, data_info_keys=dset.data_info_keys, lbl_info_keys=dset.lbl_info_keys)
        

# %% ../nbs/02_data.ipynb 20
@patch
def __getitem__(cls:MainXCDataset, idx:int):
    x = {f'data_{k}': v[idx] for k,v in cls.data_info.items() if k in cls.data_info_keys}
    x['data_idx'] = idx
    if cls.data_lbl is not None:
        prefix = 'lbl2data'
        x[f'{prefix}_idx'] = cls.curr_data_lbl[idx]
        if cls.n_lbl_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:cls.n_lbl_samples]]
        if cls.lbl_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in cls.lbl_info.items() if k in cls.lbl_info_keys})
    return x
    

# %% ../nbs/02_data.ipynb 22
@patch
def _getitems(cls:MainXCDataset, idxs:List):
    return MainXCDataset(
        {k:[v[idx] for idx in idxs] for k,v in cls.data_info.items()}, 
        data_lbl=cls.data_lbl[idxs] if cls.data_lbl is not None else None, 
        lbl_info=cls.lbl_info, 
        data_lbl_filterer=Filterer.sample(cls.data_lbl_filterer, sz=cls.data_lbl.shape, idx=idxs) if cls.data_lbl_filterer is not None else None,
        n_lbl_samples=cls.n_lbl_samples,
        data_info_keys=cls.data_info_keys,
        lbl_info_keys=cls.lbl_info_keys,
    )

# %% ../nbs/02_data.ipynb 31
class MetaXCDataset(BaseXCDataset):

    def __init__(self,
                 prefix:str,
                 data_meta:sparse.csr_matrix, 
                 lbl_meta:sparse.csr_matrix, 
                 meta_info:Optional[Dict]=None, 
                 n_data_meta_samples:Optional[int]=None,
                 n_lbl_meta_samples:Optional[int]=None,
                 meta_info_keys:Optional[List]=None,
                 **kwargs):
        store_attr('prefix,data_meta,lbl_meta,meta_info,n_data_meta_samples,n_lbl_meta_samples,meta_info_keys')
        self.curr_data_meta,self.curr_lbl_meta = None,None
        self._verify_inputs()
        self._store_indices()

    def prune_data_meta(self, data_repr:torch.Tensor, meta_repr:torch.Tensor, batch_size:Optional[int]=64, thresh:Optional[float]=0.0, 
                        topk:Optional[int]=None):
        data_meta = self.prune_data_lbl(self.data_meta, data_repr, meta_repr, batch_size, thresh, topk)
        self.curr_data_meta = [o.indices.tolist() for o in data_meta]

    def prune_lbl_meta(self, lbl_repr:torch.Tensor, meta_repr:torch.Tensor, batch_size:Optional[int]=64, thresh:Optional[float]=0.0, 
                       topk:Optional[int]=None):
        lbl_meta = self.prune_data_lbl(self.lbl_meta, lbl_repr, meta_repr, batch_size, thresh, topk)
        self.curr_lbl_meta = [o.indices.tolist() for o in lbl_meta]

    def _store_indices(self):
        if self.data_meta is not None: self.curr_data_meta = [o.indices.tolist() for o in self.data_meta]
        if self.lbl_meta is not None: self.curr_lbl_meta = [o.indices.tolist() for o in self.lbl_meta]

    def update_meta_matrix(self, data_meta:sparse.csr_matrix, lbl_meta:sparse.csr_matrix):
        self.data_meta, self.lbl_meta = data_meta, lbl_meta
        self._store_indices()
        
    def _getitems(self, idxs:List):
        return MetaXCDataset(self.prefix, self.data_meta[idxs], self.lbl_meta, self.meta_info, 
                             self.n_data_meta_samples, self.n_lbl_meta_samples, self.meta_info_keys)
    
    @classmethod
    def _initialize(cls, dset):
        return cls(dset.prefix, dset.data_meta, dset.lbl_meta, dset.meta_info, 
                   dset.n_data_meta_samples, dset.n_lbl_meta_samples, dset.meta_info_keys)

    def _sample_meta_items(self, idxs:List):
        assert max(idxs) < self.n_meta, f"indices should be less than {self.n_meta}"
        meta_info = {k: [v[i] for i in idxs] for k,v in self.meta_info.items()}
        return MetaXCDataset(self.prefix, self.data_meta[:, idxs], self.lbl_meta[:, idxs], meta_info, 
                             self.n_data_meta_samples, self.n_lbl_meta_samples, self.meta_info_keys)
        
    @classmethod
    @delegates(MetaXCData.from_file)
    def from_file(cls, n_data_meta_samples:Optional[int]=None, n_lbl_meta_samples:Optional[int]=None, meta_info_keys:Optional[List]=None, **kwargs):
        return cls(**MetaXCData.from_file(**kwargs), n_data_meta_samples=n_data_meta_samples, 
                   n_lbl_meta_samples=n_lbl_meta_samples, meta_info_keys=meta_info_keys)

    @typedispatch
    def get_lbl_meta(self, idx:int):
        prefix = f'{self.prefix}2lbl2data'
        x = {f'{prefix}_idx': self.curr_lbl_meta[idx]}
        if self.n_lbl_meta_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:self.n_lbl_meta_samples]]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x
    
    @typedispatch
    def get_lbl_meta(self, idxs:List):
        prefix = f'{self.prefix}2lbl2data'
        x = {f'{prefix}_idx': [self.curr_lbl_meta[idx] for idx in idxs]}
        if self.n_lbl_meta_samples: x[f'{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:self.n_lbl_meta_samples]] for o in x[f'{prefix}_idx']]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[[v[i] for i in o] for o in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x
        
    def get_data_meta(self, idx:int):
        prefix = f'{self.prefix}2data'
        x = {f'{prefix}_idx': self.curr_data_meta[idx]}
        if self.n_data_meta_samples: x[f'{prefix}_idx'] = [x[f'{prefix}_idx'][i] for i in np.random.permutation(len(x[f'{prefix}_idx']))[:self.n_data_meta_samples]]
        if self.meta_info is not None:
            x.update({f'{prefix}_{k}':[v[i] for i in x[f'{prefix}_idx']] for k,v in self.meta_info.items() if k in self.meta_info_keys})
        return x

    def shape(self):
        return (self.n_data, self.n_lbl, self.n_meta)
        
    def show_data(self, is_lbl:Optional[bool]=False, n:Optional[int]=10, seed:Optional[int]=None):
        if n < 1: return
        if seed: np.random.seed(seed)
        idx = np.random.permutation(self.n_lbl if is_lbl else self.n_data)[:n]
        d = [self.get_lbl_meta(int(i)) for i in idx] if is_lbl else [self.get_data_meta(i) for i in idx]
        df = pd.DataFrame({k:[o[k] for o in d] for k in d[0]})
        with pd.option_context('display.max_colwidth', None):
            display(df)
    

# %% ../nbs/02_data.ipynb 33
@patch
def _verify_inputs(cls:MetaXCDataset):
    cls.n_data,cls.n_meta = cls.data_meta.shape[0],cls.data_meta.shape[1]
    
    if cls.lbl_meta is not None:
        cls.n_lbl = cls.lbl_meta.shape[0]
        if cls.lbl_meta.shape[1] != cls.n_meta:
            raise ValueError(f'`lbl_meta`({cls.lbl_meta.shape[1]}) should have same number of columns as `data_meta`({cls.n_meta}).')

    if cls.meta_info is not None:
        n_meta = cls._verify_info(cls.meta_info)
        if n_meta != cls.n_meta:
            raise ValueError(f'`meta_info`({n_meta}) should have same number of entries as number of columns of `data_meta`({cls.n_meta})')
        if cls.meta_info_keys is None: cls.meta_info_keys = list(cls.meta_info.keys())
            

# %% ../nbs/02_data.ipynb 47
class MetaXCDatasets(dict):

    def __init__(self, meta:Dict):
        super().__init__(meta)
        for o in meta: setattr(self, o, meta[o])

    def __setitem__(self, key, value):
        super().__setitem__(key, value)
        setattr(self, key, value)

    def __delitem__(self, key):
        super().__delitem__(key)
        delattr(self, key)
        

# %% ../nbs/02_data.ipynb 48
class XCDataset(BaseXCDataset):

    def __init__(self, data:MainXCDataset, **kwargs):
        super().__init__()
        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], MetaXCDataset)})
        self._verify_inputs()

    @staticmethod
    def get_meta_args(**kwargs):
        return [k for k in kwargs if re.match(r'.*_meta$', k)]
        
    def _getitems(self, idxs:List):
        return XCDataset(self.data._getitems(idxs), **{k:meta._getitems(idxs) for k,meta in self.meta.items()})

    @classmethod
    def _initialize(cls, dset):
        return cls(MainXCDataset._initialize(dset.data), **{k:MetaXCDataset._initialize(meta) for k,meta in dset.meta.items()})
        
    @classmethod
    @delegates(MainXCDataset.from_file)
    def from_file(cls, **kwargs):
        data = MainXCDataset.from_file(**kwargs)
        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}
        meta = {k:MetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}
        return cls(data, **meta)

    def _verify_inputs(self):
        self.n_data, self.n_lbl = self.data.n_data, self.data.n_lbl
        if len(self.meta):
            self.n_meta = len(self.meta)
            for meta in self.meta.values():
                if meta.n_data != self.n_data: 
                    raise ValueError(f'`meta`({meta.n_data}) and `data`({self.n_data}) should have the same number of datapoints.')
                if self.n_lbl is not None and meta.n_lbl != self.n_lbl: 
                    raise ValueError(f'`meta`({meta.n_lbl}) and `data`({self.n_lbl}) should have the same number of labels.')


    def __getitem__(self, idx:int):
        x = self.data[idx]
        if self.n_meta:
            for m in self.meta.values():
                x.update(m.get_data_meta(idx))
                if self.n_lbl: x.update(m.get_lbl_meta(x['lbl2data_idx']))
        return x

    @property
    def lbl_info(self): return self.data.lbl_info

    @property
    def lbl_dset(self): return MainXCDataset(self.data.lbl_info)

    @property
    def data_info(self): return self.data.data_info

    @property
    def data_dset(self): return MainXCDataset(self.data.data_info) 

    def one_batch(self, bsz:Optional[int]=10, seed:Optional[int]=None):
        if seed is not None: torch.manual_seed(seed)
        idxs = list(torch.randperm(len(self)).numpy())[:bsz]
        return [self[idx] for idx in idxs]

    def combined_lbl_and_meta(self, meta_name:str, pad_token:int=0, p_data=0.5, **kwargs):
        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')
            
        data_lbl = self.data.data_lbl
        data_lbl = data_lbl.multiply(1/(data_lbl.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_lbl = data_lbl.tocsr() * p_data
        
        data_meta = self.meta[f'{meta_name}_meta'].data_meta
        data_meta = data_meta.multiply(1/(data_meta.getnnz(axis=1).reshape(-1, 1) + 1e-9))
        data_meta = data_meta.tocsr() * (1 - p_data)
    
        data_info = self.data.data_info
        lbl_info = self.data.lbl_info
        meta_info = self.meta[f'{meta_name}_meta'].meta_info
    
        comb_info = dict()
        for k,v in lbl_info.items():
            if isinstance(v, tuple) or isinstance(v, list): comb_info[k] = v + meta_info[k]
            elif isinstance(v, torch.Tensor):
                n_data = v.shape[0] + meta_info[k].shape[0]
                seq_len = max(v.shape[1], meta_info[k].shape[1]) 
                
                if k == 'input_ids': 
                    info = torch.full((n_data, seq_len), pad_token, dtype=v.dtype)
                elif k == 'attention_mask': 
                    info = torch.full((n_data, seq_len), 0, dtype=v.dtype)
                    
                info[:v.shape[0], :v.shape[1]] = v
                info[v.shape[0]:, :meta_info[k].shape[1]] = meta_info[k]
        
                comb_info[k] = info
    
        n_lbl_samples = kwargs.get('n_lbl_samples') if 'n_lbl_samples' in kwargs else self.data.n_lbl_samples
        data_info_keys = kwargs.get('data_info_keys') if 'data_info_keys' in kwargs else self.data.data_info_keys
        lbl_info_keys = kwargs.get('lbl_info_keys') if 'lbl_info_keys' in kwargs else self.data.lbl_info_keys
        
        dset = MainXCDataset(data_info, sp.hstack([data_lbl, data_meta]), comb_info, self.data.data_lbl_filterer,
                             n_lbl_samples=n_lbl_samples, data_info_keys=data_info_keys, lbl_info_keys=lbl_info_keys)
        return XCDataset(dset)

    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, topk:Optional[int]=10, **kwargs):
        data_lbl = self.threshold_on_degree(self.data.data_lbl, thresh=thresh)
        data_meta, lbl_meta = self.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk)
        data_meta = data_meta/(data_meta.sum(axis=1) + 1e-9)
        data_meta = data_meta.tocsr()
        lbl_meta = lbl_meta/(lbl_meta.sum(axis=1) + 1e-9)
        lbl_meta = lbl_meta.tocsr()
        self.meta['ohm_meta'] = MetaXCDataset('ohm', data_meta, lbl_meta, self.data.lbl_info, **kwargs)

    def _retain_randk(self, matrix:sparse.csr_matrix, topk:Optional[int]=3):
        data, indices, indptr = [], [], np.zeros_like(matrix.indptr)
        for i,row in tqdm(enumerate(matrix), total=matrix.shape[0]):
            if row.nnz > 0:
                idx = np.random.randint(row.nnz, size=topk)
                ind, d = row.indices[idx], row.data[idx]    
            else:
                ind, d = np.arange(topk), np.zeros(topk)
                
            indptr[i+1] = indptr[i] + topk
            indices.append(ind); data.append(d)
        data = np.hstack(data)
        indices = np.hstack(indices)
        o = sparse.csr_matrix((data, indices, indptr), shape=matrix.shape, dtype=matrix.dtype)
        o.sort_indices()
        return o

    def _remove_data(self, meta_1:sparse.csr_matrix, meta_2:sparse.csr_matrix, pct:Optional[float]=0.3):
        n_data = min(len(meta_1.data),len(meta_2.data))
        n = int(n_data * pct)
        idx = np.random.permutation(n_data)
        idx_1,idx_2 = idx[:n], idx[n:]
        meta_1.data[idx_1] = 0; meta_1.eliminate_zeros()
        meta_2.data[idx_2] = 0; meta_2.eliminate_zeros()

    def _mix_meta_matrix(self, meta_1:sparse.csr_matrix, meta_2:sparse.csr_matrix, pct:Optional[float]=0.3, k:Optional[int]=3):
        meta_1 = self._retain_randk(meta_1, topk=k)
        meta_2 = self._retain_randk(meta_2, topk=k)
        self._remove_data(meta_1, meta_2, pct)
        return meta_1 + meta_2

    def mix_meta_dataset(self, meta_1:str, meta_2:str, pct:Optional[float]=0.3, k:Optional[int]=3):
        if pct < 1:
            meta_info = self.meta[f'{meta_1}_meta'].meta_info
            data_meta = self._mix_meta_matrix(self.meta[f'{meta_1}_meta'].data_meta, self.meta[f'{meta_2}_meta'].data_meta, pct=pct, k=k)
            lbl_meta = self._mix_meta_matrix(self.meta[f'{meta_1}_meta'].lbl_meta, self.meta[f'{meta_2}_meta'].lbl_meta, pct=pct, k=k)
            self.meta['hyb_meta'] = MetaXCDataset('hyb', data_meta, lbl_meta, meta_info, 
                                                  n_data_meta_samples=self.meta[f'{meta_2}_meta'].n_data_meta_samples,
                                                  n_lbl_meta_samples=self.meta[f'{meta_2}_meta'].n_lbl_meta_samples, 
                                                  meta_info_keys=self.meta[f'{meta_2}_meta'].meta_info_keys)
        else:
            self.meta['hyb_meta'] = MetaXCDataset('hyb', self.meta[f'{meta_2}_meta'].data_meta, 
                                                  self.meta[f'{meta_2}_meta'].lbl_meta, 
                                                  self.meta[f'{meta_2}_meta'].meta_info, 
                                                  n_data_meta_samples=self.meta[f'{meta_2}_meta'].n_data_meta_samples,
                                                  n_lbl_meta_samples=self.meta[f'{meta_2}_meta'].n_lbl_meta_samples, 
                                                  meta_info_keys=self.meta[f'{meta_2}_meta'].meta_info_keys)
       

# %% ../nbs/02_data.ipynb 60
class XCCollator:

    def __init__(self, tfms):
        self.tfms = tfms

    def __call__(self, x):
        return self.tfms(x)
        

# %% ../nbs/02_data.ipynb 77
class BaseXCDataBlock:

    @delegates(DataLoader.__init__)
    def __init__(self, 
                 dset:XCDataset, 
                 collate_fn:Callable=None,
                 **kwargs):
        self.dset, self.dl_kwargs, self.collate_fn = dset, self._get_dl_kwargs(**kwargs), collate_fn
        self.dl = DataLoader(dset, collate_fn=collate_fn, **self.dl_kwargs) if collate_fn is not None else None

    @classmethod
    @delegates(XCDataset.from_file)
    def from_file(cls, collate_fn:Callable=None, **kwargs):
        return BaseXCDataBlock(XCDataset.from_file(**kwargs), collate_fn, **kwargs)

    def __len__(self):
        return len(self.dset)

    def _get_dl_kwargs(self, **kwargs):
        dl_params = inspect.signature(DataLoader.__init__).parameters
        return {k:v for k,v in kwargs.items() if k in dl_params}

    
    def _getitems(self, idxs:List):
        return BaseXCDataBlock(self.dset._getitems(idxs), collate_fn=self.collate_fn, **self.dl_kwargs)

    @classmethod
    def _initialize(cls, dset):
        return cls(XCDataset._initialize(dset.dset), collate_fn=dset.collate_fn, **dset.dl_kwargs) if dset is not None else None

    @property
    def bsz(self): return self.dl.batch_size

    @bsz.setter
    def bsz(self, v):
        self.dl_kwargs['batch_size'] = v
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None

    @property
    def data_lbl_filterer(self): return self.dset.data.data_lbl_filterer

    @data_lbl_filterer.setter
    def data_lbl_filterer(self, val): self.dset.data.data_lbl_filterer = val

    @typedispatch
    def one_batch(self):
        return next(iter(self.dl))

    @typedispatch
    def one_batch(self, bsz:int):
        self.dl_kwargs['batch_size'] = bsz
        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None
        return next(iter(self.dl))
        
        

# %% ../nbs/02_data.ipynb 78
@patch
def filterer(cls:BaseXCDataBlock, train:'BaseXCDataBlock', valid:'BaseXCDataBlock', fld:Optional[str]='identifier'):
    train_info, valid_info, lbl_info = train.dset.data.data_info, valid.dset.data.data_info, train.dset.data.lbl_info
    if fld not in train_info: raise ValueError(f'`{fld}` not in `data_info`')
        
    train.data_lbl_filterer, valid_filterer = Filterer.generate(train_info[fld], valid_info[fld], lbl_info[fld], 
                                                                train.dset.data.data_lbl, valid.dset.data.data_lbl)
    _, valid_filterer, idx = Filterer.prune(valid.dset.data.data_lbl, valid_filterer)
    
    valid = valid._getitems(idx)
    valid.data_lbl_filterer = valid_filterer
    
    return train, valid

@patch
def splitter(cls:BaseXCDataBlock, valid_pct:Optional[float]=0.2, seed=None):
    if seed is not None: torch.manual_seed(seed)
    rnd_idx = list(torch.randperm(len(cls)).numpy())
    cut = int(valid_pct * len(cls))
    train, valid = cls._getitems(rnd_idx[cut:]), cls._getitems(rnd_idx[:cut])
    if cls.data_lbl_filterer is None: return train, valid
    else: return cls.filterer(train, valid)

@patch
def sample(cls:BaseXCDataBlock, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):
    if seed is not None: torch.manual_seed(seed)
    rnd_idx = list(torch.randperm(len(cls)).numpy())
    cut = int(pct * len(cls)) if n is None else max(1, n)
    return cls._getitems(rnd_idx[:cut])
    

# %% ../nbs/02_data.ipynb 88
class XCDataBlock:

    def __init__(self, train:BaseXCDataBlock=None, valid:BaseXCDataBlock=None, test:BaseXCDataBlock=None):
        self.train, self.valid, self.test = train, valid, test

    @classmethod
    def _initialize(cls, dset):
        return cls(BaseXCDataBlock._initialize(dset.train), BaseXCDataBlock._initialize(dset.valid), BaseXCDataBlock._initialize(dset.test))

    @staticmethod
    def load_cfg(fname):
        with open(fname, 'r') as f: return json.load(f)

    @property
    def lbl_info(self): return self.train.dset.data.lbl_info

    @property
    def lbl_dset(self): return MainXCDataset(self.train.dset.data.lbl_info)

    @property
    def n_lbl(self):
        if self.train.dset is None: return self.test.dset.n_lbl
        return self.train.dset.n_lbl

    @property
    def collator(self): return self.train.collate_fn

    def sample_info(self, info, idx):
        return {k: v[idx] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[i] for i in idx] for k,v in info.items()}

    def linker_dset(self, meta_name:str, remove_empty:Optional[bool]=True):
        if meta_name not in self.train.dset.meta:
            raise ValueError(f'Invalid metadata: {meta_name}')

        train_meta = self.train.dset.meta[meta_name].data_meta
        if remove_empty:
            train_idx = np.where(train_meta.getnnz(axis=1) > 0)[0]
            meta_idx = np.where(train_meta.getnnz(axis=0) > 0)[0]
            train_meta = train_meta[train_idx][:, meta_idx].tocsr()
        
        train_info = self.train.dset.data.data_info
        meta_info = self.train.dset.meta[meta_name].meta_info

        if remove_empty:
            train_info = self.sample_info(train_info, train_idx)
            meta_info = self.sample_info(meta_info, meta_idx)

        collate_fn = self.train.collate_fn
        train_dset = BaseXCDataBlock(XCDataset(MainXCDataset(data_info=train_info, data_lbl=train_meta, lbl_info=meta_info)), 
                                     collate_fn=collate_fn)
        
        if self.test is not None:
            test_meta = self.test.dset.meta[meta_name].data_meta
            if remove_empty:
                test_meta = test_meta[:, meta_idx].tocsr()
                test_idx = np.where(test_meta.getnnz(axis=1) > 0)[0]
                test_meta = test_meta[test_idx].tocsr()
    
            test_info = self.test.dset.data.data_info
            if remove_empty: test_info = self.sample_info(test_info, test_idx)
    
            test_dset = BaseXCDataBlock(XCDataset(MainXCDataset(data_info=test_info, data_lbl=test_meta, lbl_info=meta_info)), 
                                        collate_fn=collate_fn)
            return XCDataBlock(train=train_dset, test=test_dset)
        
        return XCDataBlock(train=train_dset)

    @staticmethod
    def inference_dset(data_info:Dict, data_lbl:sparse.csr_matrix, lbl_info:Dict, data_lbl_filterer, **kwargs):
        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)
        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)
        data_lbl[x_idx, y_idx] = 1
        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])
    
        pred_dset = XCDataset(MainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,
                                            data_lbl_filterer=data_lbl_filterer, **kwargs))
        return pred_dset
        
    @classmethod
    def from_cfg(cls, 
                 cfg:Union[str,Dict],
                 collate_fn:Optional[Callable]=None,
                 valid_pct:Optional[float]=0.2,
                 seed=None):
        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)
        blks = {o:BaseXCDataBlock.from_file(**cfg['path'][o], **cfg['parameters'], collate_fn=collate_fn) for o in ['train', 'valid', 'test'] if o in cfg['path']}
        # if 'valid' not in blks: blks['train'], blks['valid'] = blks['train'].splitter(valid_pct, seed=seed)
        return cls(**blks)
        
