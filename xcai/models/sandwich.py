# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/40_models.sandwich.ipynb.

# %% auto 0
__all__ = ['Parameters', 'SandwichConfig', 'CrossCombinerBlock', 'EncoderOutput', 'BaseEncoder', 'Encoder', 'SAWModelOutput',
           'SAW000', 'SAW001']

# %% ../../nbs/40_models.sandwich.ipynb 3
import torch, torch.nn as nn, torch.nn.functional as F, re
from typing import Optional, Dict, Tuple
from dataclasses import dataclass

from transformers.utils.generic import ModelOutput
from transformers import PretrainedConfig, DistilBertConfig, DistilBertPreTrainedModel, DistilBertModel
from transformers.models.distilbert.modeling_distilbert import create_sinusoidal_embeddings, TransformerBlock

from ..losses import *
from ..learner import XCDataParallel
from .modeling_utils import *

# %% ../../nbs/40_models.sandwich.ipynb 4
from .product_key import *
from ..core import store_attr

# %% ../../nbs/40_models.sandwich.ipynb 19
class Parameters:
    
    @staticmethod
    def from_data_aug_meta_prefix_for_encoder(prefix:str, **kwargs):
        inputs = {}
        args = [arg for arg in kwargs if prefix is not None and re.match(f'^{prefix}.*_(input_ids|attention_mask|data2ptr|idx)$', arg)]
        for arg in args:
            meta,param = arg.split('_', maxsplit=1)
            inputs.setdefault(meta, {})[param] = kwargs[arg]
        return inputs
    
    @staticmethod
    def from_aug_meta_prefix_for_feature(feat:str, prefix:str, **kwargs):
        keys = ['attention_mask', 'input_ids', 'idx']        
        inputs = {f'{prefix}_{k}': kwargs[f'{prefix}_{k}'] for k in keys if f'{prefix}_{k}' in kwargs}
        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:
            inputs.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})
        return inputs

    @staticmethod
    def from_aug_meta_prefix_for_loss(feat:str, prefix:str, **kwargs):
        keys = [f'{prefix}_idx', f'p{prefix}_idx']
        args = {k: kwargs[k] for k in keys if k in kwargs}
        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:
            args.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})
        if prefix is not None and f'p{prefix}_{feat}2ptr' in kwargs:
            args.update({f'p{prefix}_data2ptr': kwargs[f'p{prefix}_{feat}2ptr']})

        inputs = {}
        for arg in args:
            meta,param = arg.split('_', maxsplit=1)
            inputs.setdefault(meta, {})[param] = args[arg]
        return inputs
        

# %% ../../nbs/40_models.sandwich.ipynb 25
class SandwichConfig(DistilBertConfig):

    def __init__(
        self,
        data_aug_meta_prefix:Optional[str] = None, 
        lbl2data_aug_meta_prefix:Optional[str] = None,

        data_enrich:Optional[bool] = True,
        lbl2data_enrich:Optional[bool] = True,
        
        num_batch_labels:Optional[int] = None,
        batch_size:Optional[int] = None,
        margin:Optional[float] = 0.3,
        num_negatives:Optional[int] = 10,
        tau:Optional[float] = 0.1,
        apply_softmax:Optional[bool] = True,

        use_calib_loss:Optional[float] = False,
        calib_margin:Optional[float] = 0.05,
        calib_num_negatives:Optional[int] = 10,
        calib_tau:Optional[float] = 0.1,
        calib_apply_softmax:Optional[bool] = False,
        calib_loss_weight:Optional[float] = 0.1,
        
        use_query_loss:Optional[float] = True,
        
        use_meta_loss:Optional[bool] = False,
        meta_loss_weight:Optional[float] = 0.1,
        
        use_encoder_parallel:Optional[bool] = True,
        
        **kwargs,
    ):
        self.data_aug_meta_prefix = data_aug_meta_prefix
        self.lbl2data_aug_meta_prefix = lbl2data_aug_meta_prefix

        self.data_enrich = data_enrich
        self.lbl2data_enrich = lbl2data_enrich

        self.num_batch_labels = num_batch_labels
        self.batch_size = batch_size
        self.margin = margin
        self.num_negatives = num_negatives
        self.tau = tau
        self.apply_softmax = apply_softmax

        self.use_calib_loss = use_calib_loss
        self.calib_loss_weight = calib_loss_weight
        self.calib_margin = calib_margin
        self.calib_num_negatives = calib_num_negatives
        self.calib_tau = calib_tau
        self.calib_apply_softmax = calib_apply_softmax
        
        self.use_query_loss = use_query_loss
        
        self.use_meta_loss = use_meta_loss
        self.meta_loss_weight = meta_loss_weight

        self.use_encoder_parallel = use_encoder_parallel
        
        super().__init__(**kwargs)
        

# %% ../../nbs/40_models.sandwich.ipynb 30
class CrossCombinerBlock(TransformerBlock):

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)

    def post_init(self):
        for module in self.modules(): self._init_weights(module)

    def _initialize_weights(self, module: nn.Module):
        for m in module.modules(): self._init_weights(m)

    def _init_weights(self, module: nn.Module):
        if isinstance(module, nn.Linear):
            torch.nn.init.eye_(module.weight)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(
        self,
        x: torch.Tensor,
        m: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, ...]:
        
        # Cross-Attention
        ca_output = self.attention(
            query=x,
            key=m,
            value=m,
            mask=attn_mask,
            head_mask=head_mask,
            output_attentions=output_attentions,
        )
        if output_attentions:
            ca_output, ca_weights = ca_output  # (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)
        else:  # To handle these `output_attentions` or `output_hidden_states` cases returning tuples
            if type(ca_output) is not tuple:
                raise TypeError(f"ca_output must be a tuple but it is {type(ca_output)} type")

            ca_output = ca_output[0]
        ca_output = self.sa_layer_norm(ca_output + x)  # (bs, seq_length, dim)

        # Feed Forward Network
        ffn_output = self.ffn(ca_output)  # (bs, seq_length, dim)
        ffn_output: torch.Tensor = self.output_layer_norm(ffn_output + ca_output)  # (bs, seq_length, dim)

        output = (ffn_output,)
        if output_attentions:
            output = (ca_weights,) + output
        return output
        

# %% ../../nbs/40_models.sandwich.ipynb 32
@dataclass
class EncoderOutput(ModelOutput):
    data_repr: Optional[torch.FloatTensor] = None
    data_meta_repr: Optional[torch.FloatTensor] = None
    enriched_data_repr: Optional[torch.FloatTensor] = None
    meta_repr: Optional[torch.FloatTensor] = None
    

# %% ../../nbs/40_models.sandwich.ipynb 33
class BaseEncoder(DistilBertPreTrainedModel):
    
    config_class= None

    def __init__(
        self, 
        config:PretrainedConfig, 
    ):
        super().__init__(config)
        self.distilbert = DistilBertModel(config)
        self.meta_distilbert = DistilBertModel(config)
        
        self.query_head = RepresentationHead(config)
        self.meta_query_head = RepresentationHead(config)
        self.enriched_query_head = RepresentationHead(config)
        
        self.post_init()

    @torch.no_grad()
    def init_heads_to_identity(self):
        self.query_head.post_init()
        self.meta_query_head.post_init()
        self.enriched_query_head.post_init()

    @torch.no_grad()
    def init_meta_distilbert(self):
        sd, msd = self.distilbert.state_dict(), self.meta_distilbert.state_dict()
        sd_keys, msd_keys = sd.keys(), msd.keys()
        assert len(sd_keys) == len(msd_keys), f'mismatched keys: {len(sd_keys)} != {len(msd_keys)}'
        for k in sd_keys:
            assert sd[k].shape == msd[k].shape
            msd[k].copy_(sd[k])

    def get_position_embeddings(self) -> nn.Embedding:
        return self.distilbert.get_position_embeddings()
    
    def resize_position_embeddings(self, new_num_position_embeddings: int):
        self.distilbert.resize_position_embeddings(new_num_position_embeddings)
    
    def encode(self, input_ids:torch.Tensor, attention_mask:torch.Tensor, **kwargs):
        return self.distilbert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

    def encode_meta(self, input_ids:torch.Tensor, attention_mask:torch.Tensor, **kwargs):
        return self.meta_distilbert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )
        
    def encode_query(self, embed:torch.Tensor, attention_mask:torch.Tensor):
        embed = self.query_head(embed)
        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)

    def encode_meta_query(self, embed:torch.Tensor, attention_mask:torch.Tensor):
        embed = self.meta_query_head(embed)
        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)

    def encode_enriched_query(self, embed:torch.Tensor, attention_mask:torch.Tensor):
        embed = self.enriched_query_head(embed)
        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)

    def enrich_query_representation(self):
        raise NotImplementedError("Override this method in a subclass.")

    def forward(self, data_input_ids: torch.Tensor, data_attention_mask: torch.Tensor, data_aug_meta_prefix: Optional[str]=None,
                data_enrich: Optional[bool]=True, **kwargs):  
        raise NotImplementedError("Override this method in a subclass.")
    

# %% ../../nbs/40_models.sandwich.ipynb 35
class Encoder(BaseEncoder):
    
    config_class = SandwichConfig
    
    def __init__(
        self, 
        config:PretrainedConfig, 
    ):
        super().__init__(config)
        self.combiner_head = CrossCombinerBlock(config)
        self.post_init()

    @torch.no_grad()
    def init_heads_to_identity(self):
        self.combiner_head.post_init()
        super().init_heads_to_identity()

    @torch.no_grad()
    def init_combiner_to_last_layer(self):
        lsd, csd = self.distilbert.transformer.layer[-1].state_dict(), self.combiner_head.state_dict()
        lsd_keys, csd_keys = lsd.keys(), csd.keys()
        assert len(lsd_keys) == len(csd_keys), f'mismatched keys: {len(lsd_keys)} != {len(csd_keys)}'
        for k in csd_keys:
            assert csd[k].shape == lsd[k].shape
            csd[k].copy_(lsd[k])

    @torch.no_grad()
    def init_meta_distilbert(self):
        super().init_meta_distilbert()

    def enrich_query_representation(self, data_o:torch.Tensor, data_meta_o:torch.Tensor, data_attention_mask:torch.Tensor):
        attn_mask = data_attention_mask.view(len(data_attention_mask), 1, 1, -1).bool()
        fusion_o = self.combiner_head(x=data_o, m=data_meta_o, attn_mask=attn_mask)
        enriched_data_repr = self.encode_enriched_query(fusion_o[0], data_attention_mask)
        return enriched_data_repr

    def forward(
        self, 
        data_input_ids: torch.Tensor, 
        data_attention_mask: torch.Tensor,
        data_aug_meta_prefix: Optional[str]=None,
        data_enrich: Optional[bool]=True,
        **kwargs
    ):
        data_o = self.encode(data_input_ids, data_attention_mask)
        data_repr = self.encode_query(data_o[0], data_attention_mask)

        data_meta_o = self.encode_meta(data_input_ids, data_attention_mask)
        data_meta_repr = self.encode_meta_query(data_meta_o[0], data_attention_mask)
        
        meta_kwargs = Parameters.from_data_aug_meta_prefix_for_encoder(data_aug_meta_prefix, **kwargs)
        meta_kwargs = meta_kwargs.get(data_aug_meta_prefix, None)

        meta_repr = None
        if meta_kwargs is not None and len(meta_kwargs['idx']):
            meta_o = self.encode_meta(meta_kwargs['input_ids'], meta_kwargs['attention_mask'])
            meta_repr = self.encode_meta_query(meta_o[0], meta_kwargs['attention_mask'])

        enriched_data_repr = self.enrich_query_representation(data_o[0], data_meta_o[0], data_attention_mask) if data_enrich else None
        
        return EncoderOutput(
            data_repr=data_repr,
            data_meta_repr=data_meta_repr,
            enriched_data_repr=enriched_data_repr,
            meta_repr=meta_repr,
        )
        

# %% ../../nbs/40_models.sandwich.ipynb 42
@dataclass
class SAWModelOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    data_repr: Optional[torch.FloatTensor] = None
    data_enriched_repr: Optional[torch.FloatTensor] = None
    lbl2data_repr: Optional[torch.FloatTensor] = None
    lbl2data_enriched_repr: Optional[torch.FloatTensor] = None
    

# %% ../../nbs/40_models.sandwich.ipynb 43
class SAW000(nn.Module):

    config_class = SandwichConfig
    
    def __init__(
        self, 
        config: SandwichConfig,
    ):
        super().__init__(config)
        self.config, self.encoder = config, None
        self.rep_loss_fn = MultiTriplet(margin=config.margin, n_negatives=config.num_negatives, tau=config.tau, 
                                        apply_softmax=config.apply_softmax, reduce='mean')
        self.cab_loss_fn = Calibration(margin=config.calib_margin, tau=config.calib_tau, n_negatives=config.calib_num_negatives, 
                                       apply_softmax=config.calib_apply_softmax, reduce='mean')
        
    def init_heads_to_identity(self):
        if self.encoder is None: raise ValueError('Encoder not initialized.')
        self.encoder.init_heads_to_identity()

    def init_combiner_to_last_layer(self):
        if self.encoder is None: raise ValueError('Encoder not initialized.')
        self.encoder.init_combiner_to_last_layer()

    def init_meta_distilbert(self):
        if self.encoder is None: raise ValueError('Encoder not initialized.')
        self.encoder.init_meta_distilbert()
        
    def compute_loss(self, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx):
        return self.rep_loss_fn(inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)

    def calibration_loss(self, einp_repr, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx):
        return self.config.calib_loss_weight * self.cab_loss_fn(einp_repr, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)

    def compute_meta_loss(self, data_o, lbl2data_o, **kwargs):
        loss = 0.0
        meta_kwargs = Parameters.from_aug_meta_prefix_for_loss('data', self.config.data_aug_meta_prefix, **kwargs)
        prefix = self.config.data_aug_meta_prefix
        if meta_kwargs is not None and len(meta_kwargs[prefix]['idx']):
            idx = torch.where(meta_kwargs[prefix]['data2ptr'] > 0)[0]
            loss += self.config.meta_loss_weight * self.compute_loss(data_o.data_meta_repr[idx], 
                                                                     data_o.meta_repr,
                                                                     meta_kwargs[prefix]['data2ptr'][idx],
                                                                     meta_kwargs[prefix]['idx'],
                                                                     meta_kwargs[f'p{prefix}']['data2ptr'][idx],
                                                                     meta_kwargs[f'p{prefix}']['idx'])
            
        meta_kwargs = Parameters.from_aug_meta_prefix_for_loss('lbl', self.config.lbl2data_aug_meta_prefix, **kwargs)
        prefix = self.config.lbl2data_aug_meta_prefix
        if meta_kwargs is not None and len(meta_kwargs[prefix]['idx']):
            idx = torch.where(meta_kwargs[prefix]['data2ptr'] > 0)[0]
            loss += self.config.meta_loss_weight * self.compute_loss(lbl2data_o.data_meta_repr[idx], 
                                                                     lbl2data_o.meta_repr,
                                                                     meta_kwargs[prefix]['data2ptr'][idx],
                                                                     meta_kwargs[prefix]['idx'],
                                                                     meta_kwargs[f'p{prefix}']['data2ptr'][idx],
                                                                     meta_kwargs[f'p{prefix}']['idx'])
        return loss

    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_data2ptr:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        plbl2data_data2ptr:Optional[torch.Tensor]=None,
        plbl2data_idx:Optional[torch.Tensor]=None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ): 
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        if self.config.use_encoder_parallel: 
            encoder = XCDataParallel(module=self.encoder)
        else: encoder = self.encoder
        
        data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('data', self.config.data_aug_meta_prefix, **kwargs)
        data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, 
                         data_aug_meta_prefix=self.config.data_aug_meta_prefix, data_enrich=self.config.data_enrich, **data_meta_kwargs)
        
        loss = None; lbl2data_o = EncoderOutput()
        if lbl2data_input_ids is not None:
            lbl2data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('lbl', self.config.lbl2data_aug_meta_prefix, **kwargs)
            lbl2data_o = encoder(data_input_ids=lbl2data_input_ids, data_attention_mask=lbl2data_attention_mask, 
                                 data_aug_meta_prefix=self.config.lbl2data_aug_meta_prefix, data_enrich=self.config.lbl2data_enrich, **lbl2data_meta_kwargs)
            
            loss = self.compute_loss(data_o.enriched_data_repr, lbl2data_o.enriched_data_repr,lbl2data_data2ptr,lbl2data_idx,
                                     plbl2data_data2ptr,plbl2data_idx)

            if self.config.use_query_loss:
                loss += self.compute_loss(data_o.data_repr, lbl2data_o.data_repr,lbl2data_data2ptr,lbl2data_idx,
                                          plbl2data_data2ptr,plbl2data_idx)

            if self.config.use_calib_loss:
                loss += self.calibration_loss(data_o.enriched_data_repr, data_o.data_repr, lbl2data_o.data_repr,
                                              lbl2data_data2ptr,lbl2data_idx,plbl2data_data2ptr,plbl2data_idx)
                loss += self.calibration_loss(data_o.enriched_data_repr, data_o.data_repr, lbl2data_o.enriched_data_repr,
                                              lbl2data_data2ptr,lbl2data_idx, plbl2data_data2ptr,plbl2data_idx)

            if self.config.use_meta_loss:
                loss += self.compute_meta_loss(data_o, lbl2data_o, **kwargs)
            
        if not return_dict:
            o = (data_o.data_repr,data_o.enriched_data_repr,lbl2data_o.data_repr,lbl2data_o.enriched_data_repr)
            return ((loss,) + o) if loss is not None else o
        
        return SAWModelOutput(
            loss=loss,
            data_repr=data_o.data_repr,
            data_enriched_repr=data_o.enriched_data_repr,
            lbl2data_repr=lbl2data_o.data_repr,
            lbl2data_enriched_repr=lbl2data_o.enriched_data_repr,
        )
        

# %% ../../nbs/40_models.sandwich.ipynb 45
class SAW001(SAW000, DistilBertPreTrainedModel):
    use_generation,use_representation = False,True
    _tied_weights_keys = ["encoder.distilbert"]

    def __init__(self, config):
        super().__init__(config)
        self.encoder = Encoder(config)
        
        self.post_init()
        self.remap_post_init()

    def remap_post_init(self):
        self.distilbert = self.encoder.distilbert
        
