# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/30_models.BBB0XX.ipynb.

# %% auto 0
__all__ = ['BRT009Encoder', 'BRT009', 'BRT023Encoder', 'BRT023']

# %% ../../nbs/30_models.BBB0XX.ipynb 2
import torch, re, inspect, pickle, os, torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, List, Tuple, Mapping, Any, Union
from transformers import BertPreTrainedModel, BertModel

from transformers.activations import get_activation
from transformers.utils.generic import ModelOutput

from fastcore.meta import *

from ..losses import *
from ..core import store_attr
from ..learner import XCDataParallel
from .modeling_utils import *

# %% ../../nbs/30_models.BBB0XX.ipynb 18
class BRT009Encoder(BertPreTrainedModel):
    
    def __init__(self, config, *args, **kwargs):
        super().__init__(config, *args, **kwargs)
        self.bert = BertModel(config)
        self.activation = get_activation(config.hidden_act)
        
        self.dr_transform = nn.Linear(config.hidden_size, config.hidden_size)
        self.dr_layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dr_projector = nn.Linear(config.hidden_size, config.hidden_size)

    @torch.no_grad()
    def init_dr_head(self):
        torch.nn.init.eye_(self.dr_transform.weight)
        torch.nn.init.eye_(self.dr_projector.weight)
        
    @delegates(BertModel.__call__)
    def forward(
        self, 
        input_ids:Optional[torch.Tensor]=None, 
        attention_mask:Optional[torch.Tensor]=None,
        **kwargs
    ):
        o = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )
        rep = self.dr_transform(o[0])
        rep = self.activation(rep)
        rep = self.dr_layer_norm(rep)
        rep = self.dr_projector(rep)
        return o, F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)
    

# %% ../../nbs/30_models.BBB0XX.ipynb 19
class BRT009(BertPreTrainedModel):
    use_generation,use_representation = False,True
    _tied_weights_keys = ["encoder.bert"]
    
    def __init__(
        self,
        config,
        bsz:Optional[int]=None,
        tn_targ:Optional[int]=None,
        margin:Optional[float]=0.3,
        tau:Optional[float]=0.1,
        apply_softmax:Optional[bool]=False,
        n_negatives:Optional[int]=5,
        use_encoder_parallel:Optional[bool]=True,
        *args, 
        **kwargs
    ):
        super().__init__(config, *args, **kwargs)
        store_attr('use_encoder_parallel')
        self.encoder = BRT009Encoder(config)
        self.loss_fn = MultiTriplet(margin=margin, n_negatives=n_negatives, tau=tau, 
                                    apply_softmax=apply_softmax, reduce='mean')
        self.post_init()
        self.remap_post_init()
        
    def init_dr_head(self):
        self.encoder.init_dr_head()
        
    def remap_post_init(self):
        self.bert = self.encoder.bert

    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_data2ptr:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        plbl2data_data2ptr:Optional[torch.Tensor]=None,
        plbl2data_idx:Optional[torch.Tensor]=None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        if self.use_encoder_parallel: 
            encoder = nn.DataParallel(module=self.encoder)
        else: encoder = self.encoder
        
        data_o, data_repr = encoder(data_input_ids, data_attention_mask, 
                                    output_attentions=output_attentions, 
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict)

        loss, lbl2data_repr = None, None
        if lbl2data_input_ids is not None:
            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  
                                                output_attentions=output_attentions, 
                                                output_hidden_states=output_hidden_states,
                                                return_dict=return_dict)
            
            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, 
                                plbl2data_data2ptr, plbl2data_idx, **kwargs)

        if not return_dict:
            o = (data_repr, lbl2data_repr)
            return ((loss,) + o) if loss is not None else o

        return XCModelOutput(
            loss=loss,
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
        )
        

# %% ../../nbs/30_models.BBB0XX.ipynb 26
class BRT023Encoder(BertPreTrainedModel):
    
    def __init__(
        self, 
        config,
        normalize:Optional[bool]=False,
        use_ln:Optional[bool]=False,
        *args, 
        **kwargs
    ):
        super().__init__(config, *args, **kwargs)
        store_attr('normalize,use_ln')
        self.bert = BertModel(config)
        
        self.transform = nn.Linear(config.hidden_size, config.hidden_size)
        self.projector = nn.Linear(config.hidden_size, config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-6) if use_ln else None
        self.activation = get_activation(config.hidden_act)

    @torch.no_grad()
    def init_dr_head(self):
        torch.nn.init.eye_(self.transform.weight)
        torch.nn.init.eye_(self.projector.weight)
        
    @delegates(BertModel.__call__)
    def forward(
        self, 
        input_ids:Optional[torch.Tensor]=None, 
        attention_mask:Optional[torch.Tensor]=None,
        **kwargs
    ):
        o = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )
        rep = self.transform(o[0])
        if self.use_ln: 
            rep = self.layer_norm(rep)
        rep = self.projector(rep)
        rep = Pooling.mean_pooling(rep, attention_mask)
        
        return o, F.normalize(rep, dim=1) if self.normalize else rep
        

# %% ../../nbs/30_models.BBB0XX.ipynb 27
class BRT023(BertPreTrainedModel):
    use_generation,use_representation = False,True
    _tied_weights_keys = ["encoder.bert"]
    
    def __init__(
        self,
        config,
        normalize:Optional[bool]=False,
        use_layer_norm:Optional[bool]=False,
        use_encoder_parallel:Optional[bool]=True,
        *args, **kwargs
    ):
        super().__init__(config, *args, **kwargs)
        store_attr('use_encoder_parallel')
        self.encoder = BRT023Encoder(config, normalize=normalize, use_ln=use_layer_norm)
        self.loss_fn = MarginMSEWithNegatives()
        self.post_init(); self.remap_post_init()
        
    def init_dr_head(self):
        self.encoder.init_dr_head()
        
    def remap_post_init(self):
        self.bert = self.encoder.bert

    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        
        lbl2data_input_ids:Optional[torch.Tensor]=None,
        lbl2data_attention_mask:Optional[torch.Tensor]=None,
        lbl2data_scores:Optional[torch.Tensor]=None,

        neg2data_input_ids:Optional[torch.Tensor]=None,
        neg2data_attention_mask:Optional[torch.Tensor]=None,
        neg2data_scores:Optional[torch.Tensor]=None,
        
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        if self.use_encoder_parallel: 
            encoder = nn.DataParallel(module=self.encoder)
        else: encoder = self.encoder
        
        data_o, data_repr = encoder(data_input_ids, data_attention_mask, 
                                    output_attentions=output_attentions, 
                                    output_hidden_states=output_hidden_states,
                                    return_dict=return_dict)
        
        loss, lbl2data_repr, neg2data_repr = None, None, None
        if (
            lbl2data_input_ids is not None and 
            neg2data_input_ids is not None
        ):
            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  
                                                output_attentions=output_attentions, 
                                                output_hidden_states=output_hidden_states,
                                                return_dict=return_dict)
            
            neg2data_o, neg2data_repr = encoder(neg2data_input_ids, neg2data_attention_mask,  
                                                output_attentions=output_attentions, 
                                                output_hidden_states=output_hidden_states,
                                                return_dict=return_dict)

            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_scores, neg2data_repr, neg2data_scores, **kwargs)

        if not return_dict:
            o = (data_repr, lbl2data_repr)
            return ((loss,) + o) if loss is not None else o

        return XCModelOutput(
            loss=loss,
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
            neg2data_repr=neg2data_repr,
        )
        
