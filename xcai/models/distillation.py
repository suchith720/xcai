# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/17_models.distillation.ipynb.

# %% auto 0
__all__ = ['TCHOutput', 'TCHConfig', 'TCH001', 'TCH002', 'TCH003', 'TCH004', 'DTLOutput', 'DTLConfig', 'DTL001', 'DTL002',
           'DTL003', 'DTL004']

# %% ../../nbs/17_models.distillation.ipynb 2
import torch, numpy as np, torch.nn.functional as F, torch.nn as nn
from typing import Optional
from dataclasses import dataclass
from types import MethodType

from ..core import store_attr
from ..losses import Cosine, MultiTriplet, MarginMSEWithNegatives, MultiTripletWithNegatives
from .PPP0XX import XCModelOutput
from .oak import OAK001
from .radga import RADOutput
from ..bandits import *

from transformers import DistilBertPreTrainedModel,DistilBertConfig
from transformers.utils.generic import ModelOutput

# %% ../../nbs/17_models.distillation.ipynb 13
@dataclass
class TCHOutput(ModelOutput):
    data_repr: Optional[torch.FloatTensor] = None
    lbl2data_repr: Optional[torch.FloatTensor] = None
    neg2data_repr: Optional[torch.FloatTensor] = None
    

# %% ../../nbs/17_models.distillation.ipynb 15
class TCHConfig(DistilBertConfig):

    def __init__(
        self,
        n_data:Optional[int]=None,
        n_lbl:Optional[int]=None,
        n_neg:Optional[int]=None,
        embed_dim:Optional[int]=None,
        normalize:Optional[bool]=True,
        **kwargs,
    ):
        self.n_data, self.n_lbl, self.n_neg, self.embed_dim, self.normalize = n_data, n_lbl, n_neg, embed_dim, normalize
        super().__init__(**kwargs)
        

# %% ../../nbs/17_models.distillation.ipynb 17
class TCH001(DistilBertPreTrainedModel):

    def __init__(self, config:TCHConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.data_repr = nn.Embedding(config.n_data, config.dim)
        self.lbl_repr = nn.Embedding(config.n_lbl, config.dim)
        self.register_buffer("neg2lbl_idx", None if config.n_neg is None else torch.arange(config.n_neg), persistent=True)

    @torch.no_grad()
    def set_neg2lbl_idx_mapping(self, neg2lbl_idx:torch.Tensor):
        assert neg2lbl_idx.shape[0] == self.neg2lbl_idx.shape[0], f"Shape mismatch, `neg2lbl_idx` should have {self.neg2lbl_idx.shape[0]} elements."
        self.neg2lbl_idx.copy_(neg2lbl_idx)

    def get_lbl_embeddings(self):
        return self.lbl_repr.weight

    def get_data_embeddings(self):
        return self.data_repr.weight

    @torch.no_grad()
    def init_embeddings(self, data_repr:torch.Tensor, lbl_repr:torch.Tensor):
        self.data_repr.weight.copy_(data_repr)
        self.lbl_repr.weight.copy_(lbl_repr)

    def freeze_embeddings(self):
        self.data_repr.requires_grad_(False)
        self.lbl_repr.requires_grad_(False)

    def freeze_data_embeddings(self):
        self.data_repr.requires_grad_(False)

    def unfreeze_embeddings(self):
        self.data_repr.requires_grad_(True)
        self.lbl_repr.requires_grad_(True)

    def forward(
        self,
        data_idx:torch.Tensor,
        lbl2data_idx:torch.Tensor,
        neg2data_idx:Optional[torch.Tensor]=None,
        **kwargs,
    ):
        return TCHOutput(
            data_repr=self.data_repr(data_idx),
            lbl2data_repr= self.lbl_repr(lbl2data_idx),
            neg2data_repr=None if neg2data_idx is None else self.lbl_repr(self.neg2lbl_idx[neg2data_idx]) 
        )
        

# %% ../../nbs/17_models.distillation.ipynb 30
class TCH002(DistilBertPreTrainedModel):

    def __init__(self, config:TCHConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.data_repr = nn.Embedding(config.n_data, config.dim)
        self.lbl_repr = nn.Embedding(config.n_lbl, config.dim)
        self.lbl_embeddings = nn.Embedding(config.n_lbl, config.dim)

    def get_lbl_embeddings(self):
        return self.lbl_repr.weight + self.lbl_embeddings.weight

    def get_data_embeddings(self):
        return self.data_repr.weight

    @torch.no_grad()
    def init_representations(self, data_repr:torch.Tensor, lbl_repr:torch.Tensor):
        self.data_repr.weight.copy_(data_repr)
        self.lbl_repr.weight.copy_(lbl_repr)

    @torch.no_grad()
    def init_lbl_embeddings(self):
        nn.init.zeros_(self.lbl_embeddings.weight)

    def freeze_representations(self):
        self.data_repr.requires_grad_(False)
        self.lbl_repr.requires_grad_(False)

    def unfreeze_representations(self):
        self.data_repr.requires_grad_(True)
        self.lbl_repr.requires_grad_(True)

    def forward(
        self,
        data_idx:torch.Tensor,
        lbl2data_idx:torch.Tensor,
        neg2data_idx:Optional[torch.Tensor]=None,
        **kwargs,
    ):
        data_repr = self.data_repr(data_idx)
        lbl2data_repr = self.lbl_repr(lbl2data_idx) + self.lbl_embeddings(lbl2data_idx)
        neg2data_repr = None if neg2data_idx is None else self.lbl_repr(neg2data_idx) + self.lbl_embeddings(neg2data_idx)
        return TCHOutput(
            data_repr=data_repr,
            lbl2data_repr=lbl2data_repr,
            neg2data_repr=neg2data_repr,
        )
        

# %% ../../nbs/17_models.distillation.ipynb 38
class TCH003(DistilBertPreTrainedModel):

    def __init__(self, config:TCHConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.data_repr = nn.Embedding(config.n_data, config.dim)

    def get_data_embeddings(self):
        return self.data_repr.weight

    @torch.no_grad()
    def init_embeddings(self, data_repr:torch.Tensor):
        self.data_repr.weight.copy_(data_repr)

    def freeze_embeddings(self):
        self.data_repr.requires_grad_(False)

    def unfreeze_representations(self):
        self.data_repr.requires_grad_(True)

    def forward(
        self,
        data_idx:torch.Tensor,
        **kwargs,
    ):
        data_repr = self.data_repr(data_idx)
        data_repr = F.normalize(data_repr, dim=1) if self.config.normalize else data_repr
        return TCHOutput(
            data_repr=data_repr,
        )
        

# %% ../../nbs/17_models.distillation.ipynb 46
class TCH004(DistilBertPreTrainedModel):

    def __init__(self, config:TCHConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.data_repr = nn.Embedding(config.n_data, config.embed_dim)
        self.transform = nn.Linear(config.embed_dim, config.dim)

    def get_data_embeddings(self):
        return self.data_repr.weight

    @torch.no_grad()
    def init_embeddings(self, data_repr:torch.Tensor):
        self.data_repr.weight.copy_(data_repr)

    def freeze_embeddings(self):
        self.data_repr.requires_grad_(False)

    def unfreeze_representations(self):
        self.data_repr.requires_grad_(True)

    @torch.no_grad()
    def init_transform(self, embed:Optional[torch.Tensor]=None):
        if embed is None: nn.init.eye_(self.transform.weight)
        else: self.transform.weight.copy_(embed)
        nn.init.zeros_(self.transform.bias)

    def forward(
        self,
        data_idx:torch.Tensor,
        **kwargs,
    ):
        data_repr = self.transform(self.data_repr(data_idx))
        data_repr = F.normalize(data_repr, dim=1) if self.config.normalize else data_repr
        return TCHOutput(
            data_repr=data_repr,
        )
        

# %% ../../nbs/17_models.distillation.ipynb 55
@dataclass
class DTLOutput(ModelOutput):
    loss:Optional[torch.FloatTensor]=None
    data_repr: Optional[torch.FloatTensor] = None
    data_fused_repr:Optional[torch.FloatTensor] = None
    lbl2data_repr: Optional[torch.FloatTensor] = None
    lbl2data_fused_repr:Optional[torch.FloatTensor] = None
    

# %% ../../nbs/17_models.distillation.ipynb 57
class DTLConfig(DistilBertConfig):

    def __init__(
        self,
        margin:Optional[float]=0.3,
        tau:Optional[float]=0.1,
        apply_softmax:Optional[bool]=False,
        n_negatives:Optional[int]=5,
        teacher_data_student_label_loss_weight:Optional[float]=1.0,
        student_data_teacher_label_loss_weight:Optional[float]=1.0,
        data_mse_loss_weight:Optional[float]=0.1,
        label_mse_loss_weight:Optional[float]=0.1,
        teacher_data_repr_name:Optional[str]='data_repr',
        student_data_repr_name:Optional[str]='data_fused_repr',
        teacher_lbl2data_repr_name:Optional[str]='lbl2data_repr',
        student_lbl2data_repr_name:Optional[str]='lbl2data_repr',
        teacher_neg2data_repr_name:Optional[str]='neg2data_repr',
        student_neg2data_repr_name:Optional[str]='neg2data_repr',
        bandit_learning_rate:Optional[float]=0.01,
        bandit_minimum_value:Optional[float]=0.1,
        bandit_collector:Optional[int]=20,
        **kwargs,
    ):
        self.margin, self.tau, self.apply_softmax, self.n_negatives = margin, tau, apply_softmax, n_negatives
        self.teacher_data_student_label_loss_weight = teacher_data_student_label_loss_weight
        self.student_data_teacher_label_loss_weight = student_data_teacher_label_loss_weight
        self.data_mse_loss_weight, self.label_mse_loss_weight = data_mse_loss_weight, label_mse_loss_weight
        self.teacher_data_repr_name, self.student_data_repr_name = teacher_data_repr_name, student_data_repr_name
        self.teacher_lbl2data_repr_name, self.student_lbl2data_repr_name = teacher_lbl2data_repr_name, student_lbl2data_repr_name
        self.teacher_neg2data_repr_name, self.student_neg2data_repr_name = teacher_neg2data_repr_name, student_neg2data_repr_name
        self.bandit_learning_rate, self.bandit_minimum_value = bandit_learning_rate, bandit_minimum_value
        self.bandit_collector = bandit_collector
        super().__init__(**kwargs)
        

# %% ../../nbs/17_models.distillation.ipynb 59
class DTL001(DistilBertPreTrainedModel):
    use_representation,use_generation = True,False
    _tied_weights_keys = ["m_student.encoder.distilbert"]
    
    def __init__(
        self,
        config,
        m_student:nn.Module,
        m_teacher:nn.Module,
        **kwargs
    ):
        super().__init__(config, **kwargs)
        store_attr('m_student,m_teacher')
        self.mse_loss_fn = nn.MSELoss()
        self.rep_loss_fn = MultiTriplet(margin=config.margin, n_negatives=config.n_negatives, tau=config.tau, 
                                        apply_softmax=config.apply_softmax, reduce='mean')

        if hasattr(m_student, 'get_label_representation'):
            def get_label_representation(
                self,
                data_idx:Optional[torch.Tensor]=None,
                data_input_ids:Optional[torch.Tensor]=None,
                data_attention_mask:Optional[torch.Tensor]=None,
                **kwargs
            ):
                return self.m_student.get_label_representation(data_idx, data_input_ids, data_attention_mask, **kwargs)
            self.get_label_representation = MethodType(get_label_representation, self)

    def combine_losses(self, student_loss:float, tdsl_loss:float, sdtl_loss:float, dm_loss:float, lm_loss:float, 
                       student_data_repr:Optional[torch.Tensor]=None, student_lbl2data_repr:Optional[torch.Tensor]=None, **kwargs):
        loss = student_loss
        loss += self.config.teacher_data_student_label_loss_weight * tdsl_loss
        loss += self.config.student_data_teacher_label_loss_weight * sdtl_loss
        loss += self.config.data_mse_loss_weight * dm_loss + self.config.label_mse_loss_weight * lm_loss
        return loss
        
    def forward(
        self,
        data_idx:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        **kwargs
    ):
        student_o = self.m_student(lbl2data_idx=lbl2data_idx, **kwargs)
        student_data_repr = getattr(student_o, self.config.student_data_repr_name, None)
        student_lbl2data_repr = getattr(student_o, self.config.student_lbl2data_repr_name, None)

        loss = None
        if student_o.loss is not None:
            with torch.no_grad(): teacher_o = self.m_teacher(data_idx=data_idx, lbl2data_idx=lbl2data_idx)
            teacher_data_repr = getattr(student_o, self.config.teacher_data_repr_name, None)
            teacher_lbl2data_repr = getattr(student_o, self.config.teacher_lbl2data_repr_name, None)

            tdsl_loss = 0.0
            if teacher_data_repr is not None and student_lbl2data_repr is not None and self.config.teacher_data_student_label_loss_weight > 0:
                tdsl_loss = self.rep_loss_fn(teacher_data_repr, student_lbl2data_repr, kwargs['lbl2data_data2ptr'], lbl2data_idx, 
                                             kwargs['plbl2data_data2ptr'], kwargs['plbl2data_idx'], **kwargs)

            sdtl_loss = 0.0
            if student_data_repr is not None and teacher_lbl2data_repr is not None and self.config.student_data_teacher_label_loss_weight > 0:
                sdtl_loss = self.rep_loss_fn(student_data_repr, teacher_lbl2data_repr, kwargs['lbl2data_data2ptr'], lbl2data_idx, 
                                             kwargs['plbl2data_data2ptr'], kwargs['plbl2data_idx'], **kwargs)

            dm_loss = 0.0
            if teacher_data_repr is not None and student_data_repr is not None and self.config.data_mse_loss_weight > 0:
                dm_loss = self.mse_loss_fn(teacher_data_repr, student_data_repr)

            lm_loss = 0.0
            if teacher_lbl2data_repr is not None and student_lbl2data_repr is not None and self.config.label_mse_loss_weight > 0:
                lm_loss = self.mse_loss_fn(teacher_lbl2data_repr, student_lbl2data_repr)

            loss = self.combine_losses(student_o.loss, tdsl_loss, sdtl_loss, dm_loss, lm_loss, 
                                       student_data_repr, student_lbl2data_repr, lbl2data_idx=lbl2data_idx, **kwargs)

        return DTLOutput(
            loss=loss,
            data_repr=getattr(student_o, 'data_repr', None),
            data_fused_repr=getattr(student_o, 'data_fused_repr', None),
            lbl2data_repr=getattr(student_o, 'lbl2data_repr', None),
            lbl2data_fused_repr=getattr(student_o, 'lbl2data_fused_repr', None),
        )
        

# %% ../../nbs/17_models.distillation.ipynb 70
class DTL002(DTL001):
    use_representation,use_generation = True,False
    _tied_weights_keys = ["m_student.encoder.distilbert"]
    
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        rest_init = [self.config.teacher_data_student_label_loss_weight, self.config.student_data_teacher_label_loss_weight, 
                     self.config.data_mse_loss_weight, self.config.label_mse_loss_weight]
        self.loss_weights = RLLossWeightsCumuluative(num_samples=len(rest_init), reward_func=AccMiniBatch, lr=self.config.bandit_learning_rate, 
                                                     collector=self.config.bandit_collector, std=0.1, min=self.config.bandit_minimum_value,
                                                     rest_init=rest_init)

    def combine_losses(self, student_loss:float, tdsl_loss:float, sdtl_loss:float, dm_loss:float, lm_loss:float, 
                       student_data_repr:Optional[torch.Tensor]=None, student_lbl2data_repr:Optional[torch.Tensor]=None, **kwargs):
        ws = self.loss_weights.sample(kwargs['lbl2data_idx'].device)
        if self.training:
            self.loss_weights.step(student_data_repr, student_lbl2data_repr, kwargs['lbl2data_data2ptr'], 
                                   kwargs['lbl2data_idx'], kwargs['plbl2data_data2ptr'], 
                                   kwargs['plbl2data_idx'])
        loss = student_loss + ws[0]*tdsl_loss + ws[1]*sdtl_loss + ws[2]*dm_loss + ws[3]*lm_loss
        return loss
        

# %% ../../nbs/17_models.distillation.ipynb 81
class DTL003(DTL001):
    use_representation,use_generation = True,False
    _tied_weights_keys = ["m_student.encoder.distilbert"]
    
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        self.rep_loss_fn = self.rep_loss_fn = MarginMSEWithNegatives()

    def forward(
        self,
        data_idx:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        neg2data_idx:Optional[torch.Tensor]=None,
        **kwargs
    ):
        student_o = self.m_student(lbl2data_idx=lbl2data_idx, **kwargs)
        student_data_repr = getattr(student_o, self.config.student_data_repr_name, None)
        student_lbl2data_repr = getattr(student_o, self.config.student_lbl2data_repr_name, None)
        student_neg2data_repr = getattr(student_o, self.config.student_neg2data_repr_name, None)

        loss = None
        if student_o.loss is not None:
            with torch.no_grad(): 
                teacher_o = self.m_teacher(data_idx=data_idx, lbl2data_idx=lbl2data_idx, neg2data_idx=neg2data_idx)
            teacher_data_repr = getattr(student_o, self.config.teacher_data_repr_name, None)
            teacher_lbl2data_repr = getattr(student_o, self.config.teacher_lbl2data_repr_name, None)
            teacher_neg2data_repr = getattr(student_o, self.config.teacher_neg2data_repr_name, None)

            tdsl_loss = 0.0
            if (
                teacher_data_repr is not None and student_lbl2data_repr is not None and 
                student_neg2data_repr is not None and self.config.teacher_data_student_label_loss_weight > 0
            ):
                tdsl_loss = self.rep_loss_fn(teacher_data_repr, student_lbl2data_repr, kwargs['lbl2data_scores'], 
                                             student_neg2data_repr, kwargs['neg2data_scores'], **kwargs)
            
            sdtl_loss = 0.0
            if (
                student_data_repr is not None and teacher_lbl2data_repr is not None and 
                teacher_neg2data_repr is not None and self.config.student_data_teacher_label_loss_weight > 0
            ):
                sdtl_loss = self.rep_loss_fn(student_data_repr, teacher_lbl2data_repr, kwargs['lbl2data_scores'], 
                                             teacher_neg2data_repr, kwargs['neg2data_scores'], **kwargs)
                
            dm_loss = 0.0
            if teacher_data_repr is not None and student_data_repr is not None and self.config.data_mse_loss_weight > 0:
                dm_loss = self.mse_loss_fn(teacher_data_repr, student_data_repr)

            lm_loss = 0.0
            if teacher_lbl2data_repr is not None and student_lbl2data_repr is not None and self.config.label_mse_loss_weight > 0:
                lm_loss += self.mse_loss_fn(teacher_lbl2data_repr, student_lbl2data_repr)
                
            if teacher_neg2data_repr is not None and student_neg2data_repr is not None and self.config.label_mse_loss_weight > 0:
                lm_loss += self.mse_loss_fn(teacher_neg2data_repr, student_neg2data_repr)

            loss = self.combine_losses(student_o.loss, tdsl_loss, sdtl_loss, dm_loss, lm_loss, 
                                       student_data_repr, student_lbl2data_repr, lbl2data_idx=lbl2data_idx, **kwargs)
            
        return DTLOutput(
            loss=loss,
            data_repr=getattr(student_o, 'data_repr', None),
            data_fused_repr=getattr(student_o, 'data_fused_repr', None),
            lbl2data_repr=getattr(student_o, 'lbl2data_repr', None),
            lbl2data_fused_repr=getattr(student_o, 'lbl2data_fused_repr', None),
        )
        

# %% ../../nbs/17_models.distillation.ipynb 93
class DTL004(DTL001):
    use_representation,use_generation = True,False
    _tied_weights_keys = ["m_student.encoder.distilbert"]
    
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        self.rep_loss_fn = MultiTripletWithNegatives(margin=config.margin, n_negatives=config.n_negatives, 
                                                     tau=config.tau, apply_softmax=config.apply_softmax, 
                                                     reduce='mean')
        
    def forward(
        self,
        data_idx:Optional[torch.Tensor]=None,
        lbl2data_idx:Optional[torch.Tensor]=None,
        neg2data_idx:Optional[torch.Tensor]=None,
        **kwargs
    ):
        student_o = self.m_student(lbl2data_idx=lbl2data_idx, **kwargs)
        student_data_repr = getattr(student_o, self.config.student_data_repr_name, None)
        student_lbl2data_repr = getattr(student_o, self.config.student_lbl2data_repr_name, None)
        student_neg2data_repr = getattr(student_o, self.config.student_neg2data_repr_name, None)

        loss = None
        if student_o.loss is not None:
            with torch.no_grad(): 
                teacher_o = self.m_teacher(data_idx=data_idx, lbl2data_idx=lbl2data_idx, neg2data_idx=neg2data_idx)
            teacher_data_repr = getattr(student_o, self.config.teacher_data_repr_name, None)
            teacher_lbl2data_repr = getattr(student_o, self.config.teacher_lbl2data_repr_name, None)
            teacher_neg2data_repr = getattr(student_o, self.config.teacher_neg2data_repr_name, None)

            tdsl_loss = 0.0
            if (
                teacher_data_repr is not None and student_lbl2data_repr is not None and 
                student_neg2data_repr is not None and self.config.teacher_data_student_label_loss_weight > 0
            ):
                tdsl_loss = self.rep_loss_fn(teacher_data_repr, pos_targ=student_lbl2data_repr, 
                                             n_pos=kwargs['lbl2data_data2ptr'], pos_idx=kwargs['lbl2data_idx'], 
                                             neg_targ=student_neg2data_repr, n_neg=kwargs['neg2data_data2ptr'], 
                                             neg_idx=kwargs['neg2data_idx'], n_ppos=kwargs['plbl2data_data2ptr'], 
                                             ppos_idx=kwargs['plbl2data_idx'], **kwargs)
                
            sdtl_loss = 0.0
            if (
                student_data_repr is not None and teacher_lbl2data_repr is not None and 
                teacher_neg2data_repr is not None and self.config.student_data_teacher_label_loss_weight > 0
            ):
                sdtl_loss = self.rep_loss_fn(student_data_repr, pos_targ=teacher_lbl2data_repr, 
                                             n_pos=kwargs['lbl2data_data2ptr'], pos_idx=kwargs['lbl2data_idx'], 
                                             neg_targ=teacher_neg2data_repr, n_neg=kwargs['neg2data_data2ptr'], 
                                             neg_idx=kwargs['neg2data_idx'], n_ppos=kwargs['plbl2data_data2ptr'], 
                                             ppos_idx=kwargs['plbl2data_idx'], **kwargs)
                
            dm_loss = 0.0
            if teacher_data_repr is not None and student_data_repr is not None and self.config.data_mse_loss_weight > 0:
                dm_loss = self.mse_loss_fn(teacher_data_repr, student_data_repr)

            lm_loss = 0.0
            if teacher_lbl2data_repr is not None and student_lbl2data_repr is not None and self.config.label_mse_loss_weight > 0:
                lm_loss += self.mse_loss_fn(teacher_lbl2data_repr, student_lbl2data_repr)
                
            if teacher_neg2data_repr is not None and student_neg2data_repr is not None and self.config.label_mse_loss_weight > 0:
                lm_loss += self.mse_loss_fn(teacher_neg2data_repr, student_neg2data_repr)

            loss = self.combine_losses(student_o.loss, tdsl_loss, sdtl_loss, dm_loss, lm_loss, 
                                       student_data_repr, student_lbl2data_repr, lbl2data_idx=lbl2data_idx, **kwargs)

        return DTLOutput(
            loss=loss,
            data_repr=getattr(student_o, 'data_repr', None),
            data_fused_repr=getattr(student_o, 'data_fused_repr', None),
            lbl2data_repr=getattr(student_o, 'lbl2data_repr', None),
            lbl2data_fused_repr=getattr(student_o, 'lbl2data_fused_repr', None),
        )
        
