# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/22_models.lora.ipynb.

# %% auto 0
__all__ = ['LOR001']

# %% ../../nbs/22_models.lora.ipynb 2
import torch, numpy as np, os, pickle
from typing import Optional
import torch.nn as nn
from dataclasses import dataclass

from ..core import store_attr
from ..losses import MultiTriplet

from .modeling_utils import XCModelOutput, Parameters

from transformers import DistilBertPreTrainedModel,DistilBertConfig
from transformers.utils.generic import ModelOutput

from peft import (
    LoraConfig, 
    get_peft_model, 
    TaskType,
    PeftModel
)

# %% ../../nbs/22_models.lora.ipynb 12
class LOR001(DistilBertPreTrainedModel):
    use_representation,use_generation = True,False
    _tied_weights_keys = ["peft_model.base_model.model.encoder.distilbert"]

    def __init__(
        self, config, model, peft_config, 
        
        pred_meta_prefix:Optional[str]=None, 
        
        num_batch_labels:Optional[int]=None, 
        batch_size:Optional[int]=None,
        margin:Optional[float]=0.3,
        num_negatives:Optional[int]=5,
        tau:Optional[float]=0.1,
        apply_softmax:Optional[bool]=True,
        
        meta_loss_weight:Optional[float]=0.1,
        
        **kwargs
    ):
        super().__init__(config, **kwargs)
        store_attr('pred_meta_prefix,meta_loss_weight')
        self.peft_model = get_peft_model(model, peft_config)
        self.rep_loss_fn = MultiTriplet(bsz=batch_size, tn_targ=num_batch_labels, margin=margin, n_negatives=num_negatives, 
                                        tau=tau, apply_softmax=apply_softmax, reduce='mean')

        self._mark_entire_model_as_trainable()

    def _mark_entire_model_as_trainable(self):
        for p in self.peft_model.parameters(): p.requires_grad_(True)

    def _mark_only_adapters_as_trainable(self):
        self.peft_model.base_model._mark_only_adapters_as_trainable(self.peft_model)

    def forward(
        self,
        data_input_ids:Optional[torch.Tensor]=None,
        data_attention_mask:Optional[torch.Tensor]=None,
        **kwargs
    ):  
        data_o = self.peft_model(data_input_ids, data_attention_mask, **kwargs)

        loss = data_o.loss
        meta_inputs = Parameters.from_meta_pred_prefix(self.pred_meta_prefix, **kwargs)
        if meta_inputs and loss is not None:
            self._mark_only_adapters_as_trainable()
            meta_inputs = next(iter(meta_inputs.values()))
        
            idx = torch.where(meta_inputs['data2ptr'])[0]
            if len(idx) > 0:
                meta_o = self.peft_model(data_input_ids=meta_inputs['input_ids'], data_attention_mask=meta_inputs['attention_mask'])
                m_loss = self.rep_loss_fn(data_o.data_repr[idx], meta_o.data_repr, meta_inputs['data2ptr'][idx], meta_inputs['idx'], 
                                      meta_inputs['pdata2ptr'][idx], meta_inputs['pidx'])
                loss += self.meta_loss_weight * m_loss
                
        self._mark_entire_model_as_trainable()
        
        return XCModelOutput(
            loss=loss,
            data_repr=data_o.data_repr,
            lbl2data_repr=data_o.lbl2data_repr,
        )
        
