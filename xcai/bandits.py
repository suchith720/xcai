# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/31_bandits.ipynb.

# %% auto 0
__all__ = ['get_sparse_matrix', 'RLLossWeights', 'RLLossWeightsCumuluative', 'AccMiniBatch']

# %% ../nbs/31_bandits.ipynb 2
import torch, numpy as np
from typing import Optional

# %% ../nbs/31_bandits.ipynb 9
def get_sparse_matrix(data_idx:torch.Tensor, n_data:torch.Tensor, scores:Optional[torch.Tensor]=None):
    data_ptr = torch.cat([torch.zeros(1, device=n_data.device, dtype=n_data.dtype), n_data.cumsum(0)])
    if scores is None: scores = torch.ones_like(data_idx)
    if data_idx.shape != scores.shape: raise ValueError(f'`data_idx` and `scores` should have same shape.')
    return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)
    

# %% ../nbs/31_bandits.ipynb 10
class RLLossWeights(torch.nn.Module):
    def __init__(self, num_samples, std=0.1, lr=0.001, reward_func=None,
                 collector=10, min=0.1, rest_init=0.1) -> None:
        super().__init__()
        init = np.ones(num_samples)
        init[:] = rest_init
        self.reward_func = reward_func
        self.collector = collector
        self.lr = lr
        self.num_samples = num_samples
        self.mu = torch.nn.Parameter(torch.Tensor(init))
        self.std = torch.nn.Parameter(torch.Tensor(np.ones(num_samples)*std),
                                      requires_grad=False)
        self.dist = torch.distributions.normal.Normal(self.mu, self.std)
        self.min = min
        self.w = None
        self.reset_metrics()

    def reset_metrics(self):
        self.collect_size = 0
        self.collect_value = 0
        self.step_counter = 0

    def sample(self, device="cpu"):
        if self.w is None:
            self.w = self.clip(self.dist.sample())
        return self.w.to(device)

    def zero_grad(self):
        self.mu.grad = None
        self.collect_size = 0
        self.collect_value = 0
        self.w = None

    def collect(self, pred, gt):
        size = pred.size(0)
        rewd = self.reward_func(pred, gt)  # TODO
        self.collect_value += rewd
        self.collect_size += size
        pass

    def step(
        self,
        inp:torch.FloatTensor,
        targ:torch.LongTensor, 
        n_inp2targ:torch.LongTensor,
        inp2targ_idx:torch.LongTensor,
        n_pinp2targ:torch.LongTensor,
        pinp2targ_idx:torch.LongTensor
    ):
        pred = inp@targ.T
        
        _, idx = torch.unique(torch.cat([inp2targ_idx, pinp2targ_idx]), return_inverse=True)
        gt = get_sparse_matrix(idx[len(inp2targ_idx):], n_pinp2targ).to_dense()[:, idx[:len(inp2targ_idx)]]
    
        self.step_counter += 1
        self.collect(pred, gt)
        if self.step_counter % self.collector == 0:
            loss = -self.dist.log_prob(self.w)*self.curr_reward
            loss = torch.sum(loss).backward()
            self.mu.data = self.mu - self.lr * self.mu.grad.data
            self.dist.loc = self.clip(self.mu)
            self.step_counter = 0
            self.zero_grad()

    def clip(self, vect):
        return torch.clamp(vect, min=self.min)

    @property
    def curr_reward(self):
        return self.collect_value/self.collect_size

    def extra_repr(self):
        return f"{self.mu}"
        

# %% ../nbs/31_bandits.ipynb 11
class RLLossWeightsCumuluative(RLLossWeights):
    def __init__(self, num_samples=1, std=0.01, lr=0.01, m=0.8,
                 reward_func=None, collector=10, min=0.1, rest_init=0.1) -> None:
        self.m = m
        super().__init__(num_samples, std, lr, reward_func, collector, min, rest_init)

    def reset_metrics(self):
        super().reset_metrics()
        self.reward_prev = None
        self.in_warmup = True

    def step(
        self,
        inp:torch.FloatTensor,
        targ:torch.LongTensor, 
        n_inp2targ:torch.LongTensor,
        inp2targ_idx:torch.LongTensor,
        n_pinp2targ:torch.LongTensor,
        pinp2targ_idx:torch.LongTensor
    ):
        pred = inp@targ.T
        
        _, idx = torch.unique(torch.cat([inp2targ_idx, pinp2targ_idx]), return_inverse=True)
        gt = get_sparse_matrix(idx[len(inp2targ_idx):], n_pinp2targ).to_dense()[:, idx[:len(inp2targ_idx)]]
    
        self.step_counter += 1
        self.collect(pred, gt)

        if self.step_counter % self.collector == 0:
            if self.in_warmup:
                self.in_warmup = False
                self.reward_prev = self.curr_reward
            else:
                reward = self.curr_reward - self.reward_prev
                loss = -self.dist.log_prob(self.w).sum()
                loss.backward()
                grad = self.mu.grad.data*reward
                grad = torch.clip(torch.nan_to_num(grad), min=-1, max=1)
                self.mu.data = self.mu - self.lr * grad
            self.dist.loc = self.clip(self.mu)
            self.step_counter = 0
            self.reward_prev = (1-self.m)*self.curr_reward + \
                self.m*self.reward_prev
            self.zero_grad()
            

# %% ../nbs/31_bandits.ipynb 13
def AccMiniBatch(pred, gt):
    gt = gt.to(pred.device)
    indices = pred.topk(largest=True, dim=1, k=1)[1]
    return torch.sum(gt.gather(1, indices)).item()
    
