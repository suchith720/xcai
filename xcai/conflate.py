# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/43_conflation.ipynb.

# %% auto 0
__all__ = ['display_items', 'compare_items', 'get_conflated_text', 'Operations', 'Filter', 'Cluster', 'Conflation', 'SaveData',
           'load_data', 'perform_similarity_based_conflation_01', 'perform_phrase_based_conflation_02']

# %% ../nbs/43_conflation.ipynb 2
import numpy as np, pandas as pd, scipy.sparse as sp, torch, json, os, re, math
import matplotlib.pyplot as plt
from IPython.display import display

from tqdm.auto import tqdm
from typing import Optional, Union, Dict, List
from IPython.display import display
from xclib.utils.sparse import retain_topk
from collections import Counter

# %% ../nbs/43_conflation.ipynb 4
def display_items(matrix:sp.csr_matrix, info:Dict, n_data:Optional[int]=10, n_view_data:Optional[int]=20, 
                          seed:Optional[int]=1000):
    np.random.seed(seed)
    rnd_idx = np.random.permutation(matrix.shape[0])[:n_data]
    
    outputs = list()
    for idx in rnd_idx:
        sort_idx = np.argsort(matrix[idx].data)[:-n_view_data:-1]
        scores = matrix[idx].data[sort_idx]
        indices = matrix[idx].indices[sort_idx]
        labels = [info['text'][i] for i in indices]
        o = {
            "Substring": info['text'][idx],
            "Predictions": [(x, float(y)) for x,y in zip(labels, scores)]
        }
        outputs.append(o)
    return outputs
    

# %% ../nbs/43_conflation.ipynb 5
def compare_items(output_1, output_2):
    rows, index = [], []
    for x,y in zip(linker_output, pretrn_output):
        assert x["Substring"] == y["Substring"]
    
        index.extend([(x["Substring"], i+1) for i in range(len(x["Predictions"]))])
        rows.extend([(a[0], b[0]) for a,b in zip(x["Predictions"], y["Predictions"])])
        
    df = pd.DataFrame(
        rows,
        index=pd.MultiIndex.from_tuples(
            index,
            names=["Query", "Index"],
        ),
        columns=["Output 1", "Output 2"],
    )
    return df
    

# %% ../nbs/43_conflation.ipynb 6
def get_conflated_text(clusters:List, meta_info:Dict):
    texts = [[meta_info["text"][i] for i in c] for c in clusters]
    idx = np.argsort([len(c) for c in clusters])[::-1]
    return [texts[i] for i in idx]
        

# %% ../nbs/43_conflation.ipynb 7
class Operations:

    @staticmethod
    def invert(a):
        return np.divide(1.0, a, out=np.zeros_like(a), where=a!=0)

    @staticmethod
    def normalize(a):
        return a.multiply(np.sqrt(Operations.invert(a.sum(axis=1)))).multiply(np.sqrt(Operations.invert(a.sum(axis=0)))).tocsr()
        
    @staticmethod
    def compute_yty(mat:sp.csr_matrix, bsz:Optional[int]=1000, normalize=True):
        mat_t = mat.transpose()
        out = sp.vstack([mat[i:i+bsz]@mat_t for i in tqdm(range(0, mat.shape[0], bsz))])
        return Operations.normalize(out)

    @staticmethod
    def minimum(matrix:sp.csr_matrix):
        return np.array([matrix.data[i:j].min() for i,j in zip(matrix.indptr, matrix.indptr[1:])])

    @staticmethod
    def min_clamp(matrix:sp.csr_matrix, value:float, inplace:Optional[bool]=True):
        if not inplace:
            matrix = matrix.copy()
        matrix.data[matrix.data <= value] = 0
        matrix.eliminate_zeros()
        return matrix

    @staticmethod
    def max_clamp(matrix:sp.csr_matrix, value:float, inplace:Optional[bool]=True):
        if not inplace:
            matrix = matrix.copy()
        matrix.data[matrix.data >= value] = 0
        matrix.eliminate_zeros()
        return matrix

    @staticmethod
    def diff_threshold(matrix:sp.csr_matrix, value:float):
        diag = matrix.diagonal()
        matrix.setdiag(0)
        
        for i,j in zip(matrix.indptr, matrix.indptr[1:]):
            data = matrix.data[i:j]
            if len(data):
                idx = np.where(data.max() - data > value)[0]
                data[idx] = 0.0
                matrix.data[i:j] = data
        matrix.eliminate_zeros()
        
        matrix.setdiag(diag)
        return matrix

    @staticmethod
    def get_clusters(groups:np.array, lbl_idx:Optional[np.array]=None):
        lbl_idx = np.arange(groups.shape[0]) if lbl_idx is None else lbl_idx
        assert groups.shape == lbl_idx.shape, "`groups` and `lbl_idx` must have the same shape."
        clusters = dict()
        for k,v in zip(groups, lbl_idx): 
            clusters.setdefault(k, []).append(v)
        clusters = list(clusters.values())
        
        cluster_sz = [len(c) for c in clusters]
        sort_idx = np.argsort(cluster_sz)[::-1]
        return [clusters[i] for i in sort_idx]
    

# %% ../nbs/43_conflation.ipynb 8
class Filter:

    @staticmethod
    def from_max_size(clusters:List, size:int):
        return [c for c in clusters if len(c) < size]

    @staticmethod
    def from_min_size(clusters:List, size:int):
        return [c for c in clusters if len(c) > size]

    @staticmethod
    def remove_top_clusters(clusters:List, topk:int):
        sort_idx = np.argsort([len(c) for c in clusters])[-topk-1::-1]
        return [clusters[i] for i in sort_idx]
        

# %% ../nbs/43_conflation.ipynb 41
class Cluster:

    @staticmethod
    def from_similarity(lbl_lbl:sp.csr_matrix, score_thresh:Optional[float]=0.3, diff_thresh:Optional[float]=0.2, 
                    sim_topk:Optional[int]=1, directed:Optional[bool]=True, connection:Optional[str]="weak"):
        lbl_lbl = lbl_lbl if diff_thresh is None else Operations.diff_threshold(lbl_lbl, diff_thresh)
        lbl_lbl = Operations.normalize(lbl_lbl)
    
        lbl_lbl.setdiag(0)
        lbl_lbl = lbl_lbl if sim_topk is None else retain_topk(lbl_lbl, k=sim_topk)
    
        n, groups = sp.csgraph.connected_components(lbl_lbl, directed=directed, connection=connection)
        return Operations.get_clusters(groups)

    @staticmethod
    def from_predictions(data_lbl:sp.csr_matrix, score_thresh:Optional[float]=0.3, diff_thresh:Optional[float]=0.2, 
                         pred_topk:Optional[int]=3, directed:Optional[bool]=True, connection:Optional[str]="weak"):
        data_lbl = retain_topk(data_lbl, k=pred_topk)
        data_lbl = Operations.min_clamp(data_lbl, score_thresh)
        data_lbl = Operations.diff_threshold(data_lbl, diff_thresh)
        
        lbl_mat = Operations.compute_yty(data_lbl.transpose())
        n, groups = sp.csgraph.connected_components(lbl_mat, directed=directed, connection=connection)
        return Operations.get_clusters(groups)

    @staticmethod
    def from_derived_phrases(lbl_phrases:sp.csr_matrix, directed:Optional[bool]=True, connection:Optional[str]="weak"):
        lbl_mat = Operations.compute_yty(lbl_phrases)
        n, groups = sp.csgraph.connected_components(lbl_mat, directed=directed, connection=connection)
        return Operations.get_clusters(groups)
        

# %% ../nbs/43_conflation.ipynb 44
class Conflation:

    @staticmethod
    def get_groups(clusters:List, n_lbl:int):
        groups, factor = np.full(n_lbl, -1), np.full(n_lbl, 1.0)
        for i,idx in enumerate(clusters):
            groups[idx] = i
            factor[idx] = 1/len(idx)
        idxs = np.where(groups == -1)[0]
        groups[idxs] = np.arange(len(clusters), len(clusters) + idxs.shape[0])
        return groups, factor

    @staticmethod
    def get_conflated_matrix(matrix:sp.csr_matrix, groups:np.array, n_lbl:Optional[int]=None, 
                             factor:Optional[np.array]=None):
        indptr = matrix.indptr

        if factor is not None:
            assert factor.shape == groups.shape
        
        data = matrix.data if factor is None else [d*factor[i] for d,i in zip(matrix.data, matrix.indices)]
        indices = [groups[i] for i in matrix.indices]
        shape = None if n_lbl is None else (matrix.shape[0], n_lbl)
        
        conflated_matrix = sp.csr_matrix((data, indices, indptr), shape=shape)
        conflated_matrix.sort_indices()
        conflated_matrix.sum_duplicates()
        return conflated_matrix

    @staticmethod
    def get_conflated_info(clusters:List, info:Dict):
        conflated_info, flag, lbl_idx = dict(), np.zeros(len(info["identifier"])), []
        
        for idxs in clusters:
            conflated_info.setdefault("identifier", []).append(" || ".join([str(info["identifier"][i]) for i in idxs]))
            conflated_info.setdefault("text", []).append(" [SEP] ".join([info["text"][i] for i in idxs if isinstance(info["text"][i], str)]))
            lbl_idx.append(idxs)
            flag[idxs] = 1
            
        idxs = np.where(flag == 0)[0]
        for idx in idxs:
            conflated_info.setdefault("identifier", []).append(info["identifier"][idx])
            conflated_info.setdefault("text", []).append(info["text"][idx])
            lbl_idx.append([idx])
            flag[idx] = 1
            
        assert flag.all(), "All items should be covered."
        return conflated_info, lbl_idx

    @staticmethod
    def perform_conflation(clusters:List, trn_meta:sp.csr_matrix, meta_info:Dict, tst_meta:Optional[sp.csr_matrix]=None, 
                           lbl_meta:Optional[sp.csr_matrix]=None):
        
        groups, _ = Conflation.get_groups(clusters, n_lbl=trn_meta.shape[1])

        conflated_trn_meta = Conflation.get_conflated_matrix(trn_meta, groups)
        conflated_meta_info, meta_idx = Conflation.get_conflated_info(clusters, meta_info)
        
        conflated_tst_meta = None if tst_meta is None else Conflation.get_conflated_matrix(tst_meta, groups, n_lbl=conflated_trn_meta.shape[1])
        conflated_lbl_meta = None if lbl_meta is None else Conflation.get_conflated_matrix(lbl_meta, groups, n_lbl=conflated_trn_meta.shape[1])

        return conflated_trn_meta, conflated_meta_info, meta_idx, conflated_tst_meta, conflated_lbl_meta
    

# %% ../nbs/43_conflation.ipynb 70
class SaveData:

    @staticmethod
    def save_raw(fname:str, ids:List, txt:List):
        df = pd.DataFrame({"identifier": ids, "text": txt})
        df.to_csv(fname, index=False)

    @staticmethod
    def proc(save_dir:str, trn_file:str, trn_meta:sp.csr_matrix, info_file:str, meta_info:Dict, 
             tst_file:Optional[str]=None, tst_meta:Optional[sp.csr_matrix]=None, lbl_file:Optional[str]=None, 
             lbl_meta:Optional[sp.csr_matrix]=None, reuse_prev_dir:Optional[bool]=False):
        
        n = len([o for o in os.listdir(save_dir) if re.match(r'conflation_[0-9]{2}', o)])
        n = n if reuse_prev_dir else n + 1
        save_dir = f"{save_dir}/conflation_{n:02d}"
        
        raw_dir = f"{save_dir}/raw_data"
        os.makedirs(raw_dir, exist_ok=True)

        sp.save_npz(f"{save_dir}/{os.path.basename(trn_file)}", trn_meta)
        
        if tst_meta is not None:
            sp.save_npz(f"{save_dir}/{os.path.basename(tst_file)}", tst_meta)
            
        if lbl_meta is not None:
            sp.save_npz(f"{save_dir}/{os.path.basename(lbl_file)}", lbl_meta)

        raw_file = f"{raw_dir}/{os.path.basename(info_file)}"
        SaveData.save_raw(raw_file, meta_info["identifier"], meta_info["text"])
        

# %% ../nbs/43_conflation.ipynb 76
def load_data(data_dir:str, meta_type:str):
    trn_file = f"{data_dir}/{meta_type}_trn_X_Y.npz"
    tst_file = f"{data_dir}/{meta_type}_tst_X_Y.npz"
    lbl_file = f"{data_dir}/{meta_type}_lbl_X_Y.npz"
    
    info_file = f"{data_dir}/raw_data/{meta_type}.raw.csv"
    
    trn_meta = None if trn_file is None else sp.load_npz(trn_file)
    tst_meta = None if tst_file is None else sp.load_npz(tst_file)
    lbl_meta = None if lbl_file is None else sp.load_npz(lbl_file)
    
    meta_info = pd.read_csv(info_file)
    
    meta_phrases = sp.load_npz(f"{data_dir}/derived-phrases_{meta_type}_X_Y.npz")

    return (trn_meta, tst_meta, lbl_meta), meta_info, meta_phrases, (trn_file, tst_file, lbl_file, info_file)
    

# %% ../nbs/43_conflation.ipynb 77
def perform_similarity_based_conflation_01(meta_file:str, trn_meta:sp.csr_matrix, tst_meta:sp.csr_matrix, 
                                           lbl_meta:sp.csr_matrix, meta_info:Dict):
    meta_mat = retain_topk(sp.load_npz(meta_file), k=20)
    clusters = Cluster.from_similarity(meta_mat, diff_thresh=0.2, sim_topk=1)
    
    return Conflation.perform_conflation(clusters, trn_meta, meta_info, tst_meta=tst_meta, lbl_meta=lbl_meta)
    

# %% ../nbs/43_conflation.ipynb 78
def perform_phrase_based_conflation_02(meta_phrases:sp.csr_matrix, meta_file:str, trn_meta:sp.csr_matrix, 
                                       tst_meta:sp.csr_matrix, lbl_meta:sp.csr_matrix, meta_info:Dict):
    clusters = Cluster.from_derived_phrases(meta_phrases)
    clusters = Filter.remove_top_clusters(clusters, topk=1)

    output = Conflation.perform_conflation(clusters, trn_meta, meta_info, tst_meta=tst_meta, lbl_meta=lbl_meta)
    ctrn_meta, cmeta_info, meta_idx, ctst_meta, clbl_meta = output

    meta_mat = sp.load_npz(meta_file)
    groups, factor = Conflation.get_groups(meta_idx, n_lbl=trn_meta.shape[1])
    assert groups.max() == ctrn_meta.shape[1] - 1
    
    conflated_meta_mat = Conflation.get_conflated_matrix(meta_mat, groups, factor=factor)
    conflated_meta_mat = conflated_meta_mat.T.tocsr()
    conflated_meta_mat = Conflation.get_conflated_matrix(conflated_meta_mat, groups, factor=factor)

    clusters = Cluster.from_similarity(conflated_meta_mat, diff_thresh=0.2, sim_topk=1)
    return Conflation.perform_conflation(clusters, ctrn_meta, cmeta_info, tst_meta=ctst_meta, lbl_meta=clbl_meta)
    
