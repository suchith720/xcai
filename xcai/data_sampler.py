# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/19_data_sampler.ipynb.

# %% auto 0
__all__ = ['XCSamplerFeatTfm', 'NGAMESamplerFeatTfm', 'OAKSamplerFeatTfm', 'CrossSamplerFeatTfm']

# %% ../nbs/19_data_sampler.ipynb 3
import os,pickle,torch,re, numpy as np
from typing import Optional, List, Dict
from itertools import chain

from transformers import BatchEncoding, AutoTokenizer

from fastcore.utils import *

from .transform import PadFeatTfm,CollapseTfm
from .core import store_attr

# %% ../nbs/19_data_sampler.ipynb 18
class XCSamplerFeatTfm:

    def __init__(
        self,
        pad_token:Optional[int]=0,
        oversample:Optional[bool]=False,
        sampling_features:Optional[List]=None,
        **kwargs
    ):
        store_attr('sampling_features,oversample')
        self.pad_proc = PadFeatTfm(pad_tok=pad_token, in_place=False, drop=False)
        self.col_proc = CollapseTfm()

    # sample_base_feature

    def collate_feature_idx(self, x, name, sampling_name=None):
        level = name.count('2')
        o = self.pad_proc(x, prefix=f'{name}_idx', lev=level)
    
        if f'{name}_idx' in o:
            if sampling_name is not None and f'{sampling_name}_idx' not in o:
                o[f'{sampling_name}_idx'] = o.pop(f'{name}_idx')
            o = self.rename_idx_ptr(o, name, sampling_name)
            o = {f'p{k}':v for k,v in o.items()}
            
        return o

    def get_rnd_idx_from_ptr(self, x:List, n_samples:int, oversample:Optional[bool]=True):
        if oversample: return [torch.randint(i, size=(n_samples,)) if i>0 else torch.tensor([-1]) for i in x]
        else: return [torch.randperm(i)[:n_samples] if i>0 else torch.tensor([-1]) for i in x]

    def rename_idx_ptr(self, x:Dict, prefix:str, sampling_prefix:Optional[str]=None):
        prefixes = prefix.split('2')
        for i,n in enumerate(range(len(prefixes)-1,0,-1)):
            s = '2'.join(prefixes[n:])
            p = prefix if sampling_prefix is None else sampling_prefix
            x[f'{p}_{s}2ptr'] = x.pop(f'{prefix}_idx_ptr-{i+1}')
        return x

    def get_features(self, x:Dict, prefix:str):
        pat = f'^({prefix.replace(",","|")})_.*'
        return [o for o in x if re.match(pat, o)]

    def sample_batch(self, batch:List, features:List, idxs:List, level:int):
        sbatch = []
        for b,idx in zip(batch, idxs):
            sfeatures = {}
            for feature in features:
                cfeature = self.col_proc(b[feature], level)[0]
                sfeatures[feature] = [] if idx[0] == -1 else [cfeature[i] for i in idx]
            sbatch.append(sfeatures)
        return sbatch

    def remove_unwanted_ptr(self, x:Dict):
        return {k:v for k,v in x.items() if not re.match('.*_ptr-[0-9]+$', k)}
    
    def rename_keys(self, x:Dict, prefix:str):
        keys = list(x.keys())
        for k in keys:
            nk = k.split('_', maxsplit=1)[1]
            nk = f'{prefix}_{nk}'
            if nk not in x:
                x[nk] = x[k]
                del x[k]
        return x
    
    def collate_features(self, x:List, name:str, sampling_name:Optional[str]=None):
        level = name.count('2')

        o = self.pad_proc.coll_proc(x, prefix=name, lev=level)
        if f'{name}_input_ids' in o and f'{name}_attention_mask' not in o: 
            o[f'{name}_attention_mask'] = [[1]*len(i) for i in o[f'{name}_input_ids']]
        o = self.pad_proc.proc(o)
        
        o = self.rename_idx_ptr(o, name, sampling_name)
        o = self.remove_unwanted_ptr(o)
        if sampling_name is not None: o = self.rename_keys(o, sampling_name)
        return o
                
    def sample_base_feature(self, batch:List, prefix_names:str, name:str, n_sample:int, 
                            oversample:Optional[bool]=True):
        sampled_batch, sbatch = {}, {}
        
        feat_prefix = name.split('2')
        sampling_name,ptr_name = f'{feat_prefix[0]}2{feat_prefix[-1]}',feat_prefix[-1]
        
        o = self.collate_feature_idx(batch, name=name, sampling_name=sampling_name)
        
        if len(o):
            sampling_idx = self.get_rnd_idx_from_ptr(o[f'p{sampling_name}_{ptr_name}2ptr'], n_sample, oversample=oversample)
            
            sampled_batch.update(o)
            
            feats,level = self.get_features(batch[0], prefix_names), name.count('2')-1
            sbatch = self.sample_batch(batch, feats, sampling_idx, level)
        
            o = self.collate_features(sbatch, name=name, sampling_name=sampling_name)
            sampled_batch.update(o)
        
        return sampled_batch, sbatch
        
    # sample_dep_features

    def sample_sbatch(self, batch:List, features:List, n_samples:int, oversample:Optional[bool]=True):
        sbatch = []
        for b in batch:
            
            idxs = []
            for val in b[features[0]]:
                if oversample: idx = np.random.randint(len(val), size=n_samples) if len(val) > 0 else []
                else: idx = np.random.permutation(len(val))[:n_samples]
                idxs.append(idx)
            
            sfeatures = {}
            for feature in features:
                
                svalues = []
                for val,idx in zip(b[feature],idxs):
                    svalues.append([val[i] for i in idx])
                    
                sfeatures[feature] = svalues
                
            sbatch.append(sfeatures)
        return sbatch
    
    def sample_dep_features(
        self, 
        sampled_batch:List, 
        sbatch:List, 
        names:List, 
        n_samples:List, 
        oversample:Optional[bool]=True,
    ):
        for name,n_sample in zip(names,n_samples):
            sampling_name = '2'.join(name.split('2')[:2])
            o = self.collate_feature_idx(sbatch, name=name, sampling_name=sampling_name)
    
            if len(o):
                sampled_batch.update(o)
                
                feats = self.get_features(sbatch[0], name)
                o = self.sample_sbatch(sbatch, feats, n_sample, oversample=oversample)
                o = self.collate_features(o, name=name, sampling_name=sampling_name)
                sampled_batch.update(o)
    
        return sampled_batch

    # sample features

    def sample_feature(self, batch:List, names:str, n_samples:Union[int, List], oversample:Optional[bool]=False):
        feature_names = names.split(',')
        
        if isinstance(n_samples, int): 
            n_samples = (n_samples,)*len(feature_names)

        if len(feature_names) != len(n_samples):
            raise ValueError(f'`feature_names` and `n_samples` should have same length.')
        
        base_name, dep_names = feature_names[0], feature_names[1:]
        base_n_sample, dep_n_samples = n_samples[0], n_samples[1:]

        if f'{base_name}_input_ids' in batch[0]: self.add_attention_mask = True

        for p in dep_names:
            if not p.endswith(base_name): 
                raise ValueError(f'{p} does not end with the base prefix `{base_name}`.')

        sampled_batch, sbatch = self.sample_base_feature(batch, names, base_name, base_n_sample, oversample)
        return self.sample_dep_features(sampled_batch, sbatch, dep_names, dep_n_samples, oversample)

    # left-over features
    
    def process_features(self, sampled_batch:BatchEncoding, batch:BatchEncoding, names:List):
        for name in names:
            o = self.collate_features(batch, name=name)
            sampled_batch.update(o)
        return sampled_batch


# %% ../nbs/19_data_sampler.ipynb 19
@patch
def __call__(
    self:XCSamplerFeatTfm, 
    batch:List, 
    sampling_features:Optional[List]=None,
    oversample:Optional[bool]=None,
):  
    store_attr('sampling_features,oversample', is_none=False)

    sampled_features = set()
    out = BatchEncoding({})
    for name, n_sample in self.sampling_features:
        o = self.sample_feature(batch, name, n_sample, self.oversample)
        out.update(o)

        sampled_features.update(name.split(','))

    all_features = set([k.split('_')[0] for k in batch[0].keys()])
    remaining_features = all_features.difference(sampled_features)
    out = self.process_features(out, batch, remaining_features)
    
    return out
    

# %% ../nbs/19_data_sampler.ipynb 54
class NGAMESamplerFeatTfm:

    def __init__(self, sampler_num_labels:Optional[int]=1, sampler_label_oversample:Optional[bool]=False, 
                 pad_token:Optional[int]=0, **kwargs):
        self.n_labels, self.lbl_oversample = sampler_num_labels, sampler_label_oversample
        self.pad_proc = PadFeatTfm(pad_tok=pad_token, in_place=False, drop=False)

    def align_features(self, features:List, prefix:str, level:int):
        o = self.pad_proc.coll_proc(features, prefix=prefix, lev=level)
        if f'{prefix}_input_ids' in o and f'{prefix}_attention_mask' not in o: 
            o[f'{prefix}_attention_mask'] = [[1]*len(i) for i in o[f'{prefix}_input_ids']]
        return self.pad_proc.proc(o)

    def collate_data(self, batch:Dict, features:List):
        if 'data_idx' in features[0]:
            batch['data_idx'] = torch.tensor([o['data_idx'] for o in features], dtype=torch.int64)
            
        o =  self.align_features(features, 'data', 0)    
        batch['data_input_ids'], batch['data_attention_mask'] = o['data_input_ids'], o['data_attention_mask']

    def sample_features(self, batch:Dict, features:List, name:str, level:int, n_samples:Optional[int]=1,
                       oversample:Optional[bool]=False):
        if f'{name}_idx' in features[0]:
            batch[f'p{name}_data2ptr'] = torch.tensor([len(o[f'{name}_idx']) for o in features], dtype=torch.int64)
            batch[f'p{name}_idx'] = torch.tensor(list(chain(*[o[f'{name}_idx'] for o in features])), dtype=torch.int64)

            o =  self.align_features(features, name, level)
            input_ids, attention_mask = o[f'{name}_input_ids'], o[f'{name}_attention_mask']
            
            indptr = torch.cat([torch.zeros((1,), dtype=torch.int64), batch[f'p{name}_data2ptr'].cumsum(dim=0)])
            if oversample: idx = torch.hstack([torch.randint(n, size=(n_samples,))+offset for n,offset in zip(batch[f'p{name}_data2ptr'], indptr)])
            else: idx = torch.hstack([torch.randperm(n)[:n_samples]+offset for n,offset in zip(batch[f'p{name}_data2ptr'], indptr)])
        
            batch[f'{name}_data2ptr'] = torch.clamp(batch[f'p{name}_data2ptr'], max=n_samples)
            batch[f'{name}_idx'] = batch[f'p{name}_idx'][idx]
            batch[f'{name}_attention_mask'] = attention_mask[idx]
            batch[f'{name}_input_ids'] = input_ids[idx]

    def collate_labels(self, batch:Dict, features:List):
        self.sample_features(batch, features, 'lbl2data', level=1, n_samples=self.n_labels, 
                             oversample=self.lbl_oversample)

    def __call__(self, features:List):
        batch = BatchEncoding({})
        self.collate_data(batch, features)
        self.collate_labels(batch, features, n_labels=self.n_labels)
        return batch


# %% ../nbs/19_data_sampler.ipynb 62
class OAKSamplerFeatTfm(NGAMESamplerFeatTfm):

    def __init__(self, sampler_metadata_info:Union[Dict, List, str], **kwargs):
        super().__init__(**kwargs)
        if isinstance(sampler_metadata_info, str): sampler_metadata_info = [sampler_metadata_info]
        self.meta_info = sampler_metadata_info if isinstance(sampler_metadata_info, dict) else {o: 1 for o in sampler_metadata_info}
        
    def collate_metadata(self, batch:Dict, features:List):
        for meta_name, n_meta in self.meta_info.items():
            self.sample_features(batch, features, f'{meta_name}2data', level=1, n_samples=n_meta)

    def __call__(self, features:List):
        batch = BatchEncoding({})
        self.collate_data(batch, features)
        self.collate_labels(batch, features)
        self.collate_metadata(batch, features)
        return batch
        

# %% ../nbs/19_data_sampler.ipynb 70
class CrossSamplerFeatTfm(OAKSamplerFeatTfm):

    def __init__(self, label_outer_product:Optional[bool]=True, meta_outer_product:Optional[bool]=False, 
                 sampler_use_sep:Optional[bool]=True, **kwargs):
        super().__init__(**kwargs)
        self.lbl_outer_prod, self.meta_outer_prod, self.use_sep = label_outer_product, meta_outer_product, sampler_use_sep
        
    def prepare_cross_features(self, features:List, name:str, outer_product:Optional[bool]=False, 
                               use_sep:Optional[bool]=True):
        input_ids, attention_mask, labels = list(), list(), list()
        
        if 'data_input_ids' in features:
            if outer_product:
                for i,data in enumerate(features['data_input_ids']):
                    for j,meta in enumerate(features[f'{name}2data_input_ids']):
                        input_ids.extend([data+o if use_sep else data[:-1]+o for o in meta])
                        labels.extend([1]*len(meta) if i == j else [0]*len(meta))
            else:
                for data, meta in zip(features['data_input_ids'], features[f'{name}2data_input_ids']):
                    input_ids.extend([data+o if use_sep else data[:-1]+o for o in meta])
                    labels.extend([1]*len(meta))
                    
        if 'data_attention_mask' in features:
            if outer_product:
                for i,data in enumerate(features['data_attention_mask']):
                    for j,meta in enumerate(features[f'{name}2data_attention_mask']):
                        attention_mask.extend([data+o if use_sep else data[:-1]+o for o in meta])
            else:
                for data, meta in zip(features['data_input_ids'], features[f'{name}2data_attention_mask']):
                    attention_mask.extend([data+o if use_sep else data[:-1]+o for o in meta])
        else:
            attention_mask.append([[1]*len(o) for o in input_ids])

        return input_ids, attention_mask, labels

    def sample_features(self, batch:Dict, features:List, name:str, level:int, n_samples:Optional[int]=1,
                        oversample:Optional[bool]=False, outer_product:Optional[bool]=False, use_sep:Optional[bool]=True):
        if f'{name}_idx' in features[0]:
            batch[f'p{name}_data2ptr'] = torch.tensor([len(o[f'{name}_idx']) for o in features], dtype=torch.int64)
            batch[f'p{name}_idx'] = torch.tensor(list(chain(*[o[f'{name}_idx'] for o in features])), dtype=torch.int64)

            # this is wrong; do it again
            input_ids, attention_mask, labels = self.prepare_cross_features(features, name, outer_product, use_sep)
            
            indptr = torch.cat([torch.zeros((1,), dtype=torch.int64), batch[f'p{name}_data2ptr'].cumsum(dim=0)])
            if oversample: idx = torch.hstack([torch.randint(n, size=(n_samples,))+offset for n,offset in zip(batch[f'p{name}_data2ptr'], indptr)])
            else: idx = torch.hstack([torch.randperm(n)[:n_samples]+offset for n,offset in zip(batch[f'p{name}_data2ptr'], indptr)])
        
            batch[f'{name}_data2ptr'] = torch.clamp(batch[f'p{name}_data2ptr'], max=n_samples)
            batch[f'{name}_idx'] = batch[f'p{name}_idx'][idx]
            batch[f'{name}_attention_mask'] = attention_mask[idx]
            batch[f'{name}_input_ids'] = input_ids[idx]

    def collate_labels(self, batch:Dict, features:List):
        self.sample_features(batch, features, 'lbl2data', level=1, n_samples=self.n_labels, 
                             oversample=self.lbl_oversample, outer_product=self.lbl_outer_prod, 
                             use_sep=self.use_sep)

    def collate_metadata(self, batch:Dict, features:List):
        for meta_name, n_meta in self.meta_info.items():
            self.sample_features(batch, features, f'{meta_name}2data', level=1, n_samples=n_meta,
                                 outer_product=self.meta_outer_prod, use_sep=self.use_sep)
        
