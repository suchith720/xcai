# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['show_data', 'Info', 'Filterer', 'store_attr', 'get_attr', 'sorted_metric', 'display_metric', 'get_tensor_statistics',
           'total_recall', 'BalancedClusters', 'ClusterGroupedSampler']

# %% ../nbs/00_core.ipynb 2
import pandas as pd, numpy as np, logging, sys, re, os, torch
import torch.nn.functional as F
from torch.utils.data import Sampler
from itertools import chain
from scipy import sparse
from IPython.display import display
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from typing import List, Dict, Union, Optional, Any

from fastcore.dispatch import *
from fastcore.basics import *

# %% ../nbs/00_core.ipynb 5
def show_data(x:Dict, n:Optional[int]=10, seed:Optional[int]=None):
    with pd.option_context('display.max_colwidth', None):
        display(pd.DataFrame(x).sample(n, random_state=seed))

# %% ../nbs/00_core.ipynb 6
class Info():

    def __init__(self):
        self.tokz, self.info = None, None
        
    @staticmethod
    def _read_text(fname:str, enc:Optional[str]='latin-1'):
        with open(fname, encoding=enc) as f:
            info = [o[:-1] for o in f]
        return info
        
    @staticmethod
    def _read_info(fname:str, sep:Optional[str]='->', info_column_names:Optional[List]=None, enc:Optional[str]='latin-1'):
        info = Info._read_text(fname, enc=enc)
        info = list(zip(*[o.split(sep) for o in info]))
        info_column_names = list(range(len(info))) if info_column_names is None else info_column_names
        if len(info_column_names) != len(info): raise ValueError(f'`info_column_names` and `info` should have same number of elements.')
        return {p:q for p,q in zip(info_column_names, info)}

    def read_info(self, fname:Optional[str], sep:Optional[str]='->', info_column_names:Optional[List]=None, enc:Optional[str]='latin-1'):
        self.info = Info._read_info(fname, sep, info_column_names, enc)
        return self.info
    
    def tokenize(self, tokenization_column:Union[int, str], tokenizer:Union[str, PreTrainedTokenizerBase], max_sequence_length:Optional[int]=None):
        if self.tokz is None: self.tokz = tokenizer if isinstance(tokenizer, PreTrainedTokenizerBase) else AutoTokenizer.from_pretrained(tokenizer)
        tokenization_column = list(self.info.keys())[0] if tokenization_column is None else tokenization_column
        if tokenization_column is None: logging.info(f'`tokenization_column` not given as input, so value set to {tokenization_column}.')
        if tokenization_column not in self.info: raise ValueError(f'`{tokenization_column}` is invalid `tokenization_column` value.')
        self.info.update(self.tokz(self.info[tokenization_column], truncation=True, max_length=max_sequence_length))
        return self.info

    def show_data(self, n:Optional[int]=10, seed:Optional[int]=None):
        with pd.option_context('display.max_colwidth', None):
            display(pd.DataFrame(self.info).sample(n, random_state=seed))

    def __len__(self):
        if self.info is None: return 0
        n_info = [len(v) for v in self.info.values()]
        if len(n_info) == 0: raise ValueError('`info` cannot be empty.')
        if not np.all([o == n_info[0] for o in n_info]): raise ValueError('`info` should contain features with same length.')
        return n_info[0]

    @classmethod
    def from_txt(cls, 
                 fname:str, 
                 sep:Optional[str]='->', 
                 info_column_names:Optional[List]=None, 
                 enc:Optional[str]='latin-1',
                 use_tokenizer:Optional[bool]=False,
                 tokenizer:Optional[Union[str,PreTrainedTokenizerBase]]=None,
                 tokenization_column:Optional[str]=None,
                 max_sequence_length:Optional[int]=None, 
                 **kwargs):
        self = cls()
        self.info = self.read_info(fname, sep, info_column_names, enc)
        if use_tokenizer: self.tokenize(tokenization_column, tokenizer, max_sequence_length)
        return self.info
        

# %% ../nbs/00_core.ipynb 15
class Filterer:

    @staticmethod
    def load_filter(fname:str):
        if fname is not None and os.path.exists(fname): return np.loadtxt(fname, dtype=np.int64)
        
    @staticmethod
    def generate(train_id:List, test_id:List, lbl_id:List, train_lbl:sparse.csr_matrix, test_lbl:sparse.csr_matrix):
        _, train_idx, lbl2train_idx = np.intersect1d(train_id, lbl_id, return_indices=True)
        train_lbl_filterer = np.vstack([train_idx, lbl2train_idx]).T
        
        _, test_idx, lbl2test_idx = np.intersect1d(test_id, lbl_id, return_indices=True)
        test_lbl_filterer = np.vstack([test_idx, lbl2test_idx]).T
        
        train_udx, train_udx2idx = np.unique(train_idx, return_index=True)
        lbl2test_udx, lbl2test_udx2idx = np.unique(lbl2test_idx, return_index=True)
        
        _test_lbl_filterer = train_lbl[train_udx][:, lbl2test_udx].T
        
        rows, cols = _test_lbl_filterer.nonzero()
        test_idx = test_idx[lbl2test_udx2idx[rows]]
        lbl2test_idx = lbl2train_idx[train_udx2idx[cols]]
        
        _test_lbl_filterer = np.vstack([test_idx, lbl2test_idx]).T
        test_lbl_filterer = np.vstack([test_lbl_filterer, _test_lbl_filterer])
    
        return train_lbl_filterer, test_lbl_filterer

    @staticmethod
    def sample(f:np.array, sz:tuple, idx:List):
        f = sparse.coo_matrix((np.full(f.shape[0],1), (f[:, 0], f[:, 1])), shape=sz).tocsr()
        f = f[idx].tocoo()
        return np.vstack([f.row, f.col]).T

    @staticmethod
    def prune(data:sparse.csr_matrix, data_filterer:np.array):
        data = data.copy()
        data[data_filterer[:,0], data_filterer[:,1]] = 0
        data.eliminate_zeros()
        
        idx = np.where(data.getnnz(axis=1) > 0)[0]
        return data[idx], Filterer.sample(data_filterer, data.shape, idx), idx

    @staticmethod
    def apply(data:sparse.csr_matrix, data_filterer:np.array):
        data[data_filterer[:,0], data_filterer[:,1]] = 0
        data.eliminate_zeros()
        return data

        

# %% ../nbs/00_core.ipynb 17
def store_attr(names=None, self=None, but='', cast=False, store_args=None, is_none=True, **attrs):
    fr = sys._getframe(1)
    args = argnames(fr, True)
    if self: args = ('self', *args)
    else: self = fr.f_locals[args[0]]
    if store_args is None: store_args = not hasattr(self,'__slots__')
    if store_args and not hasattr(self, '__stored_args__'): self.__stored_args__ = {}
    anno = annotations(self) if cast else {}
    if names and isinstance(names,str): names = re.split(', *', names)
    ns = names if names is not None else getattr(self, '__slots__', args[1:])
    added = {n:fr.f_locals[n] for n in ns}
    attrs = {**attrs, **added}
    if isinstance(but,str): but = re.split(', *', but)
    attrs = {k:v for k,v in attrs.items() if k not in but}
    return _store_attr(self, anno, is_none, **attrs)
    

# %% ../nbs/00_core.ipynb 18
def _store_attr(self, anno, is_none, **attrs):
    stored = getattr(self, '__stored_args__', None)
    for n,v in attrs.items():
        if n in anno: v = anno[n](v)
        if is_none or v is not None: setattr(self, n, v)
        if stored is not None: stored[n] = v
       

# %% ../nbs/00_core.ipynb 19
def get_attr(x, attr:str):
    for a in attr.split('.'): x = getattr(x, a)
    return x

# %% ../nbs/00_core.ipynb 20
def sorted_metric(keys:List, order:Optional[Dict]=None):
    order = {o.split('@')[0]:i for i,o in enumerate(keys)}
    def _key_fn(x): return order[x[0]],int(x[1])
    def key_fn(x): return _key_fn(x.split('@'))
    return sorted(keys, key=key_fn)

def display_metric(metrics, remove_prefix:Optional[bool]=True, order:Optional[List]=None, scale:Optional[int]=100.0):
    metrics = {k.split('_', maxsplit=1)[1]:v for k,v in metrics.items()} if remove_prefix else metrics
    metric_keys, other_keys = sorted_metric([k for k in metrics if '@' in k], order), [k for k in metrics if '@' not in k]
    
    from IPython.display import display
    with pd.option_context('display.precision',4,'display.max_colwidth',None,'display.max_columns',None):
        metric,other = pd.DataFrame([metrics])[metric_keys]*scale, pd.DataFrame([metrics])[other_keys]
        display(pd.concat([metric, other], axis=1))

# %% ../nbs/00_core.ipynb 21
def get_tensor_statistics(x:torch.Tensor):
    c = ['mean', 'std', '25', '50', '75']
    s = torch.cat([x.float().mean(dim=0, keepdim=True), 
                   x.float().std(dim=0, keepdim=True),
                   torch.quantile(x.float(), torch.tensor([0.25, 0.5, 0.75]))])
    return pd.DataFrame([s.tolist()], columns=c)

def total_recall(inp_idx:torch.Tensor, n_inp:torch.Tensor, targ:sparse.csr_matrix, filterer:sparse.csr_matrix):
    val, ptr = torch.ones(len(inp_idx)), torch.cat([torch.zeros(1, dtype=torch.int64), n_inp.cumsum(0)])
    inp = sparse.csr_matrix((val,inp_idx,ptr), shape=targ.shape); inp.sum_duplicates(); inp.data[:] = 1
    if filterer is not None: inp, targ = Filterer.apply(inp, filterer), Filterer.apply(targ, filterer)
    sc = inp.multiply(targ)/(targ.getnnz(axis=1)[:, None]*targ.shape[0])
    return sc.sum(), sc

# %% ../nbs/00_core.ipynb 23
class BalancedClusters:

    @staticmethod
    def binary_kmeans(x:torch.Tensor, idx:Optional[torch.Tensor]=None, tol:Optional[float]=1e-4):
        n, x = x.shape[0], F.normalize(x)
        if n == 1: return (idx,)
            
        rnd_idx = torch.randperm(n)[:2]
        c = x[rnd_idx]
        sim = x@c.T
        
        old_sc, new_sc = None, None
        while old_sc is None or new_sc - old_sc >= tol:
            p,q = torch.chunk(torch.argsort(sim[:,1]-sim[:,0]), 2)
            c = torch.vstack([x[p].mean(dim=0, keepdim=True), x[q].mean(dim=0, keepdim=True)])
            sim = x@c.T
            sc = sim[p,0].sum() + sim[q,1].sum()
            new_sc, old_sc = sc/n, new_sc
        if idx is None: return p,q
        else: return (idx[p],idx[q])

    @staticmethod
    def proc(x:torch.Tensor, n_cluster:int, cluster:Optional[List[torch.Tensor]]=None):
        def _nearest_two_power(x): return 2**int(np.ceil(np.log2(x)))
        n_cluster = _nearest_two_power(n_cluster)
        cluster = (torch.arange(x.shape[0]),) if cluster is None else cluster
        nsz, osz = None, None
        while len(cluster) < n_cluster and (nsz != osz or osz is None):
            cluster = list(chain(*[BalancedClusters.binary_kmeans(x[o],o) for o in cluster]))
            nsz,osz = len(cluster),nsz
        x2cluster = torch.zeros(len(x), dtype=torch.int64)
        for i,c in enumerate(cluster): x2cluster[c] = i
        return cluster, x2cluster
        

# %% ../nbs/00_core.ipynb 28
class ClusterGroupedSampler(Sampler):

    def __init__(self, n:int, cluster:Optional[List]=None, generator:Optional[Any]=None):
        store_attr('n,cluster,generator')

    def __len__(self):
        return self.n

    def set_cluster(self, cluster): self.cluster = cluster

    def __iter__(self):
        if self.cluster is None: return iter(torch.randperm(self.n).tolist())
        csz = sum([len(o) for o in self.cluster])
        if len(self) != csz: raise ValueError(f'`n`({len(self)}) should be equal to total elements in `cluster`({csz})')
        cluster = [self.cluster[i] for i in torch.randperm(len(self.cluster))]
        indices = torch.hstack([o[torch.randperm(len(o))] for o in cluster]).tolist()
        return iter(indices)
        
