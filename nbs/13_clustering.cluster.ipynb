{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca75398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clustering.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ff6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn.functional as F, scipy.sparse as sp, numpy as np, functools, operator, torch, time, sys, gc, os\n",
    "from sklearn.preprocessing import normalize\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Sampler\n",
    "from typing import Optional, List, Union, Any\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.clustering.fast_cluster import balanced_cluster, next_power_of_two\n",
    "\n",
    "from fastcore.dispatch import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ee598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa1ac1",
   "metadata": {},
   "source": [
    "## `BalancedClusters`: CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def b_kmeans_dense_multi(fts_lbl, index, tol=1e-4):\n",
    "    lbl_cent = normalize(np.squeeze(fts_lbl[:, 0, :]))\n",
    "    lbl_fts = normalize(np.squeeze(fts_lbl[:, 1, :]))\n",
    "    if lbl_cent.shape[0] == 1:\n",
    "        return [index]\n",
    "    cluster = np.random.randint(low=0, high=lbl_cent.shape[0], size=(2))\n",
    "    while cluster[0] == cluster[1]:\n",
    "        cluster = np.random.randint(low=0, high=lbl_cent.shape[0], size=(2))\n",
    "    _centeroids = lbl_cent[cluster]\n",
    "    _sim = np.dot(lbl_cent, _centeroids.T)\n",
    "    old_sim, new_sim = -1000000, -2\n",
    "    while new_sim - old_sim >= tol:\n",
    "        c_lbs = np.array_split(np.argsort(_sim[:, 1]-_sim[:, 0]), 2)\n",
    "        _centeroids = normalize(np.vstack([\n",
    "            np.mean(lbl_cent[x, :], axis=0) for x in c_lbs\n",
    "        ]))\n",
    "        _sim_1 = np.dot(lbl_cent, _centeroids.T)\n",
    "        _centeroids = normalize(np.vstack([\n",
    "            np.mean(lbl_fts[x, :], axis=0) for x in c_lbs\n",
    "        ]))\n",
    "        _sim_2 = np.dot(lbl_fts, _centeroids.T)\n",
    "        _sim = _sim_1 + _sim_2\n",
    "        old_sim, new_sim = new_sim, np.sum([np.sum(_sim[c_lbs[0], 0]),\n",
    "                                            np.sum(_sim[c_lbs[1], 1])])\n",
    "    return list(map(lambda x: index[x], c_lbs))\n",
    "\n",
    "\n",
    "def b_kmeans_dense(labels_features, index, tol=1e-4, *args, **kwargs):\n",
    "    if labels_features.shape[0] == 1:\n",
    "        return [index]\n",
    "    cluster = np.random.randint(low=0, high=labels_features.shape[0], size=(2))\n",
    "    while cluster[0] == cluster[1]:\n",
    "        cluster = np.random.randint(\n",
    "            low=0, high=labels_features.shape[0], size=(2))\n",
    "    _centeroids = labels_features[cluster]\n",
    "    _similarity = np.dot(labels_features, _centeroids.T)\n",
    "    old_sim, new_sim = -1000000, -2\n",
    "    while new_sim - old_sim >= tol:\n",
    "        sim_diff = _similarity[:, 1] - _similarity[:, 0]\n",
    "        sim_diff_idx = np.argsort(sim_diff)\n",
    "        clustered_lbs = np.array_split(sim_diff_idx, 2)\n",
    "        c_l = np.mean(labels_features[clustered_lbs[0], :], axis=0)\n",
    "        c_r = np.mean(labels_features[clustered_lbs[1], :], axis=0)\n",
    "        _centeroids = normalize(np.vstack([c_l, c_r]))\n",
    "        _similarity = np.dot(labels_features, _centeroids.T)\n",
    "        s_l = np.sum(_similarity[clustered_lbs[0], 0])\n",
    "        s_r = np.sum(_similarity[clustered_lbs[1], 1])\n",
    "        old_sim, new_sim = new_sim, s_l + s_r\n",
    "    return list(map(lambda x: index[x], clustered_lbs))\n",
    "\n",
    "\n",
    "def b_kmeans_sparse(labels_features, index, tol=1e-4, *args, **kwargs):\n",
    "    def _sdist(XA, XB):\n",
    "        return XA.dot(XB.transpose())\n",
    "    labels_features = normalize(labels_features)\n",
    "    if labels_features.shape[0] == 1:\n",
    "        return [index]\n",
    "    cluster = np.random.randint(low=0, high=labels_features.shape[0], size=(2))\n",
    "    while cluster[0] == cluster[1]:\n",
    "        cluster = np.random.randint(\n",
    "            low=0, high=labels_features.shape[0], size=(2))\n",
    "    _centeroids = normalize(labels_features[cluster].todense())\n",
    "    _sim = _sdist(labels_features, _centeroids)\n",
    "    old_sim, new_sim = -1000000, -2\n",
    "    while new_sim - old_sim >= tol:\n",
    "        c_lbs = np.array_split(np.argsort(_sim[:, 1]-_sim[:, 0]), 2)\n",
    "        _centeroids = normalize(np.vstack([\n",
    "            labels_features[x, :].mean(axis=0) for x in c_lbs]))\n",
    "        _sim = _sdist(labels_features, _centeroids)\n",
    "        old_sim, new_sim = new_sim, np.sum([\n",
    "            np.sum(_sim[c_lbs[0], 0]), np.sum(_sim[c_lbs[1], 1])])\n",
    "    return list(map(lambda x: index[x], c_lbs))\n",
    "\n",
    "\n",
    "def b_kmeans_dense_gpu(labels_features, index, tol=1e-4, use_cuda=False):\n",
    "    if use_cuda:\n",
    "        labels_features = labels_features.cuda()\n",
    "    if labels_features.shape[0] == 1:\n",
    "        return [index]\n",
    "    cluster = np.random.randint(low=0, high=labels_features.shape[0], size=(2))\n",
    "    while cluster[0] == cluster[1]:\n",
    "        cluster = np.random.randint(\n",
    "            low=0, high=labels_features.shape[0], size=(2))\n",
    "    _centeroids = labels_features[cluster]\n",
    "    _similarity = torch.mm(labels_features, _centeroids.T)\n",
    "    old_sim, new_sim = -1000000, -2\n",
    "    while new_sim - old_sim >= tol:\n",
    "        sim_diff = _similarity[:, 1]-_similarity[:, 0]\n",
    "        sim_diff_idx = np.argsort(sim_diff.cpu().numpy())\n",
    "        clustered_lbs = np.array_split(sim_diff_idx, 2)\n",
    "        c_l = torch.mean(labels_features[clustered_lbs[0], :], dim=0)\n",
    "        c_r = torch.mean(labels_features[clustered_lbs[1], :], dim=0)\n",
    "        _centeroids = F.normalize(torch.stack([c_l, c_r], dim=0))\n",
    "        _similarity = torch.mm(labels_features, _centeroids.T)\n",
    "        s_l = torch.sum(_similarity[clustered_lbs[0], 0]).item()\n",
    "        s_r = torch.sum(_similarity[clustered_lbs[1], 1]).item()\n",
    "        old_sim, new_sim = new_sim, s_l+s_r\n",
    "    labels_features = labels_features.cpu()\n",
    "    del labels_features\n",
    "    gc.collect()\n",
    "    return list(map(lambda x: index[x], clustered_lbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_functions(mat):\n",
    "    if torch.is_tensor(mat):\n",
    "        print(\"Using GPU for clustering\")\n",
    "        return b_kmeans_dense_gpu\n",
    "    if isinstance(mat, np.ndarray):\n",
    "        if len(mat.shape) == 3:\n",
    "            print(\"Using dense kmeans++ for multi-view\")\n",
    "            return b_kmeans_dense_multi\n",
    "        elif len(mat.shape) == 2:\n",
    "            print(\"Using dense kmeans++\")\n",
    "            return b_kmeans_dense\n",
    "    elif sp.issparse(mat):\n",
    "        print(\"Using sparse kmeans++\")\n",
    "        return b_kmeans_sparse\n",
    "    print(\"dtype not understood!!\")\n",
    "    exit(0)\n",
    "\n",
    "\n",
    "def _normalize(mat):\n",
    "    if torch.is_tensor(mat):\n",
    "        return mat\n",
    "    elif isinstance(mat, np.ndarray) or sp.issparse(mat):\n",
    "        return normalize(mat)\n",
    "    else:\n",
    "        raise TypeError(f\"{type(mat)} is not supported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cluster(labels, max_leaf_size=None, min_splits=16, num_workers=4,\n",
    "            return_smat=False, num_clusters=None, force_gpu=False):\n",
    "    num_nodes = num_clusters\n",
    "    if num_nodes is None:\n",
    "        num_nodes = np.ceil(np.log2(labels.shape[0]/max_leaf_size))\n",
    "        num_nodes = int(2**num_nodes)\n",
    "    group = [np.arange(labels.shape[0])]\n",
    "    labels = _normalize(labels)\n",
    "    if force_gpu:\n",
    "        labels = torch.from_numpy(labels).type(torch.FloatTensor)\n",
    "    else:\n",
    "        labels = np.array(labels.cpu(), dtype=np.float32)\n",
    "    splitter = get_functions(labels)\n",
    "    min_singe_thread_split = min(min_splits, num_nodes)\n",
    "    if min_singe_thread_split < 1:\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.cuda()\n",
    "    print(f\"Max leaf size {max_leaf_size}\")\n",
    "    print(f\"Total number of group are {num_nodes}\")\n",
    "    print(f\"Average leaf size is {labels.shape[0]/num_nodes}\")\n",
    "    start = time.time()\n",
    "\n",
    "    def splits(flag, labels, group):\n",
    "        if flag or torch.is_tensor(labels):\n",
    "            return map(lambda x: splitter(labels[x], x, use_cuda=not flag), group)\n",
    "        else:\n",
    "            with Pool(num_workers) as p:\n",
    "                mapps = p.starmap(splitter, map(\n",
    "                    lambda x: (labels[x], x, flag), group))\n",
    "            return mapps\n",
    "\n",
    "    def print_stats(group, end=\"\\n\", file=sys.stdout):\n",
    "        string = f\"Total groups {len(group)}\"\n",
    "        string += f\", Avg. group size {np.mean(list(map(len, group)))}\"\n",
    "        string += f\", Total time {time.time()-start} sec.\"\n",
    "        print(string, end=end, file=file)\n",
    "\n",
    "    while len(group) < num_nodes:\n",
    "        print_stats(group, \"\\r\", sys.stderr)\n",
    "        flags = len(group) < min_singe_thread_split\n",
    "        group = functools.reduce(operator.iconcat,\n",
    "                                 splits(flags, labels, group), [])\n",
    "    print_stats(group)\n",
    "    if return_smat:\n",
    "        cols = np.uint32(np.concatenate(\n",
    "            [[x]*len(y) for x, y in enumerate(group)]))\n",
    "        rows = np.uint32(np.concatenate(group))\n",
    "        group = sp.lil_matrix((labels.shape[0], np.int32(num_nodes)))\n",
    "        group[rows, cols] = 1\n",
    "        group = group.tocsr()\n",
    "    del labels\n",
    "    return group\n",
    "\n",
    "\n",
    "def partial_cluster(\n",
    "    embs_bank: torch.Tensor,\n",
    "    min_leaf_sz: int,\n",
    "    num_random_clusters: int,\n",
    "    clustering_devices: Optional[List]=None,\n",
    "    ):\n",
    "    if not isinstance(embs_bank, torch.Tensor): raise ValueError('`embs_bank` should be `torch.Tensor`')\n",
    "    embs = embs_bank.clone()\n",
    "    tree_depth = int(np.ceil(np.log(embs.shape[0] / min_leaf_sz) / np.log(2)))\n",
    "    print(f\"Updating clusters with size {min_leaf_sz}\")\n",
    "    print(f\"Tree depth = {tree_depth}\")\n",
    "\n",
    "    if clustering_devices is None:\n",
    "        clustering_devices = (\n",
    "            np.arange(len(os.getenv(\"CUDA_VISIBLE_DEVICES\").split(','))) \n",
    "            if os.getenv(\"CUDA_VISIBLE_DEVICES\") is not None else \n",
    "            np.arange(torch.cuda.device_count())\n",
    "        )\n",
    "        \n",
    "    num_random_clusters = (\n",
    "        num_random_clusters\n",
    "        if num_random_clusters != -1\n",
    "        else next_power_of_two(len(clustering_devices))\n",
    "    )\n",
    "    if num_random_clusters < len(clustering_devices):\n",
    "                print(\"num_random_clusters provided is less \\\n",
    "                    than number of clustring devices which is not optimal\")\n",
    "                \n",
    "    clusters = balanced_cluster(torch.HalfTensor(embs.half()),\n",
    "                                tree_depth,\n",
    "                                clustering_devices,\n",
    "                                num_random_clusters,\n",
    "                                True)\n",
    "    \n",
    "    del embs\n",
    "    gc.collect()\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89538d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BalancedClusters:\n",
    "\n",
    "    @staticmethod\n",
    "    def proc(x:torch.Tensor, min_cluster_sz:int, clustering_devices:Optional[List]=None, verbose:Optional[bool]=True):\n",
    "        return partial_cluster(x, min_cluster_sz, -1, clustering_devices)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989625b9",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3142279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating clusters with size 3\n",
      "Tree depth = 2\n",
      "doing random split\n",
      "lengths: [5, 5]\n",
      "remaining levels for GPU split=1\n",
      "==> gpu splitting random clusters 0 to 2\n",
      " rank=1 => Total clusters 2\tAvg. Cluster size                 2.50\tTime to split nodes on this level 1.11 sec\n",
      " rank=0 => Total clusters 2\tAvg. Cluster size                 2.50\tTime to split nodes on this level 1.10 sec\n",
      "\n",
      "CPU times: user 197 ms, sys: 128 ms, total: 325 ms\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%time clusters = BalancedClusters.proc(x, 3, use_fast_clustering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc58f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 9, 2]), array([1, 7]), array([3, 5, 8]), array([4, 6])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time clusters = BalancedClusters.proc(x, 5, use_fast_clustering=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ea10c",
   "metadata": {},
   "source": [
    "## `ClusterGroupedSampler`: CLUSTER BASED SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ClusterGroupedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, n:int, cluster:Optional[List]=None, generator:Optional[Any]=None):\n",
    "        store_attr('n,cluster,generator')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def set_cluster(self, cluster): self.cluster = cluster\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.cluster is None: return iter(torch.randperm(self.n).tolist())\n",
    "        csz = sum([len(o) for o in self.cluster])\n",
    "        if len(self) != csz: raise ValueError(f'`n`({len(self)}) should be equal to total elements in `cluster`({csz})')\n",
    "        \n",
    "        cluster = [self.cluster[i] for i in np.random.permutation(len(self.cluster))]\n",
    "        if isinstance(cluster[0], torch.Tensor):\n",
    "            indices = torch.hstack([o[torch.randperm(len(o))] for o in cluster]).tolist()\n",
    "        else: indices = np.hstack([o[np.random.permutation(len(o))] for o in cluster]).tolist()\n",
    "        \n",
    "        return iter(indices)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1435a12",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d11ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b49fcc-7832-4f7f-89cb-b4d51d961fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating clusters with size 4\n",
      "Tree depth = 2\n",
      "doing cpu split\n",
      "remaining levels for GPU split=2\n",
      "==> gpu splitting random clusters 0 to 1\n",
      " rank=0 => Total clusters 2\tAvg. Cluster size                 8.00\tTime to split nodes on this level 0.78 sec\n",
      " rank=0 => Total clusters 4\tAvg. Cluster size                 4.00\tTime to split nodes on this level 0.01 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = BalancedClusters.proc(x, 4, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf6d9e-eb09-4a10-a119-7015cbaf2a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3,  1, 15, 14]),\n",
       " array([ 2,  5, 11, 10]),\n",
       " array([ 9,  8, 12,  4]),\n",
       " array([ 7, 13,  6,  0])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d099aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ClusterGroupedSampler(16)\n",
    "dl = DataLoader(torch.arange(len(x)), batch_size=5, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.sampler.set_cluster(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([13,  6,  0,  7,  5]),\n",
       " tensor([ 2, 10, 11,  3,  1]),\n",
       " tensor([14, 15, 12,  8,  4]),\n",
       " tensor([9])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9f4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74588031-0a25-437e-b993-a954a7dae4a1",
   "metadata": {},
   "source": [
    "## `Cluster mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c339241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_cluster_mapping(embeddings:torch.Tensor, cluster_sz:int=3):\n",
    "    clusters = BalancedClusters.proc(embeddings.half(), min_cluster_sz=cluster_sz)\n",
    "\n",
    "    cluster_mapping = torch.zeros(embeddings.shape[0], dtype=torch.int64)\n",
    "    for i,o in enumerate(clusters): cluster_mapping[o] = i\n",
    "    return cluster_mapping, len(clusters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb400f-d43e-4a6c-a9ac-a58b5590417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_cluster_size(emb_sz, cluster_sz):\n",
    "    return 2**int(np.ceil(np.log2(emb_sz / cluster_sz)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efd1b8-2c17-4c64-85f8-00666e3bfe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
