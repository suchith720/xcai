{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa699db4-a4b7-430e-8aef-1944f185fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d402020-9db0-4fc9-97dc-301109edd3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048a095b-e95c-4870-b0f7-9e772c387879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f01b50-cb84-4bfc-aa47-c0655db43642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, torch, scipy.sparse as sp, joblib, argparse, pickle, numpy as np, inspect\n",
    "from typing import Optional, Dict, Callable, Union, List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DistilBertConfig\n",
    "\n",
    "from xcai.sdata import SMainXCDataset, SXCDataBlock, SXCDataset\n",
    "from xcai.data import XCDataBlock, XCDataset, XCCollator\n",
    "from xcai.block import NXCBlock, SXCBlock, XCBlock, CFGS\n",
    "from xcai.core import Info, get_best_model, load_config\n",
    "from xcai.transform import AugmentMetaInputIdsTfm\n",
    "from xcai.learner import XCLearningArguments, XCLearner\n",
    "\n",
    "from xcai.models.PPP0XX import DBT024\n",
    "from xcai.models.distillation import TCH001\n",
    "from xcai.models.classifiers import CLS001\n",
    "from xcai.clustering.cluster import BalancedClusters, get_cluster_size\n",
    "\n",
    "from sugar.core import *\n",
    "\n",
    "from xclib.utils.sparse import retain_topk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726e730-56ec-4dc4-9b68-18a7f824c81f",
   "metadata": {},
   "source": [
    "## `Arguement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2725be79-e4fc-418d-9798-61009ab95eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--build_block', action='store_true')\n",
    "    parser.add_argument('--use_pretrained', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--do_train_inference', action='store_true')\n",
    "    parser.add_argument('--do_test_inference', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--save_train_prediction', action='store_true')\n",
    "    parser.add_argument('--save_test_prediction', action='store_true')\n",
    "    parser.add_argument('--save_label_prediction', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--save_representation', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--use_sxc_sampler', action='store_true')\n",
    "    parser.add_argument('--only_test', action='store_true')\n",
    "    parser.add_argument('--text_mode', action='store_true')\n",
    "    parser.add_argument('--use_oracle', action='store_true')\n",
    "\n",
    "    parser.add_argument('--score_data_lbl', action='store_true')\n",
    "    parser.add_argument('--score_data_meta', action='store_true')\n",
    "    parser.add_argument('--score_lbl_meta', action='store_true')\n",
    "\n",
    "    parser.add_argument('--pickle_dir', type=str, default=None)\n",
    "    \n",
    "    parser.add_argument('--prediction_suffix', type=str, default='')\n",
    "\n",
    "    parser.add_argument('--exact', action='store_true')\n",
    "    parser.add_argument('--dataset', type=str)\n",
    "\n",
    "    parser.add_argument('--num_meta_cluster', type=int, default=1)\n",
    "    parser.add_argument('--normalize', action='store_true')\n",
    "    parser.add_argument('--use_ln', action='store_true')\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff777e2e-f4af-4e42-9206-30356a141159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_inference_mode(args):\n",
    "    return args.do_train_inference or args.do_test_inference or args.save_train_prediction or args.save_test_prediction or args.save_representation or args.score_data_lbl or args.score_data_meta or args.score_lbl_meta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c701d7-0d93-4a3e-9a2f-d1e9e9204465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3f925c-a93e-4c25-8692-e08f3983ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_metadata_representation(mname:str, meta_info:Dict, collator:XCCollator, normalize:Optional[bool]=True, \n",
    "                                use_layer_norm:Optional[bool]=True, use_encoder_parallel:Optional[bool]=True):\n",
    "    dset = SXCDataset(SMainXCDataset(data_info=meta_info))\n",
    "    model = DBT024.from_pretrained(mname, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, \n",
    "                                   normalize=normalize, use_layer_norm=use_layer_norm, \n",
    "                                   use_encoder_parallel=use_encoder_parallel)\n",
    "    model.init_dr_head()\n",
    "    \n",
    "    args = XCLearningArguments(\n",
    "        output_dir='.',\n",
    "        per_device_train_batch_size=1024,\n",
    "        per_device_eval_batch_size=800,\n",
    "        representation_num_beams=200,\n",
    "        representation_accumulation_steps=10,\n",
    "        representation_search_type='BRUTEFORCE',\n",
    "        \n",
    "        target_indices_key='plbl2data_idx',\n",
    "        target_pointer_key='plbl2data_data2ptr',\n",
    "        \n",
    "        use_encoder_parallel=True,\n",
    "        max_grad_norm=None,\n",
    "        fp16=True,\n",
    "        \n",
    "        use_cpu_for_searching=True,\n",
    "        use_cpu_for_clustering=True,\n",
    "    )\n",
    "    \n",
    "    learn = XCLearner(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        eval_dataset=dset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    return learn._get_data_representation(dset, to_cpu=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98cc06b-7bda-4692-8dfb-7cbbf1c0c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_cluster_mapping(meta_repr:Optional[Union[torch.Tensor,str]]=None, cluster_sz:Optional[int]=3, mname:Optional[str]=None, \n",
    "                        meta_info:Optional[Dict]=None, collator:Optional[XCCollator]=None, normalize:Optional[bool]=True, \n",
    "                        use_layer_norm:Optional[bool]=True, use_encoder_parallel:Optional[bool]=True):\n",
    "    if meta_repr is None: \n",
    "        meta_repr = get_metadata_representation(mname, meta_info, collator, normalize=normalize, use_layer_norm=use_layer_norm)\n",
    "    elif isinstance(meta_repr, str):\n",
    "        meta_repr = torch.load(meta_repr)\n",
    "            \n",
    "    if cluster_sz > 1:\n",
    "        clusters = BalancedClusters.proc(meta_repr.half(), min_cluster_sz=cluster_sz)\n",
    "    \n",
    "        metadata_idx2cluster = torch.zeros(meta_repr.shape[0], dtype=torch.int64)\n",
    "        for i,o in enumerate(clusters): metadata_idx2cluster[o] = i\n",
    "    \n",
    "        meta_repr = torch.vstack([meta_repr[o].mean(dim=0) for o in clusters])\n",
    "        num_meta_clusters = len(clusters)\n",
    "    else:\n",
    "        metadata_idx2cluster = torch.arange(meta_repr.shape[0])\n",
    "        num_meta_clusters = meta_repr.shape[0]\n",
    "\n",
    "    return metadata_idx2cluster, meta_repr, num_meta_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc31579-0232-488f-a83e-e6a7975bfbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6800402f-79ea-40ec-9d84-a903e11325b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_cluster_mapping(do_inference:bool, use_pretrained:bool, num_metadata:int, cluster_size:int, meta_embed_init_file:Optional[str]=None, \n",
    "                        model_name:Optional[str]=None, meta_info:Optional[Dict]=None, collator:Optional[Callable]=None, \n",
    "                        normalize:Optional[bool]=True, use_layer_norm:Optional[bool]=None, output_dir:Optional[str]=None, **kwargs):\n",
    "    if do_inference and not use_pretrained:\n",
    "        metadata_idx2cluster, meta_repr = None, None\n",
    "        num_meta_cluster = get_cluster_size(num_metadata, cluster_size) if cluster_size > 1 else num_metadata\n",
    "    else:\n",
    "        map_file = None if output_dir is None else f'{output_dir}/cluster_info.pth'\n",
    "        if map_file is not None and os.path.exists(map_file):\n",
    "            metadata_idx2cluster, meta_repr, num_meta_cluster = torch.load(map_file)\n",
    "        else:\n",
    "            metadata_idx2cluster, meta_repr, num_meta_cluster = _get_cluster_mapping(meta_embed_init_file, cluster_sz=cluster_size, \n",
    "                                                                                     mname=model_name, meta_info=meta_info, \n",
    "                                                                                     collator=collator, normalize=normalize, \n",
    "                                                                                     use_layer_norm=use_layer_norm)\n",
    "            if output_dir is not None:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                torch.save((metadata_idx2cluster, meta_repr, num_meta_cluster), map_file)\n",
    "            \n",
    "    return metadata_idx2cluster, meta_repr, num_meta_cluster\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924b980-4c91-4070-bb48-63d85e510b87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build `block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf014afd-4c32-41cd-8f74-6331981cd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Filter:\n",
    "\n",
    "    @staticmethod\n",
    "    def topk(data_lbl:sp.csr_matrix, k:Optional[int]=3):\n",
    "        return retain_topk(data_lbl, k=k)\n",
    "\n",
    "    @staticmethod\n",
    "    def threshold(data_lbl:sp.csr_matrix, t:int):\n",
    "        idx = np.where(data_lbl.data < t)[0]\n",
    "        data_lbl.data[idx] = 0\n",
    "        data_lbl.eliminate_zeros()\n",
    "        return data_lbl\n",
    "\n",
    "    @staticmethod\n",
    "    def difference(data_lbl:sp.csr_matrix, t:int):\n",
    "        rowise_max = data_lbl.max(axis=1).toarray().ravel()\n",
    "        scores = np.repeat(rowise_max, np.diff(data_lbl.indptr)) - data_lbl.data\n",
    "        data_lbl.data[scores > t] = 0\n",
    "        data_lbl.eliminate_zeros()\n",
    "        return data_lbl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a8224ab-105b-4cee-9ca5-17ff55bfea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_metadata_dset(dset, topk:Union[int,Dict]=5, abs_thresh:Union[float,Dict]=0.2, \n",
    "                         diff_thresh:Union[float,Dict]=0.3):\n",
    "    for meta_name in dset.meta.keys():\n",
    "        data_meta, lbl_meta = dset.meta[meta_name].data_meta, dset.meta[meta_name].lbl_meta\n",
    "        update_meta = False\n",
    "        \n",
    "        k = topk.get(meta_name, None) if isinstance(topk, dict) else topk\n",
    "        if k is not None: \n",
    "            data_meta = retain_topk(data_meta, k=k)\n",
    "            if lbl_meta is not None: lbl_meta = retain_topk(lbl_meta, k=k)\n",
    "            update_meta = True\n",
    "\n",
    "        t = abs_thresh.get(meta_name, None) if isinstance(abs_thresh, dict) else abs_thresh\n",
    "        if t is not None:\n",
    "            data_meta = Filter.threshold(data_meta, t=t)\n",
    "            if lbl_meta is not None: lbl_meta = Filter.threshold(lbl_meta, t=t)\n",
    "            update_meta = True\n",
    "\n",
    "        t = diff_thresh.get(meta_name, None) if isinstance(diff_thresh, dict) else diff_thresh\n",
    "        if t is not None:\n",
    "            data_meta = Filter.difference(data_meta, t=t)\n",
    "            if lbl_meta is not None: lbl_meta = Filter.difference(lbl_meta, t=t)\n",
    "            update_meta = True\n",
    "            \n",
    "        if update_meta:\n",
    "            dset.meta[meta_name].update_meta_matrix(data_meta, lbl_meta)\n",
    "            if dset.meta[meta_name].use_meta_distribution or dset.meta[meta_name].return_scores:\n",
    "                dset.meta[meta_name]._store_scores() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696c0633-5daf-4348-ad2c-373adc0ea88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_metadata_block(block, train_topk:Union[int,Dict]=5, train_abs_thresh:Union[float,Dict]=0.2, \n",
    "                          train_diff_thresh:Union[float,Dict]=0.3, test_topk:Union[int,Dict]=5, \n",
    "                          test_abs_thresh:Union[float,Dict]=0.2, test_diff_thresh:Union[float,Dict]=0.3):\n",
    "    if block.train is not None: \n",
    "        filter_metadata_dset(block.train.dset, train_topk, train_abs_thresh, train_diff_thresh)\n",
    "    if block.test is not None:\n",
    "        filter_metadata_dset(block.test.dset, test_topk, test_abs_thresh, test_diff_thresh)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b53320-2edd-4fdb-af67-5c0fff8f7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def retain_topk_metadata(block, train_k:Union[int,Dict]=5, test_k:Union[int,Dict]=3):\n",
    "    if train_k is not None and block.train is not None:\n",
    "        for meta_name in block.train.dset.meta.keys():\n",
    "            k = train_k.get(meta_name, None) if isinstance(train_k, dict) else train_k\n",
    "            if k is not None:\n",
    "                data_meta = retain_topk(block.train.dset.meta[meta_name].data_meta, k=k)\n",
    "                lbl_meta = block.train.dset.meta[meta_name].lbl_meta\n",
    "                block.train.dset.meta[meta_name].update_meta_matrix(data_meta, lbl_meta)\n",
    "                if block.train.dset.meta[meta_name].use_meta_distribution or block.train.dset.meta[meta_name].return_scores: \n",
    "                    block.train.dset.meta[meta_name]._store_scores()\n",
    "\n",
    "    if test_k is not None and block.test is not None:\n",
    "        for meta_name in block.test.dset.meta.keys():\n",
    "            k = test_k.get(meta_name, None) if isinstance(test_k, dict) else test_k\n",
    "            if k is not None:\n",
    "                data_meta = retain_topk(block.test.dset.meta[meta_name].data_meta, k=k)\n",
    "                lbl_meta = block.test.dset.meta[meta_name].lbl_meta\n",
    "                block.test.dset.meta[meta_name].update_meta_matrix(data_meta, lbl_meta)\n",
    "                if block.test.dset.meta[meta_name].use_meta_distribution or block.test.dset.meta[meta_name].return_scores: \n",
    "                    block.test.dset.meta[meta_name]._store_scores()\n",
    "\n",
    "def retain_topk_labels(block, train_k:int=5, test_k:int=3):\n",
    "    if train_k is not None and block.train is not None:\n",
    "        block.train.dset.data.data_lbl = retain_topk(block.train.dset.data.data_lbl, k=train_k)\n",
    "        block.train.dset.data._store_indices()\n",
    "        if block.train.dset.data.use_main_distribution or block.train.dset.data.return_scores: \n",
    "            block.train.dset.data._store_scores()\n",
    "            \n",
    "    if test_k is not None and block.test is not None:\n",
    "        block.test.dset.data.data_lbl = retain_topk(block.test.dset.data.data_lbl, k=test_k)\n",
    "        block.test.dset.data._store_indices()\n",
    "        if block.test.dset.data.use_main_distribution or block.test.dset.data.return_scores:\n",
    "            block.test.dset.data._store_scores()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18242e7a-187f-448a-9d0f-c3d530fb3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_valid_dset(block):\n",
    "    num_empty_idx = sum(block.dset.data.data_lbl.getnnz(axis=1) == 0)\n",
    "    return block._getitems(np.where(block.dset.data.data_lbl.getnnz(axis=1) > 0)[0]) if num_empty_idx > 0 else block\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293d38d6-f35d-43a9-9238-b304fdd0052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_config(config:Union[str, Dict], config_key:Optional[str]=None, data_dir:Optional[str]=None, **kwargs):\n",
    "    if isinstance(config, str) and os.path.exists(config):\n",
    "        config = load_config(config, config_key)\n",
    "    elif isinstance(config, str):\n",
    "        config = CFGS[config](data_dir)[config_key]\n",
    "    elif isinstance(config, dict):\n",
    "        pass\n",
    "    else: raise ValueError(f'Invalid configuration: {config}')\n",
    "        \n",
    "    for k in config['parameters']:\n",
    "        if k in kwargs:config['parameters'][k]=kwargs.pop(k)\n",
    "    return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f406bd-fd91-4f5e-9048-61bea0456fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_info(info:Dict, config:Dict, max_sequence_length:int):\n",
    "    tokz, tokz_args = Info(), {p:config[p] for p in inspect.signature(Info.tokenize).parameters if p in config}\n",
    "    tokz.info = info\n",
    "    tokz.tokenize(**tokz_args, max_sequence_length=max_sequence_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ed756a-763d-452b-8aad-ea61a0d5328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augment_metadata(dset:Union[XCDataset,SXCDataset], meta_name:str, config:Union[str, Dict], \n",
    "                     config_key:Optional[str]=None, data_dir:Optional[str]=None, \n",
    "                     prompt:Optional[Callable]=None, sep_tok:Optional[str]=\" :: \", **kwargs):\n",
    "    if prompt is None: prompt = lambda x:x\n",
    "        \n",
    "    text = [prompt(o) for o in dset.data_info['input_text']]\n",
    "    data_meta = dset.meta[meta_name].data_meta\n",
    "    meta_info = dset.meta[meta_name].meta_info\n",
    "\n",
    "    aug_text = []\n",
    "    for p,q,txt in tqdm(zip(data_meta.indptr, data_meta.indptr[1:], text), total=len(text)):\n",
    "        aug_text.append(txt + sep_tok.join([meta_info['input_text'][i] for i in data_meta.indices[p:q]]))\n",
    "\n",
    "    config = get_config(config, config_key=config_key, data_dir=data_dir)\n",
    "    dset.data.data_info['input_text'] = aug_text\n",
    "    tokenize_info(dset.data.data_info, config['parameters'], \n",
    "                  max_sequence_length=config['parameters']['main_max_data_sequence_length'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42256e96-1d43-49b9-aeb4-bed88f55c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pkl_file(pkl_dir:str, fname:str, use_sxc_sampler:bool, use_exact:Optional[bool]=False, \n",
    "                 use_only_test:Optional[bool]=False, use_oracle:Optional[bool]=False, \n",
    "                 use_text_mode:Optional[bool]=False):\n",
    "    pkl_file = f'{pkl_dir}/{fname}'\n",
    "\n",
    "    if use_sxc_sampler: pkl_file = f'{pkl_file}_sxc'\n",
    "    else: pkl_file = f'{pkl_file}_xcs'\n",
    "            \n",
    "    if use_exact: pkl_file = f'{pkl_file}_exact'\n",
    "    if use_only_test: pkl_file = f'{pkl_file}_only-test'\n",
    "    if use_oracle: pkl_file = f'{pkl_file}_oracle'\n",
    "    if use_text_mode: pkl_file = f'{pkl_file}_text-mode'\n",
    "\n",
    "    return f'{pkl_file}.joblib'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89521f1e-c346-46f4-a070-d121a23e196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d9b5a44-cb8b-4339-bee8-3481d8269285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_block(pkl_file:str, config:Union[str,Dict], use_sxc:Optional[bool]=True, config_key:Optional[str]=None, \n",
    "                do_build:Optional[bool]=False, only_test:Optional[bool]=False, use_oracle:Optional[bool]=False, \n",
    "                remove_empty_datapoints:Optional[bool]=False, train_label_topk:Optional[int]=None, test_label_topk:Optional[int]=None, \n",
    "                \n",
    "                train_meta_topk:Optional[int]=None, test_meta_topk:Optional[int]=None, train_meta_abs_thresh:Optional[float]=None, \n",
    "                test_meta_abs_thresh:Optional[float]=None, train_meta_diff_thresh:Optional[float]=None, test_meta_diff_thresh:Optional[float]=None,\n",
    "                \n",
    "                \n",
    "                aug_meta_name:Optional[str]=None, aug_data_seq_length:Optional[int]=128, aug_lbl_seq_length:Optional[int]=128, \n",
    "                aug_exclude_sep:Optional[bool]=False, perform_data_meta_aug:Optional[bool]=False, perform_lbl_meta_aug:Optional[bool]=False, \n",
    "                prompt:Optional[Callable]=None, data_dir:Optional[str]=None, **kwargs):\n",
    "\n",
    "    if not os.path.exists(pkl_file): do_build = True\n",
    "\n",
    "    if do_build:\n",
    "        if isinstance(config, str) and os.path.exists(config): \n",
    "            config = load_config(config, config_key)\n",
    "            if only_test and 'train' in config['path']: del config['path']['train'] \n",
    "    \n",
    "        if use_sxc:\n",
    "            block = SXCBlock.from_cfg(config, config_key, padding=True, return_tensors='pt', data_dir=data_dir, **kwargs)\n",
    "            if use_oracle:\n",
    "                if block.train is not None and aug_meta_name in block.train.dset.meta:\n",
    "                    augment_metadata(dset=block.train.dset, meta_name=aug_meta_name, config=config, \n",
    "                                     config_key=config_key, data_dir=data_dir, prompt=prompt, padding=True, return_tensors='pt')\n",
    "                if block.test is not None and aug_meta_name in block.test.dset.meta:\n",
    "                    augment_metadata(dset=block.test.dset, meta_name=aug_meta_name, config=config, \n",
    "                                     config_key=config_key, data_dir=data_dir, prompt=prompt, padding=True, return_tensors='pt')\n",
    "        else: \n",
    "            block = XCBlock.from_cfg(config, config_key, transform_type='xcs', data_dir=data_dir, **kwargs)\n",
    "\n",
    "            if perform_data_meta_aug: block = AugmentMetaInputIdsTfm.apply(block, aug_meta_name, 'data', aug_data_seq_length, aug_exclude_sep)\n",
    "            if perform_lbl_meta_aug: block = AugmentMetaInputIdsTfm.apply(block, aug_meta_name, 'lbl', aug_lbl_seq_length, aug_exclude_sep)\n",
    "            \n",
    "        joblib.dump(block, pkl_file)\n",
    "    else:\n",
    "        block = joblib.load(pkl_file)\n",
    "\n",
    "        def set_data_attribute(obj):\n",
    "            for k in [o for o in vars(obj).keys() if not o.startswith('__')]: \n",
    "                if k in kwargs: setattr(obj, k, kwargs[k])\n",
    "\n",
    "        def get_meta_args(kwargs, meta_name, key):\n",
    "            return kwargs[key][meta_name] if isinstance(kwargs[key], dict) and meta_name in kwargs[key] else kwargs[key]\n",
    "\n",
    "        def set_meta_attribute(obj, meta_name):\n",
    "            for k in [o for o in vars(obj).keys() if not o.startswith('__')]:\n",
    "                if k in kwargs: setattr(obj, k, get_meta_args(kwargs, meta_name, k))\n",
    "\n",
    "        def set_block_attribute(bkl):            \n",
    "            if bkl is not None: \n",
    "                set_data_attribute(bkl.dset.data)\n",
    "                if bkl.dset.data.data_lbl_scores is None and (bkl.dset.data.use_main_distribution or bkl.dset.data.return_scores): \n",
    "                    bkl.dset.data._store_scores()\n",
    "                    \n",
    "                for k in bkl.dset.meta: \n",
    "                    set_meta_attribute(bkl.dset.meta[k], k)\n",
    "                    if bkl.dset.meta[k].data_meta_scores is None and (bkl.dset.meta[k].use_meta_distribution or bkl.dset.meta[k].return_scores): \n",
    "                        bkl.dset.meta[k]._store_scores()\n",
    "\n",
    "        set_block_attribute(block.train)\n",
    "        set_block_attribute(block.test)\n",
    "                        \n",
    "    if remove_empty_datapoints: \n",
    "        block = type(block)(train=get_valid_dset(block.train), test=get_valid_dset(block.test))\n",
    "\n",
    "    if train_label_topk is not None or test_label_topk is not None:\n",
    "        retain_topk_labels(block, train_k=train_label_topk, test_k=test_label_topk)\n",
    "\n",
    "    if (\n",
    "        train_meta_topk is not None or test_meta_topk is not None or\n",
    "        train_meta_abs_thresh is not None or test_meta_abs_thresh is not None or\n",
    "        train_meta_diff_thresh is not None or test_meta_diff_thresh is not None\n",
    "    ):\n",
    "        filter_metadata_block(block, train_topk=train_meta_topk, train_abs_thresh=train_meta_abs_thresh, \n",
    "                              train_diff_thresh=train_meta_diff_thresh, test_topk=test_meta_topk, \n",
    "                              test_abs_thresh=test_meta_abs_thresh, test_diff_thresh=test_meta_diff_thresh)\n",
    "        # retain_topk_metadata(block, train_k=train_meta_topk, test_k=test_meta_topk)\n",
    "\n",
    "    return block\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da69f1-9ff8-4f7f-b906-34950a2eb932",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9593d780-bf37-4df5-84eb-f583b7458cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def raw_mapping(src_file, targ_file):\n",
    "    src_ids, src_txt = load_raw_file(src_file)\n",
    "    targ_ids, targ_txt = load_raw_file(targ_file)\n",
    "\n",
    "    assert set(src_ids).difference(targ_ids) == 0\n",
    "    \n",
    "    src2targ_idx = torch.zeros(len(targ_ids), dtype=torch.long)\n",
    "    \n",
    "    targ_ids2idx = {k:i for i,k in enumerate(targ_ids)}\n",
    "    for i,id in enumerate(src_ids): src2targ_idx[i] = targ_ids2idx[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05311055-f92b-4d0f-a258-427858530fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_model(output_dir:str, model_fn:Callable, model_args:Dict, init_fn:Callable, init_args:Optional[Dict]=dict(), \n",
    "               do_inference:Optional[bool]=False, use_pretrained:Optional[bool]=False):\n",
    "    if do_inference:\n",
    "        os.environ['WANDB_MODE'] = 'disabled'\n",
    "        if not use_pretrained: model_args['mname'] = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'\n",
    "\n",
    "    model = model_fn(**model_args)\n",
    "    \n",
    "    if not do_inference or use_pretrained: \n",
    "        init_fn(model, **init_args)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28889520-f9ae-48d9-9d4c-af9a78a70b49",
   "metadata": {},
   "source": [
    "## `main` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4f07db6-41b4-4f32-9baf-0d1fc66423e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_output(pred_idx:torch.Tensor, pred_ptr:torch.Tensor, pred_score:torch.Tensor, n_lbl:int, **kwargs):\n",
    "    n_data = pred_ptr.shape[0]\n",
    "    pred_ptr = torch.cat([torch.zeros((1,), dtype=torch.long), pred_ptr.cumsum(dim=0)])\n",
    "    pred = sp.csr_matrix((pred_score,pred_idx,pred_ptr), shape=(n_data, n_lbl))\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effef22-5d33-4f35-89fa-2362ec75fb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1739a08-11b9-469e-b63d-c41f9952c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(learn, args, n_lbl:int, eval_dataset=None, train_dataset=None, eval_k:int=None, train_k:int=None, save_teacher:bool=False, \n",
    "         save_classifier:bool=False, resume_from_checkpoint:Optional[bool]=None, save_dir:Optional[str]=None, metadata_name:Optional[str]=None):\n",
    "    eval_dataset = learn.eval_dataset if eval_dataset is None else eval_dataset\n",
    "    train_dataset = learn.train_dataset if train_dataset is None else train_dataset\n",
    "    \n",
    "    do_infer = check_inference_mode(args)\n",
    "    \n",
    "    if do_infer:\n",
    "        trn_repr = tst_repr = lbl_repr = trn_pred = tst_pred = None\n",
    "        prediction_suffix = f'_{args.prediction_suffix}' if len(args.prediction_suffix) else ''\n",
    "        \n",
    "        pred_dir = f'{learn.args.output_dir}/predictions' if save_dir is None else f'{save_dir}/predictions'\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "        if args.save_representation:\n",
    "            trn_repr, lbl_repr = learn.get_data_and_lbl_representation(train_dataset)\n",
    "            tst_repr = learn._get_data_representation(eval_dataset)\n",
    "\n",
    "            torch.save(trn_repr, f'{pred_dir}/train_repr{prediction_suffix}.pth')\n",
    "            torch.save(tst_repr, f'{pred_dir}/test_repr{prediction_suffix}.pth')\n",
    "            torch.save(lbl_repr, f'{pred_dir}/label_repr{prediction_suffix}.pth')\n",
    "\n",
    "            if save_teacher:\n",
    "                config = TCHConfig(n_data=trn_repr.shape[0], n_lbl=lbl_repr.shape[0])\n",
    "                teacher = TCH001(config)\n",
    "                teacher.init_embeddings(trn_repr, lbl_repr)\n",
    "                teacher.freeze_embeddings()\n",
    "                teacher.save_pretrained(f'{learn.args.output_dir}/teacher')\n",
    "\n",
    "            if save_classifier:\n",
    "                classifier = CLS001(DistilBertConfig(), n_train=trn_repr.shape[0], n_test=tst_repr.shape[0], n_lbl=lbl_repr.shape[0])\n",
    "                classifier.init_representation(trn_repr, tst_repr, lbl_repr)\n",
    "                classifier.freeze_representation()\n",
    "                classifier.save_pretrained(f'{learn.args.output_dir}/representation')\n",
    "\n",
    "        if args.score_data_lbl or args.score_data_meta or args.score_lbl_meta:\n",
    "            trn_repr, lbl_repr = learn.get_data_and_lbl_representation(train_dataset)\n",
    "            tst_repr = learn._get_data_representation(eval_dataset)\n",
    "\n",
    "            save_dir = f'{learn.args.output_dir}/matrices' if save_dir is None else f'{save_dir}/matrices'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            if args.score_data_lbl:\n",
    "                trn_lbl = train_dataset.data.score_data_lbl(trn_repr, lbl_repr, batch_size=1024, normalize=args.normalize)\n",
    "                sp.save_npz(f\"{save_dir}/trn_lbl{prediction_suffix}.npz\", trn_lbl)\n",
    "                \n",
    "                tst_lbl = eval_dataset.data.score_data_lbl(tst_repr, lbl_repr, batch_size=1024, normalize=args.normalize)\n",
    "                sp.save_npz(f\"{save_dir}/tst_lbl{prediction_suffix}.npz\", tst_lbl)\n",
    "\n",
    "            if args.score_data_meta or args.score_lbl_meta:\n",
    "                meta_name = f'{learn.args.data_aug_meta_name}_meta' if metadata_name is None else f'{metadata_name}_meta'\n",
    "                meta_repr = learn._get_metadata_representation(eval_dataset if meta_name in eval_dataset.meta else train_dataset, meta_name=metadata_name)\n",
    "                if args.score_data_meta:\n",
    "                    if meta_name in train_dataset.meta:\n",
    "                        trn_meta = train_dataset.meta[meta_name].score_data_meta(trn_repr, meta_repr, batch_size=1024, normalize=args.normalize)\n",
    "                        sp.save_npz(f\"{save_dir}/trn_meta{prediction_suffix}.npz\", trn_meta)\n",
    "\n",
    "                    if meta_name in eval_dataset.meta:\n",
    "                        tst_meta = eval_dataset.meta[meta_name].score_data_meta(tst_repr, meta_repr, batch_size=1024, normalize=args.normalize)\n",
    "                        sp.save_npz(f\"{save_dir}/tst_meta{prediction_suffix}.npz\", tst_meta)\n",
    "\n",
    "                if args.score_lbl_meta:\n",
    "                    dset = train_dataset if meta_name in train_dataset.meta else eval_dataset\n",
    "                    lbl_meta = dset.meta[meta_name].score_lbl_meta(lbl_repr, meta_repr, batch_size=1024, normalize=args.normalize)\n",
    "                    sp.save_npz(f\"{save_dir}/lbl_meta{prediction_suffix}.npz\", lbl_meta)\n",
    "                    \n",
    "        if args.do_test_inference:\n",
    "            o = learn.predict(eval_dataset)\n",
    "            print(o.metrics)\n",
    "\n",
    "            tst_pred = get_output(o.pred_idx, o.pred_ptr, o.pred_score, n_lbl=n_lbl)\n",
    "            if eval_k is not None: tst_pred = retain_topk(tst_pred, k=eval_k)\n",
    "\n",
    "            if args.save_test_prediction:\n",
    "                with open(f'{pred_dir}/test_predictions{prediction_suffix}.pkl', 'wb') as file:\n",
    "                    pickle.dump(o, file)\n",
    "                sp.save_npz(f'{pred_dir}/test_predictions{prediction_suffix}.npz', tst_pred)\n",
    "\n",
    "        if args.do_train_inference:\n",
    "            o = learn.predict(train_dataset)\n",
    "            print(o.metrics)\n",
    "\n",
    "            trn_pred = get_output(o.pred_idx, o.pred_ptr, o.pred_score, n_lbl=n_lbl)\n",
    "            if train_k is not None: trn_pred = retain_topk(trn_pred, k=train_k)\n",
    "            \n",
    "            if args.save_train_prediction:\n",
    "                with open(f'{pred_dir}/train_predictions{prediction_suffix}.pkl', 'wb') as file:\n",
    "                    pickle.dump(o, file)\n",
    "                sp.save_npz(f'{pred_dir}/train_predictions{prediction_suffix}.npz', trn_pred)\n",
    "                \n",
    "        return trn_repr, tst_repr, lbl_repr, trn_pred, tst_pred\n",
    "    else:\n",
    "        learn.train(resume_from_checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc7d84-ae75-4923-b5b5-937170efb465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e54a5-2134-4446-b6ac-eee8a1970928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
