{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa699db4-a4b7-430e-8aef-1944f185fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d402020-9db0-4fc9-97dc-301109edd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048a095b-e95c-4870-b0f7-9e772c387879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f01b50-cb84-4bfc-aa47-c0655db43642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, torch, scipy.sparse as sp, joblib, argparse, pickle, numpy as np\n",
    "from typing import Optional, Dict, Callable, Union\n",
    "\n",
    "from transformers import DistilBertConfig\n",
    "\n",
    "from xcai.sdata import SXCDataBlock\n",
    "from xcai.data import XCDataBlock\n",
    "from xcai.block import SXCBlock, XCBlock\n",
    "from xcai.core import get_best_model, load_config\n",
    "from xcai.transform import AugmentMetaInputIdsTfm\n",
    "\n",
    "from xclib.utils.sparse import retain_topk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726e730-56ec-4dc4-9b68-18a7f824c81f",
   "metadata": {},
   "source": [
    "## `Arguement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2725be79-e4fc-418d-9798-61009ab95eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--build_block', action='store_true')\n",
    "    parser.add_argument('--use_pretrained', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--do_train_inference', action='store_true')\n",
    "    parser.add_argument('--do_test_inference', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--save_train_prediction', action='store_true')\n",
    "    parser.add_argument('--save_test_prediction', action='store_true')\n",
    "    parser.add_argument('--save_label_prediction', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--save_representation', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--use_sxc_sampler', action='store_true')\n",
    "    parser.add_argument('--only_test', action='store_true')\n",
    "\n",
    "    parser.add_argument('--pickle_dir', type=str, required=True)\n",
    "    \n",
    "    parser.add_argument('--prediction_suffix', type=str, default='')\n",
    "\n",
    "    parser.add_argument('--exact', action='store_true')\n",
    "    parser.add_argument('--dataset', type=str)\n",
    "    \n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924b980-4c91-4070-bb48-63d85e510b87",
   "metadata": {},
   "source": [
    "## Build `block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b53320-2edd-4fdb-af67-5c0fff8f7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def retain_topk_metadata(block, train_k:int=5, test_k:int=3):\n",
    "    for meta_name in block.test.dset.meta.keys():\n",
    "        if train_k is not None and block.train is not None:\n",
    "            data_meta = retain_topk(block.train.dset.meta[meta_name].data_meta, k=train_k)\n",
    "            lbl_meta = block.train.dset.meta[meta_name].lbl_meta\n",
    "            block.train.dset.meta[meta_name].update_meta_matrix(data_meta, lbl_meta)\n",
    "\n",
    "        if test_k is not None and block.test is not None:\n",
    "            data_meta = retain_topk(block.test.dset.meta[meta_name].data_meta, k=test_k)\n",
    "            lbl_meta = block.test.dset.meta[meta_name].lbl_meta\n",
    "            block.test.dset.meta[meta_name].update_meta_matrix(data_meta, lbl_meta)\n",
    "\n",
    "def retrain_topk_labels(block, train_k:int=5, test_k:int=3):\n",
    "    if train_k is not None and block.train is not None:\n",
    "        block.train.dset.data.data_lbl = retain_topk(block.train.dset.data.data_lbl, k=train_k)\n",
    "        block.train.dset.data._store_indices()\n",
    "\n",
    "    if test_k is not None and block.test is not None:\n",
    "        block.test.dset.data.data_lbl = retain_topk(block.test.dset.data.data_lbl, k=test_k)\n",
    "        block.test.dset.data._store_indices()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18242e7a-187f-448a-9d0f-c3d530fb3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_valid_dset(block):\n",
    "    num_empty_idx = sum(block.dset.data.data_lbl.getnnz(axis=1) == 0)\n",
    "    return block._getitems(np.where(block.dset.data.data_lbl.getnnz(axis=1) > 0)[0]) if num_empty_idx > 0 else block\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9b5a44-cb8b-4339-bee8-3481d8269285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_block(pkl_file:str, config:Union[str,Dict], use_sxc:Optional[bool]=True, config_key:Optional[str]=None, \n",
    "                do_build:Optional[bool]=False, only_test:Optional[bool]=False, remove_empty_datapoints:Optional[bool]=False, \n",
    "                train_label_topk:Optional[int]=None, test_label_topk:Optional[int]=None, train_meta_topk:Optional[int]=None, \n",
    "                test_meta_topk:Optional[int]=None, meta_name:Optional[str]=None, data_seq_length:Optional[int]=128, \n",
    "                lbl_seq_length:Optional[int]=128, exclude_sep:Optional[bool]=False, do_data_meta_aug:Optional[bool]=False,\n",
    "                do_lbl_meta_aug:Optional[bool]=False, **kwargs):\n",
    "\n",
    "    if not os.path.exists(pkl_file): do_build = True\n",
    "\n",
    "    if do_build:\n",
    "        if isinstance(config, str) and os.path.exists(config): \n",
    "            config = load_config(config, config_key)\n",
    "            if only_test and 'train' in config['path']: del config['path']['train'] \n",
    "    \n",
    "        if use_sxc: \n",
    "            block = SXCBlock.from_cfg(config, config_key, padding=True, return_tensors='pt', **kwargs)\n",
    "        else: \n",
    "            block = XCBlock.from_cfg(config, config_key, transform_type='xcs', **kwargs)\n",
    "\n",
    "            if do_data_meta_aug: block = AugmentMetaInputIdsTfm.apply(block, f'{meta_name}_meta', 'data', data_seq_length, exclude_sep)\n",
    "            if do_lbl_meta_aug: block = AugmentMetaInputIdsTfm.apply(block, f'{meta_name}_meta', 'lbl', lbl_seq_length, exclude_sep)\n",
    "            \n",
    "        joblib.dump(block, pkl_file)\n",
    "    else:\n",
    "        block = joblib.load(pkl_file)\n",
    "\n",
    "        if isinstance(block, SXCDataBlock):\n",
    "            if 'n_slbl_samples' in kwargs: \n",
    "                if block.train is not None: block.train.dset.data.n_slbl_samples = kwargs['n_slbl_samples']\n",
    "                if block.test is not None: block.test.dset.data.n_slbl_samples = kwargs['n_slbl_samples']\n",
    "                \n",
    "            if 'main_oversample' in kwargs: \n",
    "                if block.train is not None: block.train.dset.data.main_oversample = kwargs['main_oversample']\n",
    "                if block.test is not None: block.test.dset.data.main_oversample = kwargs['main_oversample']\n",
    "\n",
    "            if 'use_main_distribution' in kwargs: \n",
    "                if block.train is not None: \n",
    "                    block.train.dset.data.use_main_distribution = kwargs['use_main_distribution']\n",
    "                    if block.train.dset.data.data_lbl_scores is None and kwargs['use_main_distribution']: \n",
    "                        block.train.dset.data._store_scores()\n",
    "                if block.test is not None: \n",
    "                    block.test.dset.data.use_main_distribution = kwargs['use_main_distribution']\n",
    "                    if block.test.dset.data.data_lbl_scores is None and kwargs['use_main_distribution']: \n",
    "                        block.test.dset.data._store_scores()\n",
    "\n",
    "            if 'return_scores' in kwargs:\n",
    "                if block.train is not None: \n",
    "                    block.train.dset.data.return_scores = kwargs['return_scores']\n",
    "                    if block.train.dset.data.data_lbl_scores is None and kwargs['return_scores']: \n",
    "                        block.train.dset.data._store_scores()\n",
    "                if block.test is not None: \n",
    "                    block.test.dset.data.return_scores = kwargs['return_scores']\n",
    "                    if block.test.dset.data.data_lbl_scores is None and kwargs['return_scores']: \n",
    "                        block.test.dset.data._store_scores()\n",
    "\n",
    "            if block.test is not None:\n",
    "                for k in block.test.dset.meta:\n",
    "                    if 'n_sdata_meta_samples' in kwargs: block.test.dset.meta[k].n_sdata_meta_samples = kwargs['n_sdata_meta_samples']\n",
    "                    if 'n_slbl_meta_samples' in kwargs: block.test.dset.meta[k].n_slbl_meta_samples = kwargs['n_slbl_meta_samples']\n",
    "                    if 'meta_oversample' in kwargs: block.test.dset.meta[k].meta_oversample = kwargs['meta_oversample']\n",
    "                    if 'use_meta_distribution' in kwargs: \n",
    "                        block.test.dset.meta[k].use_meta_distribution = kwargs['use_meta_distribution']\n",
    "                        if block.test.dset.meta[k].data_meta_scores is None and kwargs['use_meta_distribution']: \n",
    "                            block.test.dset.meta[k]._store_scores()\n",
    "                    if 'return_scores' in kwargs: \n",
    "                        block.test.dset.meta[k].return_scores = kwargs['return_scores']\n",
    "                        if block.test.dset.meta[k].data_meta_scores is None and kwargs['return_scores']: \n",
    "                            block.test.dset.meta[k]._store_scores()\n",
    "\n",
    "            if block.train is not None:\n",
    "                for k in block.train.dset.meta:\n",
    "                    if 'n_sdata_meta_samples' in kwargs: block.train.dset.meta[k].n_sdata_meta_samples = kwargs['n_sdata_meta_samples']\n",
    "                    if 'n_slbl_meta_samples' in kwargs: block.train.dset.meta[k].n_slbl_meta_samples = kwargs['n_slbl_meta_samples']\n",
    "                    if 'meta_oversample' in kwargs: block.train.dset.meta[k].meta_oversample = kwargs['meta_oversample']    \n",
    "                    if 'use_meta_distribution' in kwargs: \n",
    "                        block.train.dset.meta[k].use_meta_distribution = kwargs['use_meta_distribution']\n",
    "                        if block.train.dset.meta[k].data_meta_scores is None and kwargs['use_meta_distribution']: \n",
    "                            block.train.dset.meta[k]._store_scores()\n",
    "                    if 'meta_dropout_remove' in kwargs: block.train.dset.meta[k].meta_dropout_remove = kwargs['meta_dropout_remove']\n",
    "                    if 'meta_dropout_replace' in kwargs: block.train.dset.meta[k].meta_dropout_replace = kwargs['meta_dropout_replace']\n",
    "                    if 'return_scores' in kwargs: \n",
    "                        block.train.dset.meta[k].return_scores = kwargs['return_scores']\n",
    "                        if block.train.dset.meta[k].data_meta_scores is None and kwargs['return_scores']: \n",
    "                            block.train.dset.meta[k]._store_scores()\n",
    "                        \n",
    "    if remove_empty_datapoints: block = type(block)(train=get_valid_dset(block.train), test=get_valid_dset(block.test))\n",
    "\n",
    "    if train_label_topk is not None or test_label_topk is not None:\n",
    "        retrain_topk_labels(block, train_k=train_label_topk, test_k=test_label_topk)\n",
    "\n",
    "    if train_meta_topk is not None or test_meta_topk is not None:\n",
    "        retain_topk_metadata(block, train_k=train_meta_topk, test_k=test_meta_topk)\n",
    "\n",
    "    return block\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da69f1-9ff8-4f7f-b906-34950a2eb932",
   "metadata": {},
   "source": [
    "## Load `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05311055-f92b-4d0f-a258-427858530fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_model(output_dir:str, model_fn:Callable, model_args:Dict, init_fn:Callable, init_args:Optional[Dict]=dict(), \n",
    "               do_inference:Optional[bool]=False, use_pretrained:Optional[bool]=False):\n",
    "    if do_inference:\n",
    "        os.environ['WANDB_MODE'] = 'disabled'\n",
    "        if not use_pretrained: model_args['mname'] = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'\n",
    "\n",
    "    model = model_fn(**model_args)\n",
    "    \n",
    "    if not do_inference or use_pretrained: \n",
    "        init_fn(model, **init_args)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28889520-f9ae-48d9-9d4c-af9a78a70b49",
   "metadata": {},
   "source": [
    "## `main` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f07db6-41b4-4f32-9baf-0d1fc66423e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_output(pred_idx:torch.Tensor, pred_ptr:torch.Tensor, pred_score:torch.Tensor, n_lbl:int, **kwargs):\n",
    "    n_data = pred_ptr.shape[0]\n",
    "    pred_ptr = torch.cat([torch.zeros((1,), dtype=torch.long), pred_ptr.cumsum(dim=0)])\n",
    "    pred = sp.csr_matrix((pred_score,pred_idx,pred_ptr), shape=(n_data, n_lbl))\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effef22-5d33-4f35-89fa-2362ec75fb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1739a08-11b9-469e-b63d-c41f9952c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(learn, args, n_lbl:int, eval_dataset=None, train_dataset=None, eval_k:int=None, train_k:int=None, save_teacher:bool=False, \n",
    "         save_classifier:bool=False, resume_from_checkpoint:Optional[bool]=None):\n",
    "    eval_dataset = learn.eval_dataset if eval_dataset is None else eval_dataset\n",
    "    train_dataset = learn.train_dataset if train_dataset is None else train_dataset\n",
    "    \n",
    "    do_infer = args.do_train_inference or args.do_test_inference or args.save_train_prediction \\\n",
    "            or args.save_test_prediction or args.save_representation\n",
    "    \n",
    "    if do_infer:\n",
    "        trn_repr = tst_repr = lbl_repr = trn_pred = tst_pred = None\n",
    "        prediction_suffix = f'_{args.prediction_suffix}' if len(args.prediction_suffix) else ''\n",
    "        \n",
    "        pred_dir = f'{learn.args.output_dir}/predictions'\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "        if args.save_representation:\n",
    "            trn_repr, lbl_repr = learn.get_data_and_lbl_representation(train_dataset)\n",
    "            tst_repr = learn._get_data_representation(eval_dataset)\n",
    "\n",
    "            torch.save(trn_repr, f'{pred_dir}/train_repr{prediction_suffix}.pth')\n",
    "            torch.save(tst_repr, f'{pred_dir}/test_repr{prediction_suffix}.pth')\n",
    "            torch.save(lbl_repr, f'{pred_dir}/label_repr{prediction_suffix}.pth')\n",
    "\n",
    "            if save_teacher:\n",
    "                teacher = TCH001(DistilBertConfig(), n_data=trn_repr.shape[0], n_lbl=lbl_repr.shape[0])\n",
    "                teacher.init_embeddings(trn_repr, lbl_repr)\n",
    "                teacher.freeze_embeddings()\n",
    "                teacher.save_pretrained(f'{learn.args.output_dir}/teacher')\n",
    "\n",
    "            if save_classifier:\n",
    "                classifier = CLS001(DistilBertConfig(), n_train=trn_repr.shape[0], n_test=tst_repr.shape[0], n_lbl=lbl_repr.shape[0])\n",
    "                classifier.init_representation(trn_repr, tst_repr, lbl_repr)\n",
    "                classifier.freeze_representation()\n",
    "                classifier.save_pretrained(f'{learn.args.output_dir}/representation')\n",
    "\n",
    "        if args.do_test_inference:\n",
    "            o = learn.predict(eval_dataset)\n",
    "            print(o.metrics)\n",
    "\n",
    "            tst_pred = get_output(o.pred_idx, o.pred_ptr, o.pred_score, n_lbl=n_lbl)\n",
    "            if eval_k is not None: tst_pred = retain_topk(tst_pred, k=eval_k)\n",
    "\n",
    "            if args.save_test_prediction:\n",
    "                with open(f'{pred_dir}/test_predictions{prediction_suffix}.pkl', 'wb') as file:\n",
    "                    pickle.dump(o, file)\n",
    "                sp.save_npz(f'{pred_dir}/test_predictions{prediction_suffix}.npz', tst_pred)\n",
    "\n",
    "        if args.do_train_inference:\n",
    "            o = learn.predict(train_dataset)\n",
    "            print(o.metrics)\n",
    "\n",
    "            trn_pred = get_output(o.pred_idx, o.pred_ptr, o.pred_score, n_lbl=n_lbl)\n",
    "            if train_k is not None: trn_pred = retain_topk(trn_pred, k=train_k)\n",
    "            \n",
    "            if args.save_train_prediction:\n",
    "                with open(f'{pred_dir}/train_predictions{prediction_suffix}.pkl', 'wb') as file:\n",
    "                    pickle.dump(o, file)\n",
    "                sp.save_npz(f'{pred_dir}/train_predictions{prediction_suffix}.npz', trn_pred)\n",
    "                \n",
    "        return trn_repr, tst_repr, lbl_repr, trn_pred, tst_pred\n",
    "    else:\n",
    "        learn.train(resume_from_checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be69387-7ed9-4d22-b8ca-b072aa6e25bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5c3b1-5687-454b-9de6-abad19f5f994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
