{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483ee0f1-49bb-4794-9612-9e8182da0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.upma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563da750-7f38-4597-948c-236a7ca27667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464ed0be-1ad4-4722-96e7-791081a9abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "52a38dda-cc60-48e0-8528-a896969d33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, torch.nn as nn, re, os, numpy as np, gc, torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, Union, Tuple, Any, Dict, Sequence, List\n",
    "\n",
    "from transformers import DistilBertConfig, DistilBertModel, PretrainedConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import Embeddings, TransformerBlock, create_sinusoidal_embeddings\n",
    "from transformers.activations import get_activation\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.losses import *\n",
    "from xcai.data import MainXCDataset\n",
    "from xcai.sdata import SMainXCDataset, identity_collate_fn\n",
    "from xcai.learner import XCDataParallel\n",
    "from xcai.models.modeling_utils import Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c0fa-2829-42dc-bb53-1fed8107a973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc73605-f2b1-4e69-9780-c1932fc9f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7558da2e-090d-4b47-81bc-d43c00854412",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/suchith720/Projects/data'\n",
    "config_file = 'wikiseealsotitles'\n",
    "config_key = 'data_meta'\n",
    "\n",
    "mname = 'sentence-transformers/msmarco-distilbert-base-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba6c85c-111e-4496-b9c0-66de3ac303b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = f'{data_dir}/processed/mogicX/'\n",
    "pkl_file = f'{pkl_dir}/wikiseealsotitles_data-meta_distilbert-base-uncased_sxc.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072f908a-9942-4ccb-af2d-dfb3b624bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = build_block(pkl_file, config_file, True, config_key, data_dir=data_dir, \n",
    "                    n_sdata_meta_samples=3, do_build=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d1bba-3b4e-4f99-ab30-d3ac855edd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61f91ea-52b0-428e-aaea-90352cbae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.dset.__getitems__([10, 30])\n",
    "batch = block.train.dset.__getitems__([10, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ba99d74-faae-4d3e-bac2-0c088267a3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_scores', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_scores', 'cat2data_data2ptr', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'pcat2lbl_idx', 'pcat2lbl_lbl2ptr', 'cat2lbl_idx', 'cat2lbl_scores', 'cat2lbl_lbl2ptr', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_data2ptr', 'pcat2lbl_data2ptr'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65120292-4da4-4316-81b0-d49942e92661",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "61fc143b-bd04-4d10-a838-728c10ed9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAConfig(DistilBertConfig):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        memory_module_names: Optional[Union[str, List[str]]] = \"embeddings\",\n",
    "        memory_injection_layers: Optional[Union[int, List[int]]] = None,\n",
    "        \n",
    "        num_total_metadata: Optional[int] = None,\n",
    "        num_input_metadata: Optional[int] = 3,\n",
    "        metadata_dropout: Optional[float] = 0.1,\n",
    "        \n",
    "        n_memory_layers: Optional[int] = 3,\n",
    "        \n",
    "        data_aug_meta_prefix: Optional[str] = None, \n",
    "        lbl2data_aug_meta_prefix: Optional[str] = None,\n",
    "        neg2data_aug_meta_prefix: Optional[str] = None,\n",
    "\n",
    "        data_inject_memory: Optional[bool] = True,\n",
    "        lbl2data_inject_memory: Optional[bool] = True,\n",
    "        neg2data_inject_memory: Optional[bool] = True,\n",
    "        \n",
    "        data_repr_pooling: Optional[bool] = True,\n",
    "        data_normalize: Optional[bool] = False,\n",
    "\n",
    "        margin: Optional[float] = 0.3,\n",
    "        num_negatives: Optional[int] = 10,\n",
    "        tau: Optional[float] = 0.1,\n",
    "        apply_softmax: Optional[bool] = True,\n",
    "        reduction: Optional[str] = \"mean\",\n",
    "\n",
    "        calib_margin: Optional[float] = 0.05,\n",
    "        calib_num_negatives: Optional[int] = 10,\n",
    "        calib_tau: Optional[float] = 0.1,\n",
    "        calib_apply_softmax: Optional[bool] = False,\n",
    "        \n",
    "        calib_loss_weight: Optional[float] = 0.1,\n",
    "        use_calib_loss: Optional[bool] = False,\n",
    "\n",
    "        use_encoder_parallel: Optional[bool] = False,\n",
    "\n",
    "        loss_function: Optional[str] = \"triplet\",\n",
    "        initialize_memory_embeddings_from_injection_layer_mean: Optional[bool] = True,\n",
    "        metadata_embedding_file: Optional[str] = None,\n",
    "        \n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr('memory_module_names,memory_injection_layers')\n",
    "        store_attr('num_total_metadata,num_input_metadata,metadata_dropout')\n",
    "        store_attr('n_memory_layers')\n",
    "        store_attr('data_aug_meta_prefix,lbl2data_aug_meta_prefix,neg2data_aug_meta_prefix')\n",
    "        store_attr('data_inject_memory,lbl2data_inject_memory,neg2data_inject_memory')\n",
    "        store_attr('data_repr_pooling,data_normalize')\n",
    "        store_attr('margin,num_negatives,tau,apply_softmax,reduction')\n",
    "        store_attr('calib_margin,calib_num_negatives,calib_tau,calib_apply_softmax')\n",
    "        store_attr('calib_loss_weight,use_calib_loss,use_encoder_parallel')\n",
    "        store_attr('loss_function,initialize_memory_embeddings_from_injection_layer_mean,metadata_embedding_file')\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if isinstance(memory_module_names, str): self.memory_module_names = [memory_module_names]\n",
    "        if memory_injection_layers is None: \n",
    "            self.memory_injection_layers = [self.n_layers] * len(self.memory_module_names)\n",
    "\n",
    "        assert len(self.memory_injection_layers) == len(self.memory_module_names), (\n",
    "            f\"Mismatch in configuration: expected equal lengths, but got \"\n",
    "            f\"{len(self.memory_injection_layers)} injection layers and \"\n",
    "            f\"{len(self.memory_module_names)} module names.\"\n",
    "        )\n",
    "\n",
    "        idx = np.argsort(self.memory_injection_layers)\n",
    "        self.memory_injection_layers = [self.memory_injection_layers[i] for i in idx]\n",
    "        self.memory_module_names = [self.memory_module_names[i] for i in idx]\n",
    "        \n",
    "        assert np.all([n <= self.n_layers and n > 0 for n in self.memory_injection_layers]), (\n",
    "            f\"Invalid memory injection layer: {self.memory_injection_layers}. \"\n",
    "            f\"It must be less than or equal to the total number of layers ({self.n_layers}) and greater than zero.\"\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d895775-9046-4ba6-b800-8e7ec7fc7a8f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "132017f5-4bf1-408a-8206-c0648255226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UPMAConfig(\n",
    "    memory_module_names = [\"embeddings\", \"encoder\"],\n",
    "    memory_injection_layers = [3, 1],\n",
    "    \n",
    "    num_total_metadata = block.train.dset.meta['cat_meta'].n_meta,\n",
    "    num_input_metadata = 3,\n",
    "    metadata_dropout = 0.1,\n",
    "    \n",
    "    n_memory_layers = 3,\n",
    "\n",
    "    data_aug_meta_prefix=\"cat2data\",\n",
    "    lbl2data_aug_meta_prefix=\"cat2lbl\",\n",
    "    neg2data_aug_meta_prefix=\"cat2neg\",\n",
    "\n",
    "    data_inject_memory=True,\n",
    "    lbl2data_inject_memory=False,\n",
    "    neg2data_inject_memory=False,\n",
    "    \n",
    "    data_repr_pooling=False,\n",
    "    data_normalize=False,\n",
    "\n",
    "    margin=0.3,\n",
    "    num_negatives=5,\n",
    "    tau=0.1,\n",
    "    apply_softmax=True,\n",
    "    reduction=\"mean\",\n",
    "\n",
    "    calib_margin=0.3,\n",
    "    calib_num_negatives=10,\n",
    "    calib_tau=0.1,\n",
    "    calib_apply_softmax=False,\n",
    "    \n",
    "    calib_loss_weight=0.1,\n",
    "    use_calib_loss=True,\n",
    "    \n",
    "    use_encoder_parallel=False,\n",
    "    loss_function=\"triplet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a030ffeb-9c3e-4a96-9223-2f2c79dca56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.data_inject_memory, config.lbl2data_inject_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d5c07b14-f558-4dea-bfd2-b0e279792b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 3], ['encoder', 'embeddings'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.memory_injection_layers, config.memory_module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e2642318-bb6e-4f5b-8d5b-8f38843d43a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('triplet', 'mean', False)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.loss_function, config.reduction, config.data_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19777e6-d47c-4903-a417-7031c5fe29d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d83bebd2-6fb8-4277-870c-304cf440af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_memory_module(name: str):\n",
    "    if name == \"embeddings\": return UPMAEmbeddingMemory\n",
    "    elif name == \"encoder\": return UPMAEncoderMemory\n",
    "    else: raise ValueError(f\"Invalid memory module: {name}\")\n",
    "\n",
    "def get_loss_function(name:str):\n",
    "    if name == \"triplet\": return MultiTripletWithNegatives\n",
    "    elif name == \"ranking\": return MultiRankingWithNegatives\n",
    "    elif name == \"soupcon\": return MultiSoupConWithNegatives\n",
    "    elif name == \"margin\": return MarginMSEWithNegatives\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid loss function name: '{name}'. \"\n",
    "            \"Please provide a valid loss function identifier supported by this module.\"\n",
    "        )\n",
    "\n",
    "def align_tensor(tensor:torch.Tensor, indptr:torch.Tensor, pad_tok:Optional[Union[int,float]]=0):\n",
    "    tensor_shape = tensor.shape\n",
    "    r, c = len(indptr), indptr.max()\n",
    "\n",
    "    row_idx = torch.repeat_interleave(torch.arange(r, device=tensor.device), indptr)\n",
    "    indptr = torch.cat([indptr.new_tensor([0]), indptr.cumsum(dim=0)[:-1]], dim=0)\n",
    "    within_idx = torch.arange(tensor_shape[0], device=indptr.device) - indptr[row_idx]\n",
    "\n",
    "    output = torch.full((r, c, *tensor_shape[1:]), pad_tok, device=tensor.device, dtype=tensor.dtype)\n",
    "    mask = torch.zeros((r, c), device=tensor.device)\n",
    "\n",
    "    output[row_idx, within_idx] = tensor\n",
    "    mask[row_idx, within_idx] = 1.0\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "def alignment_mask(indptr:torch.Tensor):\n",
    "    n, r, c = indptr.sum(), len(indptr), indptr.max()\n",
    "\n",
    "    row_idx = torch.repeat_interleave(torch.arange(r, device=indptr.device), indptr)\n",
    "    indptr = torch.cat([indptr.new_tensor([0]), indptr.cumsum(dim=0)[:-1]], dim=0)\n",
    "    within_idx = torch.arange(n, device=indptr.device) - indptr[row_idx]\n",
    "\n",
    "    mask = torch.zeros((r, c), device=indptr.device, dtype=torch.int64)\n",
    "    mask[row_idx, within_idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792cdc7-ac6f-4947-bbef-1d05c24ea23f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24b1d4ae-c88a-4a32-a243-837251370eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(100, size=(5, 3))\n",
    "indptr = torch.tensor([1, 1, 0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "153e6ff4-59c8-4bbb-a9a0-59a22234035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = align_tensor(t, indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8815ddc4-4bef-4485-b382-9b6223ebb954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6, 84, 31],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[16, 12, 85],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[95, 29, 80],\n",
       "          [82, 45, 10],\n",
       "          [81, 43, 63]]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a6af7-70b6-408c-ab89-40293db3ecea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "488154eb-8e12-41e0-8987-b98b0a44d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UPMAEncoderOutput(BaseModelOutput):\n",
    "    repr: Optional[torch.FloatTensor] = None\n",
    "    \n",
    "@dataclass\n",
    "class UPMAModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    neg2data_repr: Optional[torch.FloatTensor] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d2580-d5b1-4e0a-b297-bf54218a534f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3cb9f-2449-4ba3-90c4-26914e26771a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `FFN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8bab1-ce05-4092-9868-6279819f1e9d",
   "metadata": {},
   "source": [
    "* $\\hat{x} = dropout(W_2 * max(0, W_1 * x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352e0cbc-95ff-4d17-985e-f38305c0f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FFN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config:PretrainedConfig,\n",
    "        input_dim:int,\n",
    "        hidden_dim:int,\n",
    "        output_dim:int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.lin1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        self.activation = get_activation(config.activation)\n",
    "        \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
    "\n",
    "    def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.lin1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937df2a6-0c22-417d-adcc-f0a97a65192a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `Embedding Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c45035-daf4-4d72-800c-97389048673f",
   "metadata": {},
   "source": [
    "* $\\hat{\\mathcal{A}} = \\{a_1, a_2, a_3, \\dots a_n\\}$ be relevant metadata predicted by the linker.\n",
    "\n",
    "* $\\hat{\\mathcal{S}} = \\{s_1, s_2, s_3, \\dots s_n\\}$ be scores of the predicted metadata.\n",
    "\n",
    "* $\\hat{\\mathcal{R}} = \\{1, 2, 3, \\dots n\\}$ be rank for the predicted metadata.\n",
    "\n",
    "* $\\mathcal{K} \\in R^{M \\times D}$ be the $M$ memory items for each metadata.\n",
    "\n",
    "* $\\mathcal{P} \\in R^{N \\times D}$ be the $N$ positional embeddings.\n",
    "\n",
    "* Rank and score aware metadata representation: $x_m = \\mathcal{K}(\\hat{\\mathcal{A}}) + MLP(\\hat{\\mathcal{S}}) + \\mathcal{P}(\\hat{\\mathcal{R}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9aec34b2-9d4c-4fc2-9bfd-a8644e780b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAEmbeddingMemory(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PretrainedConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.metadata_embeddings = nn.Embedding(config.num_total_metadata+1, config.dim, padding_idx=config.num_total_metadata)\n",
    "        self.rank_embeddings = nn.Embedding(config.num_input_metadata, config.dim)\n",
    "        \n",
    "        self.score_ffn = FFN(config, input_dim=1, hidden_dim=config.hidden_dim, output_dim=config.dim)\n",
    "        self.out_ffn = FFN(config, input_dim=config.dim, hidden_dim=config.hidden_dim, output_dim=config.dim)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.metadata_dropout)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.num_input_metadata).expand((1, -1)), persistent=False\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls, \n",
    "        config:PretrainedConfig\n",
    "    ):\n",
    "        model = cls(config)\n",
    "        model.eval()\n",
    "        return model\n",
    "        \n",
    "    def get_metadata_embeddings(self) -> torch.Tensor:\n",
    "        return self.metadata_embeddings.weight\n",
    "\n",
    "    def set_metadata_embeddings(self, new_embeddings: torch.Tensor):\n",
    "        self.metadata_embeddings.weight.copy_(new_embeddings)\n",
    "\n",
    "    def get_rank_embeddings(self) -> torch.Tensor:\n",
    "        return self.rank_embeddings.weight\n",
    "\n",
    "    def set_rank_embeddings(self, new_embeddings: torch.Tensor):\n",
    "        self.rank_embeddings.weight.copy_(new_embeddings)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_idx: torch.Tensor,\n",
    "        input_embeds: Optional[torch.Tensor] = None,\n",
    "        scores: Optional[torch.Tensor] = None,\n",
    "        indptr: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        # `input_idx`: (total_input_metadata)\n",
    "        # `scores`: (total_input_metadata)\n",
    "        \n",
    "        if input_idx is not None:\n",
    "            input_idx, mask = align_tensor(input_idx, indptr, pad_tok=self.config.num_total_metadata) # (bs, num_input_metadata)\n",
    "            embeds = self.metadata_embeddings(input_idx) # (bs, num_input_metadata, dim)\n",
    "        else:\n",
    "            assert input_embeds is not None, \"Invalid input: both `input_idx` and `input_embeds` cannot be None.\" \n",
    "            embeds, mask = align_tensor(input_embeds, indptr)\n",
    "            \n",
    "        if embeds.size(1) != self.config.num_input_metadata:\n",
    "            raise ValueError(\n",
    "                f\"Invalid input: expected {self.config.num_input_metadata} metadata items, \"\n",
    "                f\"but got {embeds.size(1)}.\"\n",
    "            )\n",
    "\n",
    "        if scores is not None:\n",
    "            scores, mask = align_tensor(scores, indptr) # (bs, num_input_metadata)\n",
    "            \n",
    "        if position_ids is None:\n",
    "            position_ids = (\n",
    "                self.position_ids[:, :self.config.num_input_metadata]\n",
    "                if scores is None else \n",
    "                torch.argsort(scores, dim=1, descending=True)\n",
    "            )\n",
    "            \n",
    "        rank_embeddings = self.rank_embeddings(position_ids) # (bs, num_input_metadata, dim) or (1, num_input_metadata, dim)\n",
    "\n",
    "        embeddings = embeds + rank_embeddings\n",
    "\n",
    "        if scores is not None:\n",
    "            score_embeddings = self.score_ffn(scores.unsqueeze(2)) # (bs, num_input_metadata, dim)\n",
    "            embeddings = embeddings + score_embeddings\n",
    "            \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return UPMAEncoderOutput(repr=self.out_ffn(embeddings)), mask # (bs, num_input_metadata, dim), (bs, num_input_metadata)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a209d3-26be-4600-8026-3de2d8a04383",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b997cd0-4ad4-4752-8cb3-c916f81e4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPMAEmbeddingMemory(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c83b6c-a17c-4c66-b2a6-fdeb50f77619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9530e422-d41d-4baf-8362-336042293247",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'input_idx': torch.randint(config.num_total_metadata, size=(5,)),\n",
    "    'scores': torch.rand((5,)),\n",
    "    'indptr': torch.tensor([1, 1, 0, 3], dtype=torch.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "15845610-6dfd-4616-92d7-a9b9e1b2e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds, mask = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384f878-8029-4660-a2e6-fe8e79bc8629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ac7993e-1aa3-48b2-b397-2cb0d69116d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `Encoder Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13acb806-2bb2-432a-80d1-ec9766a6dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAEncoderMemory(PreTrainedModel):\n",
    "    config: UPMAConfig\n",
    "    load_tf_weights = None\n",
    "    base_model_prefix = \"distilbert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.n_layers = config.n_memory_layers\n",
    "        self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_memory_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
    "        self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                self.config.max_position_embeddings, self.config.dim, module.position_embeddings.weight\n",
    "            )\n",
    "            \n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.position_embeddings\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        num_position_embeds_diff = new_num_position_embeddings - self.config.max_position_embeddings\n",
    "\n",
    "        # no resizing needs to be done if the length stays the same\n",
    "        if num_position_embeds_diff == 0:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Setting `config.max_position_embeddings={new_num_position_embeddings}`...\")\n",
    "        self.config.max_position_embeddings = new_num_position_embeddings\n",
    "\n",
    "        old_position_embeddings_weight = self.embeddings.position_embeddings.weight.clone()\n",
    "\n",
    "        self.embeddings.position_embeddings = nn.Embedding(self.config.max_position_embeddings, self.config.dim)\n",
    "\n",
    "        if self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=self.config.max_position_embeddings, dim=self.config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if num_position_embeds_diff > 0:\n",
    "                    self.embeddings.position_embeddings.weight[:-num_position_embeds_diff] = nn.Parameter(\n",
    "                        old_position_embeddings_weight\n",
    "                    )\n",
    "                else:\n",
    "                    self.embeddings.position_embeddings.weight = nn.Parameter(\n",
    "                        old_position_embeddings_weight[:num_position_embeds_diff]\n",
    "                    )\n",
    "        # move position_embeddings to correct device\n",
    "        self.embeddings.position_embeddings.to(self.device)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: nn.Embedding):\n",
    "        self.embeddings.word_embeddings = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: dict[int, list[list[int]]]):\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.transformer.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        config:PretrainedConfig,\n",
    "    ):  \n",
    "        src_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        targ_model = cls(config)\n",
    "        targ_model.init_weights()\n",
    "        targ_model.eval()\n",
    "        \n",
    "        src_sd, targ_sd = src_model.state_dict(), targ_model.state_dict()\n",
    "        src_keys, targ_keys = set(src_sd.keys()), set(targ_sd.keys())\n",
    "        \n",
    "        for k in src_keys.intersection(targ_keys):\n",
    "            assert targ_sd[k].shape == src_sd[k].shape, (\n",
    "                f\"Shape mismatch at key '{k}'. \"\n",
    "                f\"Expected {targ_sd[k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "            )\n",
    "            targ_sd[k].copy_(src_sd[k])\n",
    "\n",
    "        diff_keys = targ_keys.difference(src_keys)\n",
    "        transformer_keys = [k for k in src_keys if k.startswith(\"transformer\")]\n",
    "        for k in transformer_keys:\n",
    "            targ_k = k.split('.', maxsplit=1)[1]\n",
    "            \n",
    "            if targ_k in targ_sd:\n",
    "                assert targ_sd[targ_k].shape == src_sd[k].shape, (\n",
    "                    f\"Shape mismatch at key '{k}'. \"\n",
    "                    f\"Expected {targ_sd[targ_k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "                )\n",
    "                \n",
    "                targ_sd[targ_k].copy_(src_sd[k])\n",
    "                diff_keys.remove(targ_k)\n",
    "\n",
    "        assert len(diff_keys) == 0, (\n",
    "            f\"Initialization error: the following parameters were not initialized in the module: {diff_keys}\"\n",
    "        )            \n",
    "        return targ_model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        indptr: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]:\n",
    "         \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        head_mask_is_none = head_mask is None\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
    "        \n",
    "        def _prepare_attention_mask(attention_mask, input_shape):\n",
    "            if self._use_flash_attention_2:\n",
    "                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "            else:\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
    "    \n",
    "                if self._use_sdpa and head_mask_is_none and not output_attentions:\n",
    "                    attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
    "                        attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n",
    "                    )\n",
    "            return attention_mask\n",
    "\n",
    "        _attention_mask = _prepare_attention_mask(attention_mask, input_shape)\n",
    "                \n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_state = embeddings\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "                \n",
    "            layer_outputs = layer_module(\n",
    "                hidden_state,\n",
    "                _attention_mask,\n",
    "                head_mask[i],\n",
    "                output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_state = layer_outputs[-1]\n",
    "\n",
    "            if output_attentions:\n",
    "                if len(layer_outputs) != 2:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 2, but it is {len(layer_outputs)}\")\n",
    "\n",
    "                attentions = layer_outputs[0]\n",
    "                all_attentions = all_attentions + (attentions,)\n",
    "            else:\n",
    "                if len(layer_outputs) != 1:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 1, but it is {len(layer_outputs)}\")\n",
    "\n",
    "        repr = Pooling.mean_pooling(hidden_state, attention_mask)\n",
    "        repr, mask = align_tensor(repr, indptr, pad_tok=0.0)\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_state, all_hidden_states, all_attentions] if v is not None)\n",
    "            \n",
    "        output = UPMAEncoderOutput(\n",
    "            repr=repr, last_hidden_state=hidden_state, hidden_states=all_hidden_states, attentions=all_attentions,\n",
    "        )\n",
    "        return output, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a339b-cf46-4a0b-b5cc-4a3d866ff300",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4ef7e6ed-f08b-47bb-a3c9-3a2ef40a3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = Parameters.from_aug_meta_prefix_for_feature('data', 'cat2data', **batch)\n",
    "inputs = Parameters.from_data_aug_meta_prefix_for_encoder('cat2data', **kwargs)['cat2data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8c355a96-4e96-4540-ac86-f3941cbaf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": inputs[\"metadata_input_ids\"],\n",
    "    \"attention_mask\": inputs[\"metadata_attention_mask\"],\n",
    "    \"indptr\": inputs[\"metadata_data2ptr\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6567bb46-42b4-4ec2-b95d-7d0f2cf1ff20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UPMAEncoderMemory.from_pretrained(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "280dedb3-2cab-4cbb-bf69-190377249041",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "18ff613c-d517-4300-aacc-37ac93a78cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "o, mask = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9809e-1da3-496b-a4dd-65c673f5069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b1e9dbd-e726-4fb5-81ef-2e7a63254339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_scores', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_scores', 'cat2data_data2ptr', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'pcat2lbl_idx', 'pcat2lbl_lbl2ptr', 'cat2lbl_idx', 'cat2lbl_scores', 'cat2lbl_lbl2ptr', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_data2ptr', 'pcat2lbl_data2ptr'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a04d-8aad-44c2-a671-a888c19d1038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "99fb3eac-ce1d-4125-a6dc-52411a85e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Parameters:\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_data_aug_meta_prefix_for_encoder(prefix:str, **kwargs):\n",
    "        inputs = {}\n",
    "        args = [arg for arg in kwargs if prefix is not None and re.match(f'^{prefix}.*_(input_ids|attention_mask|data2ptr|idx|scores)$', arg)]\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            inputs.setdefault(meta, {})[f\"metadata_{param}\"] = kwargs[arg]\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_aug_meta_prefix_for_feature(feat:str, prefix:str, **kwargs):\n",
    "        keys = ['attention_mask', 'input_ids', 'idx', 'scores']        \n",
    "        inputs = {f'{prefix}_{k}': kwargs[f'{prefix}_{k}'] for k in keys if f'{prefix}_{k}' in kwargs}\n",
    "        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:\n",
    "            inputs.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_aug_meta_prefix_for_loss(feat:str, prefix:str, **kwargs):\n",
    "        keys = [f'{prefix}_idx', f'p{prefix}_idx']\n",
    "        args = {k: kwargs[k] for k in keys if k in kwargs}\n",
    "        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:\n",
    "            args.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})\n",
    "        if prefix is not None and f'p{prefix}_{feat}2ptr' in kwargs:\n",
    "            args.update({f'p{prefix}_data2ptr': kwargs[f'p{prefix}_{feat}2ptr']})\n",
    "\n",
    "        inputs = {}\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            inputs.setdefault(meta, {})[param] = args[arg]\n",
    "        return inputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b0ffb-a31c-456e-a1b4-fc99fdf5c1bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b848a619-2e1b-498c-8502-19370b71e69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat2data_attention_mask', 'cat2data_input_ids', 'cat2data_idx', 'cat2data_data2ptr'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = Parameters.from_aug_meta_prefix_for_feature('data', 'cat2data', **batch)\n",
    "kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c90c9a8-92f1-4e26-a309-93736130afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "upma_model_inputs = Parameters.from_data_aug_meta_prefix_for_encoder('cat2data', **kwargs)['cat2data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d463dd1c-e5a8-46f6-b35e-0a4fb502da6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata_attention_mask', 'metadata_input_ids', 'metadata_idx', 'metadata_data2ptr'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upma_model_inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42de0e-b03b-469c-af20-609dfa387f08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `UPMA Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45836e79-26b9-4315-a206-a0cfc292eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAModel(PreTrainedModel):\n",
    "    config: UPMAConfig\n",
    "    load_tf_weights = None\n",
    "    base_model_prefix = \"distilbert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.memory_modules = nn.ModuleList([get_memory_module(name).from_pretrained(config) for name in config.memory_module_names])\n",
    "        \n",
    "        self.n_layers = config.n_layers\n",
    "        self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
    "        self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                self.config.max_position_embeddings, self.config.dim, module.position_embeddings.weight\n",
    "            )\n",
    "        elif isinstance(module, UPMAEmbeddingMemory) and self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                self.config.num_input_metadata, self.config.dim, module.rank_embeddings.weight\n",
    "            )\n",
    "            \n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.position_embeddings\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        num_position_embeds_diff = new_num_position_embeddings - self.config.max_position_embeddings\n",
    "\n",
    "        # no resizing needs to be done if the length stays the same\n",
    "        if num_position_embeds_diff == 0:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Setting `config.max_position_embeddings={new_num_position_embeddings}`...\")\n",
    "        self.config.max_position_embeddings = new_num_position_embeddings\n",
    "\n",
    "        old_position_embeddings_weight = self.embeddings.position_embeddings.weight.clone()\n",
    "\n",
    "        self.embeddings.position_embeddings = nn.Embedding(self.config.max_position_embeddings, self.config.dim)\n",
    "\n",
    "        if self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=self.config.max_position_embeddings, dim=self.config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if num_position_embeds_diff > 0:\n",
    "                    self.embeddings.position_embeddings.weight[:-num_position_embeds_diff] = nn.Parameter(\n",
    "                        old_position_embeddings_weight\n",
    "                    )\n",
    "                else:\n",
    "                    self.embeddings.position_embeddings.weight = nn.Parameter(\n",
    "                        old_position_embeddings_weight[:num_position_embeds_diff]\n",
    "                    )\n",
    "        # move position_embeddings to correct device\n",
    "        self.embeddings.position_embeddings.to(self.device)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: nn.Embedding):\n",
    "        self.embeddings.word_embeddings = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: dict[int, list[list[int]]]):\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.transformer.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        \n",
    "        metadata_idx: Optional[torch.Tensor] = None,\n",
    "        metadata_input_ids: Optional[torch.Tensor] = None,\n",
    "        metadata_attention_mask: Optional[torch.Tensor] = None,\n",
    "        metadata_scores: Optional[torch.Tensor] = None,\n",
    "        metadata_data2ptr: Optional[torch.Tensor] = None,\n",
    "        metadata_embeds: Optional[torch.Tensor] = None,\n",
    "        inject_memory: Optional[bool] = True,\n",
    "        \n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]:\n",
    "         \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        head_mask_is_none = head_mask is None\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
    "        \n",
    "        if inject_memory:\n",
    "            memory_embeddings, memory_mask, m_mask = [], [], attention_mask\n",
    "            \n",
    "            for module in self.memory_modules:\n",
    "                m_embeddings, _m_mask = module(\n",
    "                    input_idx = metadata_idx,\n",
    "                    input_embeds = metadata_embeds,\n",
    "                    scores = metadata_scores,\n",
    "                    indptr = metadata_data2ptr,\n",
    "                    input_ids = metadata_input_ids,\n",
    "                    attention_mask = metadata_attention_mask,\n",
    "                ) # (bs, num_input_metadata, dim), (bs, num_input_metadata)\n",
    "                m_mask = torch.cat([m_mask, _m_mask], dim=1)\n",
    "                \n",
    "                memory_embeddings.append(m_embeddings.repr)\n",
    "                memory_mask.append(m_mask)\n",
    "            \n",
    "        def _prepare_attention_mask(attention_mask, input_shape):\n",
    "            if self._use_flash_attention_2:\n",
    "                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "            else:\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
    "    \n",
    "                if self._use_sdpa and head_mask_is_none and not output_attentions:\n",
    "                    attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
    "                        attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n",
    "                    )\n",
    "            return attention_mask\n",
    "\n",
    "        attention_mask = _prepare_attention_mask(attention_mask, input_shape)\n",
    "                \n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_state = embeddings\n",
    "        memory_injection_ctr, n_memory_injection_layer = 0, len(self.config.memory_injection_layers)\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "            if (\n",
    "                inject_memory and memory_injection_ctr < n_memory_injection_layer and \n",
    "                i+1 == self.config.memory_injection_layers[memory_injection_ctr]\n",
    "            ):\n",
    "                m_embeddings, m_mask = memory_embeddings[memory_injection_ctr], memory_mask[memory_injection_ctr]\n",
    "                hidden_state = torch.cat([hidden_state, m_embeddings], dim=1)\n",
    "                attention_mask = _prepare_attention_mask(m_mask, m_mask.size())\n",
    "                memory_injection_ctr += 1\n",
    "                \n",
    "            layer_outputs = layer_module(\n",
    "                hidden_state,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_state = layer_outputs[-1]\n",
    "\n",
    "            if output_attentions:\n",
    "                if len(layer_outputs) != 2:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 2, but it is {len(layer_outputs)}\")\n",
    "\n",
    "                attentions = layer_outputs[0]\n",
    "                all_attentions = all_attentions + (attentions,)\n",
    "            else:\n",
    "                if len(layer_outputs) != 1:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 1, but it is {len(layer_outputs)}\")\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_state, all_hidden_states, all_attentions] if v is not None)\n",
    "            \n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_state, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f00484-68e2-4172-995b-b97e0986b114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46eb5481-59d1-403a-aed6-cccad5201969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UPMAConfig {\n",
       "  \"__stored_args__\": {\n",
       "    \"apply_softmax\": true,\n",
       "    \"calib_apply_softmax\": false,\n",
       "    \"calib_loss_weight\": 0.1,\n",
       "    \"calib_margin\": 0.3,\n",
       "    \"calib_num_negatives\": 10,\n",
       "    \"calib_tau\": 0.1,\n",
       "    \"data_aug_meta_prefix\": \"cat2data\",\n",
       "    \"data_inject_memory\": true,\n",
       "    \"data_repr_pooling\": false,\n",
       "    \"initialize_memory_embeddings_from_injection_layer_mean\": true,\n",
       "    \"lbl2data_aug_meta_prefix\": \"cat2lbl\",\n",
       "    \"lbl2data_inject_memory\": false,\n",
       "    \"margin\": 0.3,\n",
       "    \"memory_injection_layers\": [\n",
       "      3,\n",
       "      1\n",
       "    ],\n",
       "    \"memory_module_names\": [\n",
       "      \"embeddings\",\n",
       "      \"encoder\"\n",
       "    ],\n",
       "    \"metadata_dropout\": 0.1,\n",
       "    \"metadata_embedding_file\": null,\n",
       "    \"n_memory_layers\": 3,\n",
       "    \"num_input_metadata\": 3,\n",
       "    \"num_negatives\": 5,\n",
       "    \"num_total_metadata\": 656086,\n",
       "    \"tau\": 0.1,\n",
       "    \"use_calib_loss\": true,\n",
       "    \"use_encoder_parallel\": false\n",
       "  },\n",
       "  \"activation\": \"gelu\",\n",
       "  \"apply_softmax\": true,\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"calib_apply_softmax\": false,\n",
       "  \"calib_loss_weight\": 0.1,\n",
       "  \"calib_margin\": 0.3,\n",
       "  \"calib_num_negatives\": 10,\n",
       "  \"calib_tau\": 0.1,\n",
       "  \"data_aug_meta_prefix\": \"cat2data\",\n",
       "  \"data_inject_memory\": true,\n",
       "  \"data_repr_pooling\": false,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initialize_memory_embeddings_from_injection_layer_mean\": true,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"lbl2data_aug_meta_prefix\": \"cat2lbl\",\n",
       "  \"lbl2data_inject_memory\": false,\n",
       "  \"margin\": 0.3,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"memory_injection_layers\": [\n",
       "    1,\n",
       "    3\n",
       "  ],\n",
       "  \"memory_module_names\": [\n",
       "    \"encoder\",\n",
       "    \"embeddings\"\n",
       "  ],\n",
       "  \"metadata_dropout\": 0.1,\n",
       "  \"metadata_embedding_file\": null,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"n_memory_layers\": 3,\n",
       "  \"num_input_metadata\": 3,\n",
       "  \"num_negatives\": 5,\n",
       "  \"num_total_metadata\": 656086,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tau\": 0.1,\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_calib_loss\": true,\n",
       "  \"use_encoder_parallel\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c245a428-5d38-4fd5-aa41-a3141087ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPMAModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04928597-5ebe-457c-b6de-da6b9f8347d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata_attention_mask', 'metadata_input_ids', 'metadata_idx', 'metadata_data2ptr'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upma_model_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "620d6ea8-d23c-41ca-8488-ad911e00a64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = model(input_ids=batch['data_input_ids'], attention_mask=batch['data_attention_mask'], \n",
    "          inject_memory=True, **upma_model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5a5abb3-ecac-42f5-8c0a-c6dfef716144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 38, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f448f2-a397-4f1a-8d8e-065409cde6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d207c457-a914-4b11-b462-b561692ef66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    o = model(input_ids=batch['data_input_ids'], attention_mask=batch['data_attention_mask'], \n",
    "              inject_memory=True, **upma_model_inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f681bb-15f8-463c-8b9f-5fe302303144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923b3a20-713f-4e95-bfd7-b56a61723a1e",
   "metadata": {},
   "source": [
    "## `UPMAEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3f23c229-1bd6-453a-a754-da743661f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAEncoder(UPMAModel):\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        config:PretrainedConfig,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "    ):\n",
    "        src_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        targ_model = cls(config)\n",
    "\n",
    "        targ_model.init_weights()\n",
    "        targ_model.eval()\n",
    "        \n",
    "        src_sd, targ_sd = src_model.state_dict(), targ_model.state_dict()\n",
    "        src_keys, targ_keys = set(src_sd.keys()), set(targ_sd.keys())\n",
    "        \n",
    "        for k in src_keys.intersection(targ_keys):\n",
    "            assert targ_sd[k].shape == src_sd[k].shape, (\n",
    "                f\"Shape mismatch at key '{k}'. \"\n",
    "                f\"Expected {targ_sd[k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "            )\n",
    "            targ_sd[k].copy_(src_sd[k])\n",
    "\n",
    "        diff_keys = targ_keys.difference(src_keys)\n",
    "        transformer_keys = [k for k in src_keys if k.startswith(\"transformer\")]\n",
    "        for k in transformer_keys:\n",
    "            targ_k = k.split('.', maxsplit=1)[1]\n",
    "            \n",
    "            assert targ_k in targ_sd, (\n",
    "                f\"Unexpected key '{targ_k}' encountered, not found in target state_dict.\"\n",
    "            )\n",
    "            \n",
    "            assert targ_sd[targ_k].shape == src_sd[k].shape, (\n",
    "                f\"Shape mismatch at key '{k}'. \"\n",
    "                f\"Expected {targ_sd[targ_k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "            )\n",
    "            \n",
    "            targ_sd[targ_k].copy_(src_sd[k])\n",
    "            diff_keys.remove(targ_k)\n",
    "\n",
    "        for module, injection_layer in zip(targ_model.memory_modules, config.memory_injection_layers):\n",
    "            if config.initialize_memory_embeddings_from_injection_layer_mean and isinstance(module, UPMAEmbeddingMemory):\n",
    "                meta_embeds = targ_model.initialize_memory_embeddings_from_injection_layer_mean(\n",
    "                    memory_injection_layer=injection_layer-1,\n",
    "                    meta_dset=meta_dset,\n",
    "                    save_file=config.metadata_embedding_file,\n",
    "                    batch_size=batch_size,\n",
    "                    use_encoder_parallel=config.use_encoder_parallel,\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    module.set_metadata_embeddings(meta_embeds)\n",
    "                    \n",
    "        return targ_model\n",
    "\n",
    "    def initialize_memory_embeddings_from_injection_layer_mean(\n",
    "        self,\n",
    "        memory_injection_layer:int,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        save_file:Optional[str] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "        use_encoder_parallel:Optional[bool] = True\n",
    "    ):\n",
    "        if save_file is not None and os.path.exists(save_file):\n",
    "            meta_embeds = torch.load(save_file)\n",
    "        else:\n",
    "            if meta_dset is None: \n",
    "                raise ValueError(\n",
    "                    f\"Invalid argument: 'meta_dset' cannot be None. \"\n",
    "                    f\"Please pass a valid dataset.\"\n",
    "                )\n",
    "                \n",
    "            meta_embeds, device = [], \"cuda:0\" if use_encoder_parallel else next(self.parameters()).device\n",
    "            meta_dl = DataLoader(meta_dset, batch_size=batch_size, collate_fn=identity_collate_fn)\n",
    "    \n",
    "            model = XCDataParallel(module=self.to(device)) if use_encoder_parallel else self\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(meta_dl):\n",
    "                    for k, v in batch.items(): \n",
    "                        if isinstance(v, torch.Tensor): batch[k] = v.to(device)\n",
    "                    output = model(**batch, data_inject_memory=False, data_output_hidden_states=True)\n",
    "                    embeds = output.hidden_states[memory_injection_layer]\n",
    "                    embeds = Pooling.mean_pooling(embeds, batch['data_attention_mask']).to(\"cpu\")\n",
    "                    meta_embeds.append(embeds)\n",
    "                # adding pad metadata embeddings\n",
    "                embeds = torch.zeros(1, embeds.shape[1], dtype=embeds.dtype, device=embeds.device)\n",
    "                meta_embeds.append(embeds)\n",
    "                meta_embeds = torch.cat(meta_embeds, dim=0)\n",
    "                \n",
    "            del output, embeds, batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            self.to(\"cpu\")\n",
    "            \n",
    "            if save_file is not None:\n",
    "                os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "                torch.save(meta_embeds, save_file)\n",
    "                \n",
    "        return meta_embeds\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        data_input_ids: torch.Tensor, \n",
    "        data_attention_mask: torch.Tensor,\n",
    "        data_aug_meta_prefix: Optional[str]=None,\n",
    "        data_inject_memory: Optional[bool]=True,\n",
    "        data_output_attentions: Optional[bool] = None,\n",
    "        data_output_hidden_states: Optional[bool] = None,\n",
    "        data_return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        meta_kwargs = Parameters.from_data_aug_meta_prefix_for_encoder(data_aug_meta_prefix, **kwargs)\n",
    "        meta_kwargs = meta_kwargs.get(data_aug_meta_prefix, dict())\n",
    "        \n",
    "        output = super().forward(\n",
    "            input_ids=data_input_ids, \n",
    "            attention_mask=data_attention_mask,\n",
    "            inject_memory=data_inject_memory,\n",
    "            output_attentions=data_output_attentions,\n",
    "            output_hidden_states=data_output_hidden_states,\n",
    "            return_dict=data_return_dict,\n",
    "            **meta_kwargs\n",
    "        )\n",
    "        \n",
    "        if self.config.data_repr_pooling:\n",
    "            embeds = output[0][:, :data_attention_mask.shape[1], :]\n",
    "            attention_mask = data_attention_mask\n",
    "        else:\n",
    "            embeds = output[0]\n",
    "            if 'metadata_data2ptr' in meta_kwargs and data_inject_memory:\n",
    "                memory_mask = alignment_mask(meta_kwargs['metadata_data2ptr'])\n",
    "                attention_mask = torch.cat([data_attention_mask] + [memory_mask for _ in range(len(self.config.memory_injection_layers))], dim=1)\n",
    "            else:\n",
    "                attention_mask = data_attention_mask\n",
    "                \n",
    "        assert embeds.shape[:2] == attention_mask.shape, (\n",
    "            f\"Shape mismatch: embeds.shape[:2] = {embeds.shape[:2]} \"\n",
    "            f\"but attention_mask.shape = {attention_mask.shape}.\"\n",
    "        )\n",
    "        \n",
    "        data_repr = Pooling.mean_pooling(embeds, attention_mask)\n",
    "        data_repr = F.normalize(data_repr, dim=1) if config.data_normalize else data_repr\n",
    "        return UPMAEncoderOutput(repr=data_repr, **output)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4e74f-9080-4b21-a44d-dc2b2cf62177",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e4f06bb-bbbb-4b0a-86d4-4dd81309a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UPMAConfig(\n",
    "    memory_module_names = [\"embeddings\", \"encoder\"],\n",
    "    memory_injection_layers = [3, 1],\n",
    "    \n",
    "    num_total_metadata = block.train.dset.meta['cat_meta'].n_meta,\n",
    "    num_input_metadata = 3,\n",
    "    metadata_dropout = 0.1,\n",
    "    \n",
    "    n_memory_layers = 3,\n",
    "\n",
    "    data_aug_meta_prefix=\"cat2data\",\n",
    "    lbl2data_aug_meta_prefix=\"cat2lbl\",\n",
    "    neg2data_aug_meta_prefix=\"cat2neg\",\n",
    "\n",
    "    data_inject_memory=True,\n",
    "    lbl2data_inject_memory=False,\n",
    "    neg2data_inject_memory=False,\n",
    "    \n",
    "    data_repr_pooling=True,\n",
    "    data_normalize=True,\n",
    "\n",
    "    margin=0.3,\n",
    "    num_negatives=5,\n",
    "    tau=0.1,\n",
    "    apply_softmax=True,\n",
    "\n",
    "    calib_margin=0.3,\n",
    "    calib_num_negatives=10,\n",
    "    calib_tau=0.1,\n",
    "    calib_apply_softmax=False,\n",
    "    \n",
    "    calib_loss_weight=0.1,\n",
    "    use_calib_loss=True,\n",
    "    \n",
    "    use_encoder_parallel=False,\n",
    "\n",
    "    initialize_memory_embeddings_from_injection_layer_mean=False,\n",
    "    metadata_embedding_file=None,\n",
    "    loss_function=\"triplet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59564a-fe30-48af-80fb-d9169dd84f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "baecd2c8-1871-4caa-8fd3-8055fe2aeeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_dset = block.train.dset.meta_dset('cat_meta')\n",
    "model = UPMAEncoder.from_pretrained(config, meta_dset=meta_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424eae5-c5ea-4469-af70-8f27bffd7f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c87bebf0-9fe7-48d9-a2d8-26832cb20243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('data', config.data_aug_meta_prefix, **batch)\n",
    "\n",
    "o = model(data_input_ids=batch[\"data_input_ids\"], data_attention_mask=batch[\"data_attention_mask\"], \n",
    "          data_aug_meta_prefix=config.data_aug_meta_prefix, data_enrich=config.data_inject_memory, **data_meta_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afeaba-9f1f-4729-be19-055505b9b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9cb0275f-e5c2-4100-9232-bedd294f3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    o = model(data_input_ids=batch[\"data_input_ids\"], data_attention_mask=batch[\"data_attention_mask\"], \n",
    "          data_aug_meta_prefix=config.data_aug_meta_prefix, data_enrich=config.data_inject_memory, **data_meta_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d16ea1-e1fe-421b-a596-f54cf1b5fb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e0de6c7-5e05-4354-ae76-2fac1241aa70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `UPA000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7d28224-725c-4349-8b7f-aa2df23b1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPA000(PreTrainedModel):\n",
    "    use_generation, use_representation = False, True\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config: UPMAConfig,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.encoder = UPMAEncoder.from_pretrained(config, meta_dset=meta_dset, batch_size=batch_size)\n",
    "        \n",
    "        loss_kwargs = {\n",
    "            'margin': config.margin, 'n_negatives': config.num_negatives, 'tau': config.tau, \n",
    "            'apply_softmax': config.apply_softmax, 'reduce': config.reduction,\n",
    "        }\n",
    "        self.rep_loss_fn = get_loss_function(config.loss_function)(**loss_kwargs)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        config: PretrainedConfig,\n",
    "        meta_dset: Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size: Optional[int] = 100,\n",
    "    ):\n",
    "        return cls(config, meta_dset=meta_dset, batch_size=batch_size)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_scores:Optional[torch.Tensor]=None,\n",
    "\n",
    "        neg2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        neg2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        neg2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        neg2data_idx:Optional[torch.Tensor]=None,\n",
    "        neg2data_scores:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        encoder = XCDataParallel(module=self.encoder) if self.config.use_encoder_parallel else self.encoder\n",
    "        \n",
    "        data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('data', self.config.data_aug_meta_prefix, **kwargs)\n",
    "        data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
    "                         data_aug_meta_prefix=self.config.data_aug_meta_prefix, data_inject_memory=self.config.data_inject_memory, \n",
    "                         data_output_hidden_states=True, **data_meta_kwargs)\n",
    "        \n",
    "        loss = None; lbl2data_o = neg2data_o = UPMAEncoderOutput()\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('lbl2data', self.config.lbl2data_aug_meta_prefix, **kwargs)\n",
    "            lbl2data_o = encoder(data_input_ids=lbl2data_input_ids, data_attention_mask=lbl2data_attention_mask, \n",
    "                                 data_aug_meta_prefix=self.config.lbl2data_aug_meta_prefix, data_inject_memory=self.config.lbl2data_inject_memory, \n",
    "                                 data_output_hidden_states=True, **lbl2data_meta_kwargs)\n",
    "\n",
    "            if neg2data_input_ids is not None:\n",
    "                neg2data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('neg2data', self.config.neg2data_aug_meta_prefix, **kwargs)\n",
    "                neg2data_o = encoder(data_input_ids=neg2data_input_ids, data_attention_mask=neg2data_attention_mask, \n",
    "                                     data_aug_meta_prefix=self.config.neg2data_aug_meta_prefix, data_inject_memory=self.config.neg2data_inject_memory, \n",
    "                                     data_output_hidden_states=True, **neg2data_meta_kwargs)\n",
    "\n",
    "            loss = self.rep_loss_fn(data_o.repr, pos_targ=lbl2data_o.repr, n_pos=lbl2data_data2ptr, \n",
    "                                    pos_idx=lbl2data_idx, pos_scores=lbl2data_scores,\n",
    "                                    n_ppos=plbl2data_data2ptr, ppos_idx=plbl2data_idx,\n",
    "                                    neg_targ=neg2data_o.repr, n_neg=neg2data_data2ptr, \n",
    "                                    neg_idx=neg2data_idx, neg_scores=neg2data_scores, **kwargs)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_o.repr, lbl2data_o.repr, neg2data_o.repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return UPMAModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_o.repr,\n",
    "            lbl2data_repr=lbl2data_o.repr,\n",
    "            neg2data_repr=neg2data_o.repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc769de-9935-4078-8233-75d436021d46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c5041df-fd0f-46f1-9acf-f94a0026e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPA000.from_pretrained(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3791b342-9a10-4594-a1bc-1604a8a97415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_scores', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_scores', 'cat2data_data2ptr', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'pcat2lbl_idx', 'pcat2lbl_lbl2ptr', 'cat2lbl_idx', 'cat2lbl_scores', 'cat2lbl_lbl2ptr', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_data2ptr', 'pcat2lbl_data2ptr'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e660d8a-100c-47aa-9b32-2862417c3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7bf00a8-02d4-4e2c-b5bd-2736dc130ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b7b5f-48b7-461c-88f5-180a1c3722a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1032907-0e9a-484a-b7bf-473cb3493517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    o = model(**batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e2bae-5ba2-4909-8f56-719e97ac4536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba361b2c-a4c1-46f2-8e08-77312530177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1133,  0.4918, -0.3105,  ..., -0.2339, -0.1606, -0.0092],\n",
       "         [-0.1645,  0.1927, -0.7017,  ..., -0.5304,  0.1522,  0.0663],\n",
       "         [-0.2392, -0.5515, -0.3255,  ...,  0.1439, -0.0111, -0.0919]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " torch.Size([3, 768]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr, o.data_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b80a048f-6cfb-4d64-a38e-008894ad36dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0146,  0.5108, -0.4399,  ..., -0.3527, -0.1128, -0.0712],\n",
       "         [ 0.1009,  0.3114, -0.2503,  ..., -0.2568, -0.0038,  0.0075],\n",
       "         [-0.1228,  0.1947, -0.6093,  ..., -0.4707,  0.0389, -0.0159],\n",
       "         [-0.1751,  0.1756, -0.6148,  ..., -0.5590,  0.1895, -0.0261],\n",
       "         [-0.3955, -0.2311, -0.4383,  ..., -0.0789, -0.0377,  0.2048],\n",
       "         [-0.3194, -0.6375, -0.2985,  ...,  0.1485, -0.0762,  0.2454]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " torch.Size([6, 768]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.lbl2data_repr, o.lbl2data_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "720123a4-df03-42a8-abac-2c3b2eb816d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.neg2data_repr is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aef34-6743-4623-a88c-bee05c778f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253d88b-8480-4d8d-8c41-d8ce72037aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cc4f7-5822-4a55-bdde-bded32aa53c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac08258-6a3a-460a-b8cf-9c1c5e07f3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
