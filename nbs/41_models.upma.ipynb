{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483ee0f1-49bb-4794-9612-9e8182da0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.upma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563da750-7f38-4597-948c-236a7ca27667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464ed0be-1ad4-4722-96e7-791081a9abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a38dda-cc60-48e0-8528-a896969d33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, torch.nn as nn, re, os\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, Union, Tuple, Any, Dict, Sequence\n",
    "\n",
    "from transformers import DistilBertConfig, DistilBertModel, PretrainedConfig, PreTrainedModel\n",
    "from transformers.modeling_outputs import BaseModelOutput, ModelOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import Embeddings, TransformerBlock, create_sinusoidal_embeddings\n",
    "from transformers.activations import get_activation\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.losses import *\n",
    "from xcai.data import MainXCDataset\n",
    "from xcai.sdata import SMainXCDataset, identity_collate_fn\n",
    "from xcai.learner import XCDataParallel\n",
    "from xcai.models.modeling_utils import Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c0fa-2829-42dc-bb53-1fed8107a973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc73605-f2b1-4e69-9780-c1932fc9f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7558da2e-090d-4b47-81bc-d43c00854412",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/suchith720/Projects/data'\n",
    "config_file = 'wikiseealsotitles'\n",
    "config_key = 'data_meta'\n",
    "\n",
    "mname = 'sentence-transformers/msmarco-distilbert-base-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba6c85c-111e-4496-b9c0-66de3ac303b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = f'{data_dir}/processed/mogicX/'\n",
    "pkl_file = f'{pkl_dir}/wikiseealsotitles_data-meta_distilbert-base-uncased_sxc.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072f908a-9942-4ccb-af2d-dfb3b624bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = build_block(pkl_file, config_file, True, config_key, data_dir=data_dir, \n",
    "                    n_sdata_meta_samples=3, do_build=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d1bba-3b4e-4f99-ab30-d3ac855edd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61f91ea-52b0-428e-aaea-90352cbae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.dset.__getitems__([10, 30])\n",
    "batch = block.train.dset.__getitems__([10, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba99d74-faae-4d3e-bac2-0c088267a3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_data2ptr', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'pcat2lbl_idx', 'pcat2lbl_lbl2ptr', 'cat2lbl_idx', 'cat2lbl_lbl2ptr', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_data2ptr', 'pcat2lbl_data2ptr'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65120292-4da4-4316-81b0-d49942e92661",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "61fc143b-bd04-4d10-a838-728c10ed9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAConfig(DistilBertConfig):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_total_metadata: Optional[int] = None,\n",
    "        num_input_metadata: Optional[int] = 3,\n",
    "        metadata_dropout: Optional[float] = 0.1,\n",
    "        memory_injection_layer: Optional[int] = None,\n",
    "        memory_module_name: Optional[str] = \"embeddings\",\n",
    "        \n",
    "        data_aug_meta_prefix: Optional[str] = None, \n",
    "        lbl2data_aug_meta_prefix: Optional[str] = None,\n",
    "\n",
    "        data_inject_memory: Optional[bool] = True,\n",
    "        lbl2data_inject_memory: Optional[bool] = True,\n",
    "        data_repr_pooling: Optional[bool] = True,\n",
    "\n",
    "        margin: Optional[float] = 0.3,\n",
    "        num_negatives: Optional[int] = 10,\n",
    "        tau: Optional[float] = 0.1,\n",
    "        apply_softmax: Optional[bool] = True,\n",
    "\n",
    "        calib_margin: Optional[float] = 0.05,\n",
    "        calib_num_negatives: Optional[int] = 10,\n",
    "        calib_tau: Optional[float] = 0.1,\n",
    "        calib_apply_softmax: Optional[bool] = False,\n",
    "        \n",
    "        calib_loss_weight: Optional[float] = 0.1,\n",
    "        use_calib_loss: Optional[bool] = False,\n",
    "\n",
    "        use_encoder_parallel: Optional[bool] = False,\n",
    "        \n",
    "        initialize_memory_embeddings_from_injection_layer_mean: Optional[bool] = True,\n",
    "        metadata_embedding_file: Optional[str] = None,\n",
    "        \n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr('num_total_metadata,num_input_metadata,metadata_dropout,memory_module_name')\n",
    "        store_attr('data_aug_meta_prefix,lbl2data_aug_meta_prefix')\n",
    "        store_attr('data_inject_memory,lbl2data_inject_memory,data_repr_pooling')\n",
    "        store_attr('margin,num_negatives,tau,apply_softmax')\n",
    "        store_attr('calib_margin,calib_num_negatives,calib_tau,calib_apply_softmax')\n",
    "        store_attr('calib_loss_weight,use_calib_loss,use_encoder_parallel')\n",
    "        store_attr('initialize_memory_embeddings_from_injection_layer_mean,metadata_embedding_file')\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if memory_injection_layer is None: self.memory_injection_layer = self.n_layers\n",
    "            \n",
    "        assert self.memory_injection_layer <= self.n_layers, (\n",
    "            f\"Invalid memory injection layer: {self.memory_injection_layer}. \"\n",
    "            f\"it must be less than the total number of layers ({self.n_layers}).\"\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d895775-9046-4ba6-b800-8e7ec7fc7a8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "132017f5-4bf1-408a-8206-c0648255226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UPMAConfig(\n",
    "    num_total_metadata = block.train.dset.meta['cat_meta'].n_meta,\n",
    "    num_input_metadata = 3,\n",
    "    metadata_dropout = 0.1,\n",
    "    memory_injection_layer = None,\n",
    "    memory_module_name = \"embeddings\",\n",
    "\n",
    "    data_aug_meta_prefix=\"cat2data\",\n",
    "    lbl2data_aug_meta_prefix=\"cat2lbl\",\n",
    "\n",
    "    data_inject_memory=True,\n",
    "    lbl2data_inject_memory=False,\n",
    "    data_repr_pooling=True,\n",
    "\n",
    "    margin=0.3,\n",
    "    num_negatives=5,\n",
    "    tau=0.1,\n",
    "    apply_softmax=True,\n",
    "\n",
    "    calib_margin=0.3,\n",
    "    calib_num_negatives=10,\n",
    "    calib_tau=0.1,\n",
    "    calib_apply_softmax=False,\n",
    "    \n",
    "    calib_loss_weight=0.1,\n",
    "    use_calib_loss=True,\n",
    "    \n",
    "    use_encoder_parallel=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a030ffeb-9c3e-4a96-9223-2f2c79dca56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.data_inject_memory, config.lbl2data_inject_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19777e6-d47c-4903-a417-7031c5fe29d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d83bebd2-6fb8-4277-870c-304cf440af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_memory_module(name: str):\n",
    "    if name == \"embeddings\": return UPMAEmbeddingMemory\n",
    "    else: raise ValueError(f\"Invalid memory module: {name}\")\n",
    "\n",
    "def align_tensor(tensor:torch.Tensor, indptr:torch.Tensor, pad_tok:Optional[Union[int,float]]=0):\n",
    "    tensor_shape = tensor.shape\n",
    "    r, c = len(indptr), indptr.max()\n",
    "\n",
    "    row_idx = torch.repeat_interleave(torch.arange(r, device=tensor.device), indptr)\n",
    "    indptr = torch.cat([indptr.new_tensor([0]), indptr.cumsum(dim=0)[:-1]], dim=0)\n",
    "    within_idx = torch.arange(tensor_shape[0], device=indptr.device) - indptr[row_idx]\n",
    "\n",
    "    output = torch.full((r, c, *tensor_shape[1:]), pad_tok, device=tensor.device, dtype=tensor.dtype)\n",
    "    mask = torch.zeros((r, c), device=tensor.device)\n",
    "\n",
    "    output[row_idx, within_idx] = tensor\n",
    "    mask[row_idx, within_idx] = 1.0\n",
    "\n",
    "    return output, mask\n",
    "\n",
    "def alignment_mask(indptr:torch.Tensor):\n",
    "    n, r, c = indptr.sum(), len(indptr), indptr.max()\n",
    "\n",
    "    row_idx = torch.repeat_interleave(torch.arange(r, device=indptr.device), indptr)\n",
    "    indptr = torch.cat([indptr.new_tensor([0]), indptr.cumsum(dim=0)[:-1]], dim=0)\n",
    "    within_idx = torch.arange(n, device=indptr.device) - indptr[row_idx]\n",
    "\n",
    "    mask = torch.zeros((r, c), device=indptr.device, dtype=torch.int64)\n",
    "    mask[row_idx, within_idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792cdc7-ac6f-4947-bbef-1d05c24ea23f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24b1d4ae-c88a-4a32-a243-837251370eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(100, size=(5, 3))\n",
    "indptr = torch.tensor([1, 1, 0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "153e6ff4-59c8-4bbb-a9a0-59a22234035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = align_tensor(t, indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8815ddc4-4bef-4485-b382-9b6223ebb954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6, 84, 31],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[16, 12, 85],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       " \n",
       "         [[95, 29, 80],\n",
       "          [82, 45, 10],\n",
       "          [81, 43, 63]]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 1., 1.]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a6af7-70b6-408c-ab89-40293db3ecea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "488154eb-8e12-41e0-8987-b98b0a44d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UPMAEncoderOutput(BaseModelOutput):\n",
    "    repr: Optional[torch.FloatTensor] = None\n",
    "    \n",
    "@dataclass\n",
    "class UPMAModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d2580-d5b1-4e0a-b297-bf54218a534f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3cb9f-2449-4ba3-90c4-26914e26771a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `FFN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8bab1-ce05-4092-9868-6279819f1e9d",
   "metadata": {},
   "source": [
    "* $\\hat{x} = dropout(W_2 * max(0, W_1 * x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352e0cbc-95ff-4d17-985e-f38305c0f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FFN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config:PretrainedConfig,\n",
    "        input_dim:int,\n",
    "        hidden_dim:int,\n",
    "        output_dim:int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.lin1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        self.activation = get_activation(config.activation)\n",
    "        \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
    "\n",
    "    def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.lin1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937df2a6-0c22-417d-adcc-f0a97a65192a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `Embedding Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c45035-daf4-4d72-800c-97389048673f",
   "metadata": {},
   "source": [
    "* $\\hat{\\mathcal{A}} = \\{a_1, a_2, a_3, \\dots a_n\\}$ be relevant metadata predicted by the linker.\n",
    "\n",
    "* $\\hat{\\mathcal{S}} = \\{s_1, s_2, s_3, \\dots s_n\\}$ be scores of the predicted metadata.\n",
    "\n",
    "* $\\hat{\\mathcal{R}} = \\{1, 2, 3, \\dots n\\}$ be rank for the predicted metadata.\n",
    "\n",
    "* $\\mathcal{K} \\in R^{M \\times D}$ be the $M$ memory items for each metadata.\n",
    "\n",
    "* $\\mathcal{P} \\in R^{N \\times D}$ be the $N$ positional embeddings.\n",
    "\n",
    "* Rank and score aware metadata representation: $x_m = \\mathcal{K}(\\hat{\\mathcal{A}}) + MLP(\\hat{\\mathcal{S}}) + \\mathcal{P}(\\hat{\\mathcal{R}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aec34b2-9d4c-4fc2-9bfd-a8644e780b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAEmbeddingMemory(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config: PretrainedConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.metadata_embeddings = nn.Embedding(config.num_total_metadata+1, config.dim, padding_idx=config.num_total_metadata)\n",
    "        self.rank_embeddings = nn.Embedding(config.num_input_metadata, config.dim)\n",
    "        \n",
    "        self.score_ffn = FFN(config, input_dim=1, hidden_dim=config.hidden_dim, output_dim=config.dim)\n",
    "        self.out_ffn = FFN(config, input_dim=config.dim, hidden_dim=config.hidden_dim, output_dim=config.dim)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.metadata_dropout)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.num_input_metadata).expand((1, -1)), persistent=False\n",
    "        )\n",
    "        \n",
    "    def get_metadata_embeddings(self) -> torch.Tensor:\n",
    "        return self.metadata_embeddings.weight\n",
    "\n",
    "    def set_metadata_embeddings(self, new_embeddings: torch.Tensor):\n",
    "        self.metadata_embeddings.weight.copy_(new_embeddings)\n",
    "\n",
    "    def get_rank_embeddings(self) -> torch.Tensor:\n",
    "        return self.rank_embeddings.weight\n",
    "\n",
    "    def set_rank_embeddings(self, new_embeddings: torch.Tensor):\n",
    "        self.rank_embeddings.weight.copy_(new_embeddings)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_idx: torch.Tensor,\n",
    "        embeds: Optional[torch.Tensor] = None,\n",
    "        scores: Optional[torch.Tensor] = None,\n",
    "        indptr: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        # `input_idx`: (total_input_metadata)\n",
    "        # `scores`: (total_input_metadata)\n",
    "        \n",
    "        if input_idx is not None:\n",
    "            input_idx, mask = align_tensor(input_idx, indptr, pad_tok=self.config.num_total_metadata) # (bs, num_input_metadata)\n",
    "            embeds = self.metadata_embeddings(input_idx) # (bs, num_input_metadata, dim)\n",
    "        else:\n",
    "            assert embeds is not None, \"Invalid input: both `input_idx` and `embeds` cannot be None.\" \n",
    "            embeds, mask = align_tensor(embeds, indptr)\n",
    "            \n",
    "        if embeds.size(1) != self.config.num_input_metadata:\n",
    "            raise ValueError(\n",
    "                f\"Invalid input: expected {self.config.num_input_metadata} metadata items, \"\n",
    "                f\"but got {embeds.size(1)}.\"\n",
    "            )\n",
    "\n",
    "        if scores is not None:\n",
    "            scores, mask = align_tensor(scores, indptr) # (bs, num_input_metadata)\n",
    "            \n",
    "        if position_ids is None:\n",
    "            position_ids = (\n",
    "                self.position_ids[:, :self.config.num_input_metadata]\n",
    "                if scores is None else \n",
    "                torch.argsort(scores, dim=1, descending=True)\n",
    "            )\n",
    "            \n",
    "        rank_embeddings = self.rank_embeddings(position_ids) # (bs, num_input_metadata, dim) or (1, num_input_metadata, dim)\n",
    "\n",
    "        embeddings = embeds + rank_embeddings\n",
    "\n",
    "        if scores is not None:\n",
    "            score_embeddings = self.score_ffn(scores.unsqueeze(2)) # (bs, num_input_metadata, dim)\n",
    "            embeddings = embeddings + score_embeddings\n",
    "            \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return self.out_ffn(embeddings), mask # (bs, num_input_metadata, dim), (bs, num_input_metadata)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a209d3-26be-4600-8026-3de2d8a04383",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b997cd0-4ad4-4752-8cb3-c916f81e4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPMAEmbeddingMemory(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c83b6c-a17c-4c66-b2a6-fdeb50f77619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9530e422-d41d-4baf-8362-336042293247",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'input_idx': torch.randint(config.num_total_metadata, size=(5,)),\n",
    "    'scores': torch.rand((5,)),\n",
    "    'indptr': torch.tensor([1, 1, 0, 3], dtype=torch.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "15845610-6dfd-4616-92d7-a9b9e1b2e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds, mask = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a04d-8aad-44c2-a671-a888c19d1038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99fb3eac-ce1d-4125-a6dc-52411a85e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Parameters:\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_data_aug_meta_prefix_for_encoder(prefix:str, **kwargs):\n",
    "        inputs = {}\n",
    "        args = [arg for arg in kwargs if prefix is not None and re.match(f'^{prefix}.*_(input_ids|attention_mask|data2ptr|idx)$', arg)]\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            inputs.setdefault(meta, {})[f\"metadata_{param}\"] = kwargs[arg]\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_aug_meta_prefix_for_feature(feat:str, prefix:str, **kwargs):\n",
    "        keys = ['attention_mask', 'input_ids', 'idx']        \n",
    "        inputs = {f'{prefix}_{k}': kwargs[f'{prefix}_{k}'] for k in keys if f'{prefix}_{k}' in kwargs}\n",
    "        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:\n",
    "            inputs.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_aug_meta_prefix_for_loss(feat:str, prefix:str, **kwargs):\n",
    "        keys = [f'{prefix}_idx', f'p{prefix}_idx']\n",
    "        args = {k: kwargs[k] for k in keys if k in kwargs}\n",
    "        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:\n",
    "            args.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})\n",
    "        if prefix is not None and f'p{prefix}_{feat}2ptr' in kwargs:\n",
    "            args.update({f'p{prefix}_data2ptr': kwargs[f'p{prefix}_{feat}2ptr']})\n",
    "\n",
    "        inputs = {}\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            inputs.setdefault(meta, {})[param] = args[arg]\n",
    "        return inputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b0ffb-a31c-456e-a1b4-fc99fdf5c1bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b848a619-2e1b-498c-8502-19370b71e69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat2data_attention_mask', 'cat2data_input_ids', 'cat2data_idx', 'cat2data_data2ptr'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = Parameters.from_aug_meta_prefix_for_feature('data', 'cat2data', **batch)\n",
    "kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c90c9a8-92f1-4e26-a309-93736130afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "upma_model_inputs = Parameters.from_data_aug_meta_prefix_for_encoder('cat2data', **kwargs)['cat2data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d463dd1c-e5a8-46f6-b35e-0a4fb502da6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata_attention_mask', 'metadata_input_ids', 'metadata_idx', 'metadata_data2ptr'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upma_model_inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42de0e-b03b-469c-af20-609dfa387f08",
   "metadata": {},
   "source": [
    "## `UPMA Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45836e79-26b9-4315-a206-a0cfc292eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAModel(PreTrainedModel):\n",
    "    config: UPMAConfig\n",
    "    load_tf_weights = None\n",
    "    base_model_prefix = \"distilbert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.memory_module = get_memory_module(config.memory_module_name)(config)\n",
    "        \n",
    "        self.n_layers = config.n_layers\n",
    "        self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n",
    "        self._use_sdpa = config._attn_implementation == \"sdpa\"\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                self.config.max_position_embeddings, self.config.dim, module.position_embeddings.weight\n",
    "            )\n",
    "        elif isinstance(module, UPMAEmbeddingMemory) and self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                self.config.num_input_metadata, self.config.dim, module.rank_embeddings.weight\n",
    "            )\n",
    "            \n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.position_embeddings\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        num_position_embeds_diff = new_num_position_embeddings - self.config.max_position_embeddings\n",
    "\n",
    "        # no resizing needs to be done if the length stays the same\n",
    "        if num_position_embeds_diff == 0:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Setting `config.max_position_embeddings={new_num_position_embeddings}`...\")\n",
    "        self.config.max_position_embeddings = new_num_position_embeddings\n",
    "\n",
    "        old_position_embeddings_weight = self.embeddings.position_embeddings.weight.clone()\n",
    "\n",
    "        self.embeddings.position_embeddings = nn.Embedding(self.config.max_position_embeddings, self.config.dim)\n",
    "\n",
    "        if self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=self.config.max_position_embeddings, dim=self.config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if num_position_embeds_diff > 0:\n",
    "                    self.embeddings.position_embeddings.weight[:-num_position_embeds_diff] = nn.Parameter(\n",
    "                        old_position_embeddings_weight\n",
    "                    )\n",
    "                else:\n",
    "                    self.embeddings.position_embeddings.weight = nn.Parameter(\n",
    "                        old_position_embeddings_weight[:num_position_embeds_diff]\n",
    "                    )\n",
    "        # move position_embeddings to correct device\n",
    "        self.embeddings.position_embeddings.to(self.device)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: nn.Embedding):\n",
    "        self.embeddings.word_embeddings = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: dict[int, list[list[int]]]):\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.transformer.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        \n",
    "        metadata_idx: Optional[torch.Tensor] = None,\n",
    "        metadata_input_ids: Optional[torch.Tensor] = None,\n",
    "        metadata_attention_mask: Optional[torch.Tensor] = None,\n",
    "        metadata_scores: Optional[torch.Tensor] = None,\n",
    "        metadata_data2ptr: Optional[torch.Tensor] = None,\n",
    "        metadata_embeds: Optional[torch.Tensor] = None,\n",
    "        inject_memory: Optional[bool] = True,\n",
    "        \n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]:\n",
    "         \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        head_mask_is_none = head_mask is None\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
    "        \n",
    "        if inject_memory:\n",
    "            memory_embeddings, memory_mask = self.memory_module(\n",
    "                input_idx = metadata_idx,\n",
    "                embeds = metadata_embeds,\n",
    "                scores = metadata_scores,\n",
    "                indptr = metadata_data2ptr,\n",
    "                input_ids = metadata_input_ids,\n",
    "                attention_mask = metadata_attention_mask,\n",
    "            ) # (bs, num_input_metadata, dim), (bs, num_input_metadata)\n",
    "            memory_mask = torch.cat([attention_mask, memory_mask], dim=1)\n",
    "            \n",
    "        def _prepare_attention_mask(attention_mask, input_shape):\n",
    "            if self._use_flash_attention_2:\n",
    "                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "            else:\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
    "    \n",
    "                if self._use_sdpa and head_mask_is_none and not output_attentions:\n",
    "                    attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
    "                        attention_mask, embeddings.dtype, tgt_len=input_shape[1]\n",
    "                    )\n",
    "            return attention_mask\n",
    "\n",
    "        attention_mask = _prepare_attention_mask(attention_mask, input_shape)\n",
    "                \n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_state = embeddings\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "            if inject_memory and i+1 == self.config.memory_injection_layer:\n",
    "                hidden_state = torch.cat([hidden_state, memory_embeddings], dim=1)\n",
    "                attention_mask = _prepare_attention_mask(memory_mask, memory_mask.size())\n",
    "                \n",
    "            layer_outputs = layer_module(\n",
    "                hidden_state,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_state = layer_outputs[-1]\n",
    "\n",
    "            if output_attentions:\n",
    "                if len(layer_outputs) != 2:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 2, but it is {len(layer_outputs)}\")\n",
    "\n",
    "                attentions = layer_outputs[0]\n",
    "                all_attentions = all_attentions + (attentions,)\n",
    "            else:\n",
    "                if len(layer_outputs) != 1:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 1, but it is {len(layer_outputs)}\")\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_state, all_hidden_states, all_attentions] if v is not None)\n",
    "            \n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_state, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f00484-68e2-4172-995b-b97e0986b114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c245a428-5d38-4fd5-aa41-a3141087ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPMAModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04928597-5ebe-457c-b6de-da6b9f8347d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata_attention_mask', 'metadata_input_ids', 'metadata_idx', 'metadata_data2ptr'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upma_model_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "620d6ea8-d23c-41ca-8488-ad911e00a64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o = model(input_ids=batch['data_input_ids'], attention_mask=batch['data_attention_mask'], **upma_model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "221dd54e-edbc-4100-936e-dea221d90854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 35, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5fe3f-260a-4230-988e-93d321d19de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923b3a20-713f-4e95-bfd7-b56a61723a1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `UPMAEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f23c229-1bd6-453a-a754-da743661f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPMAEncoder(UPMAModel):\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        config:PretrainedConfig,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "    ):\n",
    "        src_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        targ_model = cls(config)\n",
    "\n",
    "        targ_model.init_weights()\n",
    "        targ_model.eval()\n",
    "        \n",
    "        src_sd, targ_sd = src_model.state_dict(), targ_model.state_dict()\n",
    "        src_keys, targ_keys = set(src_sd.keys()), set(targ_sd.keys())\n",
    "        \n",
    "        for k in src_keys.intersection(targ_keys):\n",
    "            assert targ_sd[k].shape == src_sd[k].shape, (\n",
    "                f\"Shape mismatch at key '{k}'. \"\n",
    "                f\"Expected {targ_sd[k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "            )\n",
    "            targ_sd[k].copy_(src_sd[k])\n",
    "\n",
    "        diff_keys = targ_keys.difference(src_keys)\n",
    "        transformer_keys = [k for k in src_keys if k.startswith(\"transformer\")]\n",
    "        for k in transformer_keys:\n",
    "            targ_k = k.split('.', maxsplit=1)[1]\n",
    "            \n",
    "            assert targ_k in targ_sd, (\n",
    "                f\"Unexpected key '{targ_k}' encountered, not found in target state_dict.\"\n",
    "            )\n",
    "            \n",
    "            assert targ_sd[targ_k].shape == src_sd[k].shape, (\n",
    "                f\"Shape mismatch at key '{k}'. \"\n",
    "                f\"Expected {targ_sd[targ_k].shape}, but got {src_sd[k].shape} in source state_dict.\"\n",
    "            )\n",
    "            \n",
    "            targ_sd[targ_k].copy_(src_sd[k])\n",
    "            diff_keys.remove(targ_k)\n",
    "\n",
    "        if config.initialize_memory_embeddings_from_injection_layer_mean:\n",
    "            targ_model.initialize_memory_embeddings_from_injection_layer_mean(\n",
    "                meta_dset,\n",
    "                save_file=config.metadata_embedding_file,\n",
    "                batch_size=batch_size,\n",
    "                use_encoder_parallel=config.use_encoder_parallel,\n",
    "            )\n",
    "            \n",
    "        return targ_model\n",
    "\n",
    "    def initialize_memory_embeddings_from_injection_layer_mean(\n",
    "        self,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        save_file:Optional[str] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "        use_encoder_parallel:Optional[bool] = True\n",
    "    ):\n",
    "        if save_file is not None and os.path.exists(save_file):\n",
    "            meta_embeds = torch.load(save_file)\n",
    "        else:\n",
    "            if meta_dset is None: \n",
    "                raise ValueError(\n",
    "                    f\"Invalid argument: 'meta_dset' cannot be None. \"\n",
    "                    f\"Please pass a valid dataset.\"\n",
    "                )\n",
    "                \n",
    "            meta_embeds, device = [], next(self.parameters()).device\n",
    "            meta_dl = DataLoader(meta_dset, batch_size=batch_size, collate_fn=identity_collate_fn)\n",
    "    \n",
    "            model = XCDataParallel(module=self) if use_encoder_parallel else self\n",
    "            for batch in tqdm(meta_dl):\n",
    "                for k, v in batch.items(): \n",
    "                    if isinstance(v, torch.Tensor): batch[k] = v.to(device)\n",
    "                output = model(**batch, data_inject_memory=False, data_output_hidden_states=True)\n",
    "                embeds = output.hidden_states[self.config.memory_injection_layer - 1]\n",
    "                meta_embeds.append(Pooling.mean_pooling(embeds, batch['data_attention_mask']))\n",
    "            meta_embeds = torch.cat(meta_embeds, dim=0)\n",
    "            if save_file is not None: torch.save(meta_embeds, save_file)\n",
    "            \n",
    "        self.memory_module.set_metadata_embeddings(meta_embeds)\n",
    "        return meta_embeds\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        data_input_ids: torch.Tensor, \n",
    "        data_attention_mask: torch.Tensor,\n",
    "        data_aug_meta_prefix: Optional[str]=None,\n",
    "        data_inject_memory: Optional[bool]=True,\n",
    "        data_output_attentions: Optional[bool] = None,\n",
    "        data_output_hidden_states: Optional[bool] = None,\n",
    "        data_return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        meta_kwargs = Parameters.from_data_aug_meta_prefix_for_encoder(data_aug_meta_prefix, **kwargs)\n",
    "        meta_kwargs = meta_kwargs.get(data_aug_meta_prefix, dict())\n",
    "        \n",
    "        output = super().forward(\n",
    "            input_ids=data_input_ids, \n",
    "            attention_mask=data_attention_mask,\n",
    "            inject_memory=data_inject_memory,\n",
    "            output_attentions=data_output_attentions,\n",
    "            output_hidden_states=data_output_hidden_states,\n",
    "            return_dict=data_return_dict,\n",
    "            **meta_kwargs\n",
    "        )\n",
    "        \n",
    "        if self.config.data_repr_pooling:\n",
    "            embeds = output[0][:, :data_attention_mask.shape[1], :]\n",
    "            attention_mask = data_attention_mask\n",
    "        else:\n",
    "            embeds = output[0]\n",
    "            if 'metadata_data2ptr' in meta_kwargs:\n",
    "                memory_mask = alignment_mask(meta_kwargs['metadata_data2ptr'])\n",
    "                attention_mask = torch.cat([data_attention_mask, memory_mask], dim=1)\n",
    "            else:\n",
    "                attention_mask = data_attention_mask\n",
    "                \n",
    "        assert embeds.shape[:2] == attention_mask.shape, (\n",
    "            f\"Shape mismatch: embeds.shape[:2] = {embeds.shape[:2]} \"\n",
    "            f\"but attention_mask.shape = {attention_mask.shape}.\"\n",
    "        )\n",
    "        \n",
    "        data_repr = Pooling.mean_pooling(embeds, attention_mask)\n",
    "        return UPMAEncoderOutput(repr=data_repr, **output)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4e74f-9080-4b21-a44d-dc2b2cf62177",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1e401e9-6a52-4592-a2c9-171ea872be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UPMAConfig(\n",
    "    num_total_metadata=block.train.dset.meta['cat_meta'].n_meta,\n",
    "    num_input_metadata = 3,\n",
    "    pad_metadata_idx=None,\n",
    "    metadata_dropout=0.1,\n",
    "    memory_injection_layer=None,\n",
    "    memory_module_name=\"embeddings\",\n",
    "\n",
    "    data_aug_meta_prefix=\"cat2data\",\n",
    "    lbl2data_aug_meta_prefix=\"cat2lbl\",\n",
    "\n",
    "    data_enrich=True,\n",
    "    lbl2data_enrich=False,\n",
    "    data_repr_pooling=False,\n",
    "\n",
    "    margin=0.3,\n",
    "    num_negatives=5,\n",
    "    tau=0.1,\n",
    "    apply_softmax=True,\n",
    "\n",
    "    calib_margin=0.3,\n",
    "    calib_num_negatives=10,\n",
    "    calib_tau=0.1,\n",
    "    calib_apply_softmax=False,\n",
    "    \n",
    "    calib_loss_weight=0.1,\n",
    "    use_calib_loss=True,\n",
    "    \n",
    "    use_encoder_parallel=False,\n",
    "\n",
    "    initialize_memory_embeddings_from_injection_layer_mean=False,\n",
    "    metadata_embedding_file=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59564a-fe30-48af-80fb-d9169dd84f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "baecd2c8-1871-4caa-8fd3-8055fe2aeeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_dset = block.train.dset.meta_dset('cat_meta')\n",
    "model = UPMAEncoder.from_pretrained(config, meta_dset=meta_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424eae5-c5ea-4469-af70-8f27bffd7f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c87bebf0-9fe7-48d9-a2d8-26832cb20243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('data', config.data_aug_meta_prefix, **batch)\n",
    "\n",
    "o = model(data_input_ids=batch[\"data_input_ids\"], data_attention_mask=batch[\"data_attention_mask\"], \n",
    "          data_aug_meta_prefix=config.data_aug_meta_prefix, data_enrich=config.data_enrich, **data_meta_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83ff8ee6-aa7e-4dcd-837a-6808d8a03c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 35, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd679529-48e7-42c7-968b-c7b4eed0468e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e0de6c7-5e05-4354-ae76-2fac1241aa70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `UPA000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b7d28224-725c-4349-8b7f-aa2df23b1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UPA000(PreTrainedModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config: UPMAConfig,\n",
    "        meta_dset:Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size:Optional[int] = 100,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.encoder = UPMAEncoder.from_pretrained(config, meta_dset=meta_dset, batch_size=batch_size)\n",
    "        \n",
    "        self.rep_loss_fn = MultiTriplet(margin=config.margin, n_negatives=config.num_negatives, tau=config.tau, \n",
    "                                        apply_softmax=config.apply_softmax, reduce='mean')\n",
    "        self.cab_loss_fn = Calibration(margin=config.calib_margin, tau=config.calib_tau, n_negatives=config.calib_num_negatives, \n",
    "                                       apply_softmax=config.calib_apply_softmax, reduce='mean')\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        config: PretrainedConfig,\n",
    "        meta_dset: Optional[Union[MainXCDataset, SMainXCDataset]] = None,\n",
    "        batch_size: Optional[int] = 100,\n",
    "    ):\n",
    "        return cls(config, meta_dset=meta_dset, batch_size=batch_size)\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        inp_repr: Optional[torch.Tensor] = None,\n",
    "        targ_repr: Optional[torch.Tensor] = None,\n",
    "        targ_ptr: Optional[torch.Tensor] = None,\n",
    "        targ_idx: Optional[torch.Tensor] = None,\n",
    "        ptarg_ptr: Optional[torch.Tensor] = None,\n",
    "        ptarg_idx: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        return self.rep_loss_fn(inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)\n",
    "\n",
    "    def calibration_loss(\n",
    "        self,\n",
    "        einp_repr: Optional[torch.Tensor] = None,\n",
    "        inp_repr: Optional[torch.Tensor] = None,\n",
    "        targ_repr: Optional[torch.Tensor] = None,\n",
    "        targ_ptr: Optional[torch.Tensor] = None,\n",
    "        targ_idx: Optional[torch.Tensor] = None,\n",
    "        ptarg_ptr: Optional[torch.Tensor] = None,\n",
    "        ptarg_idx: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        return self.config.calib_loss_weight * self.cab_loss_fn(einp_repr, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids: Optional[torch.Tensor] = None,\n",
    "        data_attention_mask: Optional[torch.Tensor] = None,\n",
    "        lbl2data_data2ptr: Optional[torch.Tensor] = None,\n",
    "        lbl2data_idx: Optional[torch.Tensor] = None,\n",
    "        lbl2data_input_ids: Optional[torch.Tensor] = None,\n",
    "        lbl2data_attention_mask: Optional[torch.Tensor] = None,\n",
    "        plbl2data_data2ptr: Optional[torch.Tensor] = None,\n",
    "        plbl2data_idx: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        encoder = XCDataParallel(module=self.encoder) if self.config.use_encoder_parallel else self.encoder\n",
    "        \n",
    "        data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('data', self.config.data_aug_meta_prefix, **kwargs)\n",
    "        data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
    "                         data_aug_meta_prefix=self.config.data_aug_meta_prefix, data_inject_memory=self.config.data_inject_memory, **data_meta_kwargs)\n",
    "        \n",
    "        loss = None; lbl2data_o = UPMAEncoderOutput()\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_meta_kwargs = Parameters.from_aug_meta_prefix_for_feature('lbl', self.config.lbl2data_aug_meta_prefix, **kwargs)\n",
    "            lbl2data_o = encoder(data_input_ids=lbl2data_input_ids, data_attention_mask=lbl2data_attention_mask, \n",
    "                                 data_aug_meta_prefix=self.config.lbl2data_aug_meta_prefix, data_inject_memory=self.config.lbl2data_inject_memory, **lbl2data_meta_kwargs)\n",
    "            \n",
    "            loss = self.compute_loss(data_o.repr, lbl2data_o.repr,lbl2data_data2ptr,lbl2data_idx,\n",
    "                                     plbl2data_data2ptr,plbl2data_idx)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_o.repr,lbl2data_o.repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return UPMAModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_o.repr,\n",
    "            lbl2data_repr=lbl2data_o.repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc769de-9935-4078-8233-75d436021d46",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "053952ab-b015-418f-9af0-0eff9ad79268",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = UPMAConfig(\n",
    "    num_total_metadata=block.train.dset.meta['cat_meta'].n_meta,\n",
    "    num_input_metadata = 3,\n",
    "    metadata_dropout = 0.1,\n",
    "    memory_injection_layer = None,\n",
    "    memory_module_name = \"embeddings\",\n",
    "\n",
    "    data_aug_meta_prefix = \"cat2data\",\n",
    "    lbl2data_aug_meta_prefix = \"cat2lbl\",\n",
    "\n",
    "    data_inject_memory = True,\n",
    "    lbl2data_inject_memory = False,\n",
    "\n",
    "    margin = 0.3,\n",
    "    num_negatives = 5,\n",
    "    tau = 0.1,\n",
    "    apply_softmax = True,\n",
    "\n",
    "    calib_margin = 0.3,\n",
    "    calib_num_negatives = 10,\n",
    "    calib_tau = 0.1,\n",
    "    calib_apply_softmax = False,\n",
    "    \n",
    "    calib_loss_weight = 0.1,\n",
    "    use_calib_loss = True,\n",
    "    \n",
    "    use_encoder_parallel = False,\n",
    "    initialize_memory_embeddings_from_injection_layer_mean = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c5041df-fd0f-46f1-9acf-f94a0026e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UPA000.from_pretrained(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1e660d8a-100c-47aa-9b32-2862417c3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f80f1-bc23-448f-a1df-0601a4c48be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9222c1-5ac2-4ba2-b251-835e511dec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d7bf00a8-02d4-4e2c-b5bd-2736dc130ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ba361b2c-a4c1-46f2-8e08-77312530177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0329,  0.4165, -0.1727,  ..., -0.3043,  0.0117,  0.1269],\n",
       "        [-0.1260,  0.1324, -0.6800,  ..., -0.5539,  0.0975,  0.0054],\n",
       "        [-0.1265, -0.6460, -0.4209,  ...,  0.0807, -0.0174, -0.0341]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b80a048f-6cfb-4d64-a38e-008894ad36dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1009,  0.3114, -0.2503,  ..., -0.2568, -0.0038,  0.0075],\n",
       "        [ 0.0146,  0.5108, -0.4399,  ..., -0.3527, -0.1128, -0.0712],\n",
       "        [-0.3037, -0.0469, -0.4431,  ..., -0.5149, -0.0814,  0.1870],\n",
       "        [-0.1228,  0.1947, -0.6093,  ..., -0.4707,  0.0389, -0.0159],\n",
       "        [-0.3955, -0.2311, -0.4383,  ..., -0.0789, -0.0377,  0.2048],\n",
       "        [-0.3194, -0.6375, -0.2985,  ...,  0.1485, -0.0762,  0.2454]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.lbl2data_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14902286-a05e-46a2-926d-bc1f17200b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddd03a-adf8-47bc-94b7-c441f9d78d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
