{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc296f6-a8d7-4a4c-b435-59340b5f9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816d57b5-40f1-4472-ad6f-ea014d90b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027bee80-20cd-4003-925b-caa002001d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ed29e-8252-4ce5-9a4f-d89f26e4040c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcba962a-fd31-408d-bbb8-2cf8a94a7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from xcai.block import prepare_batch\n",
    "from xcai.models.MMM0XX import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c763df4d-0c1b-414d-9168-d8726571dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "fname = f'{pkl_dir}/processed/wikiseealsotitles_data_distilbert-base-uncased_xcs.pkl'\n",
    "\n",
    "with open(fname, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd015316-7ba6-4b2e-8bfc-367e5c7a43e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "m = BT0001.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa81e06c-d9fd-4e99-95e9-71d39fcb7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "batch = block.train.one_batch(bsz)\n",
    "for i, batch in enumerate(block.train.dl):\n",
    "    if i > 5: break\n",
    "        \n",
    "b = prepare_batch(m,batch, m_args=['plbl2data_idx', 'plbl2data_data2ptr'])\n",
    "m,b = m.to('cuda'),b.to('cuda')\n",
    "data_logits, lbl2data_input_ids, lbl2data_data2ptr, lbl2data_idx, lbl2data_logits, data_input_ids, data_repr, lbl2data_repr, data_embed, data_attention_mask, lbl2data_embed, lbl2data_attention_mask, kwargs = m(**b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b252b-9583-48a7-811a-c6f25262e1d3",
   "metadata": {},
   "source": [
    "## `RLLossWeights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7ecd9d-ac72-47d2-8b2c-25e2017d7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_sparse_matrix(data_idx:torch.Tensor, n_data:torch.Tensor, scores:Optional[torch.Tensor]=None):\n",
    "    data_ptr = torch.cat([torch.zeros(1, device=n_data.device, dtype=n_data.dtype), n_data.cumsum(0)])\n",
    "    if scores is None: scores = torch.ones_like(data_idx)\n",
    "    if data_idx.shape != scores.shape: raise ValueError(f'`data_idx` and `scores` should have same shape.')\n",
    "    return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea007bd-a98d-47b1-abab-8c22f8811ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLLossWeights(torch.nn.Module):\n",
    "    def __init__(self, num_samples, std=0.1, lr=0.001, reward_func=None,\n",
    "                 collector=10, min=0.1, rest_init=0.1) -> None:\n",
    "        super().__init__()\n",
    "        init = np.ones(num_samples)\n",
    "        init[:] = rest_init\n",
    "        self.reward_func = reward_func\n",
    "        self.collector = collector\n",
    "        self.lr = lr\n",
    "        self.num_samples = num_samples\n",
    "        self.mu = torch.nn.Parameter(torch.Tensor(init))\n",
    "        self.std = torch.nn.Parameter(torch.Tensor(np.ones(num_samples)*std),\n",
    "                                      requires_grad=False)\n",
    "        self.dist = torch.distributions.normal.Normal(self.mu, self.std)\n",
    "        self.min = min\n",
    "        self.w = None\n",
    "        self.reset_metrics()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.collect_size = 0\n",
    "        self.collect_value = 0\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def sample(self, device=\"cpu\"):\n",
    "        if self.w is None:\n",
    "            self.w = self.clip(self.dist.sample())\n",
    "        return self.w.to(device)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.mu.grad = None\n",
    "        self.collect_size = 0\n",
    "        self.collect_value = 0\n",
    "        self.w = None\n",
    "\n",
    "    def collect(self, pred, gt):\n",
    "        size = pred.size(0)\n",
    "        rewd = self.reward_func(pred, gt)  # TODO\n",
    "        self.collect_value += rewd\n",
    "        self.collect_size += size\n",
    "        pass\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        inp:torch.FloatTensor,\n",
    "        targ:torch.LongTensor, \n",
    "        n_inp2targ:torch.LongTensor,\n",
    "        inp2targ_idx:torch.LongTensor,\n",
    "        n_pinp2targ:torch.LongTensor,\n",
    "        pinp2targ_idx:torch.LongTensor\n",
    "    ):\n",
    "        pred = inp@targ.T\n",
    "        \n",
    "        _, idx = torch.unique(torch.cat([inp2targ_idx, pinp2targ_idx]), return_inverse=True)\n",
    "        gt = get_sparse_matrix(idx[len(inp2targ_idx):], n_pinp2targ).to_dense()[:, idx[:len(inp2targ_idx)]]\n",
    "    \n",
    "        self.step_counter += 1\n",
    "        self.collect(pred, gt)\n",
    "        if self.step_counter % self.collector == 0:\n",
    "            loss = -self.dist.log_prob(self.w)*self.curr_reward\n",
    "            loss = torch.sum(loss).backward()\n",
    "            self.mu.data = self.mu - self.lr * self.mu.grad.data\n",
    "            self.dist.loc = self.clip(self.mu)\n",
    "            self.step_counter = 0\n",
    "            self.zero_grad()\n",
    "\n",
    "    def clip(self, vect):\n",
    "        return torch.clamp(vect, min=self.min)\n",
    "\n",
    "    @property\n",
    "    def curr_reward(self):\n",
    "        return self.collect_value/self.collect_size\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{self.mu}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204a7c34-51e6-446a-baf5-9567ac1e2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLLossWeightsCumuluative(RLLossWeights):\n",
    "    def __init__(self, num_samples=1, std=0.01, lr=0.01, m=0.8,\n",
    "                 reward_func=None, collector=10, min=0.1, rest_init=0.1) -> None:\n",
    "        self.m = m\n",
    "        super().__init__(num_samples, std, lr, reward_func, collector, min, rest_init)\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        super().reset_metrics()\n",
    "        self.reward_prev = None\n",
    "        self.in_warmup = True\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        inp:torch.FloatTensor,\n",
    "        targ:torch.LongTensor, \n",
    "        n_inp2targ:torch.LongTensor,\n",
    "        inp2targ_idx:torch.LongTensor,\n",
    "        n_pinp2targ:torch.LongTensor,\n",
    "        pinp2targ_idx:torch.LongTensor\n",
    "    ):\n",
    "        pred = inp@targ.T\n",
    "        \n",
    "        _, idx = torch.unique(torch.cat([inp2targ_idx, pinp2targ_idx]), return_inverse=True)\n",
    "        gt = get_sparse_matrix(idx[len(inp2targ_idx):], n_pinp2targ).to_dense()[:, idx[:len(inp2targ_idx)]]\n",
    "    \n",
    "        self.step_counter += 1\n",
    "        self.collect(pred, gt)\n",
    "\n",
    "        if self.step_counter % self.collector == 0:\n",
    "            if self.in_warmup:\n",
    "                self.in_warmup = False\n",
    "                self.reward_prev = self.curr_reward\n",
    "            else:\n",
    "                reward = self.curr_reward - self.reward_prev\n",
    "                loss = -self.dist.log_prob(self.w).sum()\n",
    "                loss.backward()\n",
    "                grad = self.mu.grad.data*reward\n",
    "                grad = torch.clip(torch.nan_to_num(grad), min=-1, max=1)\n",
    "                self.mu.data = self.mu - self.lr * grad\n",
    "            self.dist.loc = self.clip(self.mu)\n",
    "            self.step_counter = 0\n",
    "            self.reward_prev = (1-self.m)*self.curr_reward + \\\n",
    "                self.m*self.reward_prev\n",
    "            self.zero_grad()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a5ebc-98d8-41f2-8c8a-5e5aa5b495ac",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0583b697-a27c-4ed2-a5a9-ca1cfb35d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AccMiniBatch(pred, gt):\n",
    "    gt = gt.to(pred.device)\n",
    "    indices = pred.topk(largest=True, dim=1, k=1)[1]\n",
    "    return torch.sum(gt.gather(1, indices)).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7ea05-9915-4cb6-8bed-2b139e7400ac",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba2edcb5-a5a7-42be-8ba6-c630bc694d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_w = RLLossWeightsCumuluative(num_samples=4, reward_func=AccMiniBatch, lr=0.01, collector=20, std=0.1, min=0.1,\n",
    "                                 rest_init=[1.0, 1.0, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b59d79bd-3cfa-404e-8880-ee627af783a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = loss_w.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed013548-f7f9-45d2-a686-5cfdf7e23507",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_w.step(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, kwargs['plbl2data_data2ptr'], kwargs['plbl2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bee876b-a7a5-4823-9547-277086fc766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1193, 0.9930, 0.1156, 0.1000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_w.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f3e6a-6409-49c4-a139-6048b3bb69bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770e52d-4057-4a43-92b7-0b7af7739c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
