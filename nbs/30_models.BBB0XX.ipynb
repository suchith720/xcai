{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c01bbb5-f717-43c9-87f0-69a263cdeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.BBB0XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131638d6-a86f-4fd7-8a62-63e7812eef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88a720fc-a7f7-4227-8d11-e45226d935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect, pickle, os, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any, Union\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "from transformers.activations import get_activation\n",
    "from transformers.utils.generic import ModelOutput\n",
    "\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.losses import *\n",
    "from xcai.core import store_attr\n",
    "from xcai.learner import XCDataParallel\n",
    "from xcai.models.modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ead731-aec5-4567-9b19-650eca2eff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fa3518-6023-4ec0-9d9f-0a4d2fe05c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.block import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b963ac80-9b09-4d1e-8dcf-9461dc83e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05c139-6700-499d-ab7e-e00e114ac0ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c0e8a8-f47d-4eaf-b6c9-ad24ec8715cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "\n",
    "pkl_dir = f'{data_dir}/processed'\n",
    "fname = f'{pkl_dir}/wikiseealsotitles_data_distilbert-base-uncased_xcs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024993d1-9932-4195-b012-43f9aad762bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9244c1a4-11a5-4f9e-9b8b-40e7bf7c4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(5)\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ab9238-94a2-491b-a830-d4bea262257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'data_idx'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbef34-4170-4d6f-838b-820af16a04c5",
   "metadata": {},
   "source": [
    "## `BRT009`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1312e48-cea6-4d34-8194-020a16a4e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT009Encoder(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.activation = get_activation(config.hidden_act)\n",
    "        \n",
    "        self.dr_transform = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dr_layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dr_projector = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.dr_transform.weight.data = torch.eye(self.dr_transform.out_features, self.dr_transform.in_features, \n",
    "                                                  dtype=self.dr_transform.weight.dtype)\n",
    "        self.dr_projector.weight.data = torch.eye(self.dr_projector.out_features, self.dr_projector.in_features, \n",
    "                                                  dtype=self.dr_projector.weight.dtype)\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        o = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.activation(rep)\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        return o, F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecc36dad-2344-4e19-90a9-8b4c2b8d7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT009(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    _tied_weights_keys = [\"encoder.bert\"]\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.3,\n",
    "                 tau:Optional[float]=0.1,\n",
    "                 apply_softmax:Optional[bool]=False,\n",
    "                 n_negatives:Optional[int]=5,\n",
    "                 use_encoder_parallel:Optional[bool]=True,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('use_encoder_parallel')\n",
    "        self.encoder = BRT009Encoder(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, \n",
    "                                    apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.post_init()\n",
    "        self.remap_post_init()\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.encoder.init_dr_head()\n",
    "        \n",
    "    def remap_post_init(self):\n",
    "        self.bert = self.encoder.bert\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = nn.DataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "        \n",
    "        data_o, data_repr = encoder(data_input_ids, data_attention_mask, \n",
    "                                    output_attentions=output_attentions, \n",
    "                                    output_hidden_states=output_hidden_states,\n",
    "                                    return_dict=return_dict)\n",
    "\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  \n",
    "                                                output_attentions=output_attentions, \n",
    "                                                output_hidden_states=output_hidden_states,\n",
    "                                                return_dict=return_dict)\n",
    "            \n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ca6bb-0541-429f-a4d9-b49d9cac928c",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13864cd8-4431-4b5d-b7a5-beb79c390bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BRT009 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BRT009.from_pretrained('bert-base-uncased', bsz=1024, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, \n",
    "                           use_encoder_parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56aa885e-2c0f-4211-8dba-0b0292a72995",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aaa45b5-6c45-4d7b-a3c3-aaa603d4d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69702a6-61b5-4ed7-812a-0078e7d0648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0148, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb53e88-1f8a-4d8e-a5e7-f9defeb4c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
