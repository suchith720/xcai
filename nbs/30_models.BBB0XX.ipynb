{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c01bbb5-f717-43c9-87f0-69a263cdeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.BBB0XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131638d6-a86f-4fd7-8a62-63e7812eef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a720fc-a7f7-4227-8d11-e45226d935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect, pickle, os, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any, Union\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "from transformers.activations import get_activation\n",
    "from transformers.utils.generic import ModelOutput\n",
    "\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.losses import *\n",
    "from xcai.core import store_attr\n",
    "from xcai.learner import XCDataParallel\n",
    "from xcai.models.modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ead731-aec5-4567-9b19-650eca2eff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fa3518-6023-4ec0-9d9f-0a4d2fe05c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.block import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b963ac80-9b09-4d1e-8dcf-9461dc83e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05c139-6700-499d-ab7e-e00e114ac0ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9c8cfc-7f14-40ce-8ed3-a42e4a818dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d707f8-2f29-4625-a607-67919ba2bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/suchith720/Projects/data/'\n",
    "config_file = 'wikiseealsotitles'\n",
    "config_key = 'data_meta'\n",
    "\n",
    "mname = 'google-bert/bert-base-uncased'\n",
    "\n",
    "pkl_dir = f'{data_dir}/processed/'\n",
    "pkl_file = f'{pkl_dir}/mogicX/wikiseealsotitles_data_distilbert-base-uncased_sxc.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3a6cb-9d55-4fc8-bd0f-5f000c1b4641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a326a266-ff5d-4edd-9a25-9756a1e11a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = build_block(pkl_file, config_file, True, config_key, data_dir=data_dir, n_slbl_samples=2, do_build=False, \n",
    "                    main_oversample=True, meta_oversample=True, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88f838-7d95-4ed8-835a-020da537598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3157b520-3d31-46a8-b85d-c271af691cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.meta['neg_meta'] = block.train.dset.meta['cat_meta']\n",
    "block.train.dset.meta['neg_meta'].prefix = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65523f0c-885c-47fa-b185-c986255e8a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9244c1a4-11a5-4f9e-9b8b-40e7bf7c4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(5)\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3ab9238-94a2-491b-a830-d4bea262257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_scores', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pneg2data_idx', 'pneg2data_data2ptr', 'neg2data_idx', 'neg2data_scores', 'neg2data_data2ptr', 'neg2data_identifier', 'neg2data_input_text', 'neg2data_input_ids', 'neg2data_attention_mask', 'pneg2lbl_idx', 'pneg2lbl_lbl2ptr', 'neg2lbl_idx', 'neg2lbl_scores', 'neg2lbl_lbl2ptr', 'neg2lbl_identifier', 'neg2lbl_input_text', 'neg2lbl_input_ids', 'neg2lbl_attention_mask', 'neg2lbl_data2ptr', 'pneg2lbl_data2ptr'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a176a-acfc-4cfd-8e0b-a59b6f1e1631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50bbef34-4170-4d6f-838b-820af16a04c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `BRT009`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1312e48-cea6-4d34-8194-020a16a4e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT009Encoder(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.activation = get_activation(config.hidden_act)\n",
    "        \n",
    "        self.dr_transform = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dr_layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dr_projector = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_dr_head(self):\n",
    "        torch.nn.init.eye_(self.dr_transform.weight)\n",
    "        torch.nn.init.eye_(self.dr_projector.weight)\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        o = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.activation(rep)\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        return o, F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecc36dad-2344-4e19-90a9-8b4c2b8d7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT009(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    _tied_weights_keys = [\"encoder.bert\"]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        bsz:Optional[int]=None,\n",
    "        tn_targ:Optional[int]=None,\n",
    "        margin:Optional[float]=0.3,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=False,\n",
    "        n_negatives:Optional[int]=5,\n",
    "        use_encoder_parallel:Optional[bool]=True,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('use_encoder_parallel')\n",
    "        self.encoder = BRT009Encoder(config)\n",
    "        self.loss_fn = MultiTriplet(margin=margin, n_negatives=n_negatives, tau=tau, \n",
    "                                    apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.post_init()\n",
    "        self.remap_post_init()\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.encoder.init_dr_head()\n",
    "        \n",
    "    def remap_post_init(self):\n",
    "        self.bert = self.encoder.bert\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = nn.DataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "        \n",
    "        data_o, data_repr = encoder(data_input_ids, data_attention_mask, \n",
    "                                    output_attentions=output_attentions, \n",
    "                                    output_hidden_states=output_hidden_states,\n",
    "                                    return_dict=return_dict)\n",
    "\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  \n",
    "                                                output_attentions=output_attentions, \n",
    "                                                output_hidden_states=output_hidden_states,\n",
    "                                                return_dict=return_dict)\n",
    "            \n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ca6bb-0541-429f-a4d9-b49d9cac928c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13864cd8-4431-4b5d-b7a5-beb79c390bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3416f35c538d4a4b8851c0d7e3070702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c636520f1eed433c91d46b8bc2a4d8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BRT009 were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BRT009.from_pretrained('bert-large-uncased', margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, \n",
    "                           use_encoder_parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56aa885e-2c0f-4211-8dba-0b0292a72995",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aaa45b5-6c45-4d7b-a3c3-aaa603d4d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69702a6-61b5-4ed7-812a-0078e7d0648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0286, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60066a5-3de6-496d-9b07-88129195becc",
   "metadata": {},
   "source": [
    "## `BRT023`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ed704b8-17f4-472a-90ad-fe3692961295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT023Encoder(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config,\n",
    "        normalize:Optional[bool]=False,\n",
    "        use_ln:Optional[bool]=False,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('normalize,use_ln')\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.transform = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.projector = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-6) if use_ln else None\n",
    "        self.activation = get_activation(config.hidden_act)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_dr_head(self):\n",
    "        torch.nn.init.eye_(self.transform.weight)\n",
    "        torch.nn.init.eye_(self.projector.weight)\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        o = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.transform(o[0])\n",
    "        if self.use_ln: \n",
    "            rep = self.layer_norm(rep)\n",
    "        rep = self.projector(rep)\n",
    "        rep = Pooling.mean_pooling(rep, attention_mask)\n",
    "        \n",
    "        return o, F.normalize(rep, dim=1) if self.normalize else rep\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96fbf320-8378-4a08-96ba-44634a266297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BRT023(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    _tied_weights_keys = [\"encoder.bert\"]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        normalize:Optional[bool]=False,\n",
    "        use_layer_norm:Optional[bool]=False,\n",
    "        use_encoder_parallel:Optional[bool]=True,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('use_encoder_parallel')\n",
    "        self.encoder = BRT023Encoder(config, normalize=normalize, use_ln=use_layer_norm)\n",
    "        self.loss_fn = MarginMSEWithNegatives()\n",
    "        self.post_init(); self.remap_post_init()\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.encoder.init_dr_head()\n",
    "        \n",
    "    def remap_post_init(self):\n",
    "        self.bert = self.encoder.bert\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_scores:Optional[torch.Tensor]=None,\n",
    "\n",
    "        neg2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        neg2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        neg2data_scores:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = nn.DataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "        \n",
    "        data_o, data_repr = encoder(data_input_ids, data_attention_mask, \n",
    "                                    output_attentions=output_attentions, \n",
    "                                    output_hidden_states=output_hidden_states,\n",
    "                                    return_dict=return_dict)\n",
    "        \n",
    "        loss, lbl2data_repr, neg2data_repr = None, None, None\n",
    "        if (\n",
    "            lbl2data_input_ids is not None and \n",
    "            neg2data_input_ids is not None\n",
    "        ):\n",
    "            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask,  \n",
    "                                                output_attentions=output_attentions, \n",
    "                                                output_hidden_states=output_hidden_states,\n",
    "                                                return_dict=return_dict)\n",
    "            \n",
    "            neg2data_o, neg2data_repr = encoder(neg2data_input_ids, neg2data_attention_mask,  \n",
    "                                                output_attentions=output_attentions, \n",
    "                                                output_hidden_states=output_hidden_states,\n",
    "                                                return_dict=return_dict)\n",
    "\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_scores, neg2data_repr, neg2data_scores, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            neg2data_repr=neg2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa952e-bb07-44df-a40e-7680c395b351",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a36ce252-bf06-4bae-b561-4a760df4349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BRT023 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.projector.bias', 'encoder.projector.weight', 'encoder.transform.bias', 'encoder.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BRT023.from_pretrained('bert-base-uncased', normalize=True, use_encoder_parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07e02f17-19ee-4081-8615-5bbd97f894a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e4f92f8-52e6-4981-8e10-813d3cf68b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29358361-c2e6-44be-80fc-bc18e5189f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1506, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa7622-55fe-4fac-97b4-8a2a88e3a318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
