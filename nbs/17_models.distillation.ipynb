{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db601c-f6a8-4244-9489-1e893d36a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e71b9-a1f1-41a5-b9c5-d61c5f5f684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75c653-edf1-4772-922e-fce670d99293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch, numpy as np\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from xcai.core import store_attr\n",
    "from xcai.losses import Cosine, MultiTriplet\n",
    "from xcai.models.PPP0XX import XCModelOutput\n",
    "from transformers import DistilBertPreTrainedModel,DistilBertConfig\n",
    "from transformers.utils.generic import ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755bff4-23d1-4415-88d8-5ad60fce5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,torch, pickle, numpy as np\n",
    "\n",
    "from xcai.block import *\n",
    "from xcai.basics import *\n",
    "from xcai.models.PPP0XX import DBT010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a491a-2a8f-4be4-97ce-4ebf0e856b9a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33efb6-d64a-4348-82c0-4a0e207160ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_rm_radga-aug-cat-hlk-block-032.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafe42b-f702-47dd-a84e-edce8cd2e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852f6fe-e923-4fbb-a0ce-70ff38a808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.data.data_info['aug_input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "block.train.dset.data.data_info['aug_attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.data_info['aug_input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "block.test.dset.data.data_info['aug_attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32199d9f-9658-4258-981b-bad4c4af8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8ec57-1a35-451a-ba1d-f690fcc9a6a1",
   "metadata": {},
   "source": [
    "## Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed233f6c-076e-478c-8240-f9f8c9c2ce12",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2584330-1d7f-459f-b650-4673f3689aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6459017-bae6-423f-8c6f-7c1d4ebcb58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/69-distillation-for-wikiseealso-1-1',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_steps=3000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=25,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=2,\n",
    "    maximum_cluster_size=1600,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None,\n",
    "    fp16=True,\n",
    "    label_names=['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512277b-dac2-4668-83d5-3b63cc95ec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-1'\n",
    "output_dir = f\"/home/scai/phd/aiz218323/scratch/outputs/{os.path.basename(model_output)}\"\n",
    "mname = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'\n",
    "\n",
    "m_teacher = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=800, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "\n",
    "model_weight_file,model_weights = f'{mname}/model.safetensors',{}\n",
    "with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "    for k in file.keys(): model_weights[k] = file.get_tensor(k)\n",
    "\n",
    "m_teacher.load_state_dict(model_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9296ba6-3455-4f9d-8cb5-b736f8e384ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193aade-479e-4bbb-96f1-a7f76bb8eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=m_teacher, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e6525-d162-406f-b2f2-6e2ef7c151cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b25af25ffcd418790af6818b6d42207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "o = learn.predict(block.test.dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98573c-fbea-4303-9449-32bd09e1dc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>PSP@1</th>\n",
       "      <th>PSP@3</th>\n",
       "      <th>PSP@5</th>\n",
       "      <th>PSP@10</th>\n",
       "      <th>PSN@1</th>\n",
       "      <th>PSN@3</th>\n",
       "      <th>PSN@5</th>\n",
       "      <th>PSN@10</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "      <th>loss</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1787</td>\n",
       "      <td>25.1231</td>\n",
       "      <td>18.8719</td>\n",
       "      <td>11.9556</td>\n",
       "      <td>39.1787</td>\n",
       "      <td>38.7991</td>\n",
       "      <td>40.2252</td>\n",
       "      <td>42.7056</td>\n",
       "      <td>27.7771</td>\n",
       "      <td>30.1143</td>\n",
       "      <td>32.8007</td>\n",
       "      <td>38.2846</td>\n",
       "      <td>27.7771</td>\n",
       "      <td>31.0783</td>\n",
       "      <td>33.3646</td>\n",
       "      <td>36.2856</td>\n",
       "      <td>49.0181</td>\n",
       "      <td>66.5973</td>\n",
       "      <td>70.1792</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>343.8718</td>\n",
       "      <td>516.224</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P@1      P@3      P@5     P@10      N@1      N@3      N@5     N@10  \\\n",
       "0  39.1787  25.1231  18.8719  11.9556  39.1787  38.7991  40.2252  42.7056   \n",
       "\n",
       "     PSP@1    PSP@3    PSP@5   PSP@10    PSN@1    PSN@3    PSN@5   PSN@10  \\\n",
       "0  27.7771  30.1143  32.8007  38.2846  27.7771  31.0783  33.3646  36.2856   \n",
       "\n",
       "      R@10    R@100    R@200    loss   runtime  samples_per_second  \\\n",
       "0  49.0181  66.5973  70.1792  0.0198  343.8718             516.224   \n",
       "\n",
       "   steps_per_second  \n",
       "0             0.323  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric(o.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395ca7e-748a-431f-959d-5234f8ade2db",
   "metadata": {},
   "source": [
    "### Query-label representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c3522-fd7a-478c-8e4a-f50afa84f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d570cf333144eb79f3a55681b34bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = learn.train_dataset.data_dset\n",
    "dataloader = learn.get_test_dataloader(dataset)\n",
    "data_repr = learn.get_representation(dataloader, representation_attribute='data_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290d067-90cc-471f-99a0-3fe4f667edbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96de6f814ddd4bedb48d384ab42a90e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = learn.train_dataset.lbl_dset\n",
    "dataloader = learn.get_test_dataloader(dataset)\n",
    "lbl_repr = learn.get_representation(dataloader, representation_attribute='data_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c4d68-dfeb-4e49-bc09-6efea5312c49",
   "metadata": {},
   "source": [
    "### `TCH001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bad8a-670c-4ce5-b572-f8523de1df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class TCHOutput(ModelOutput):\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933df48-d500-4058-9676-30823f7c17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TCH001(DistilBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, n_data:int, n_lbl:int, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        store_attr('n_data,n_lbl')\n",
    "        self.data_repr = nn.Embedding(self.n_data, config.dim)\n",
    "        self.lbl_repr = nn.Embedding(self.n_lbl, config.dim)\n",
    "\n",
    "    def init_embeddings(self, data_repr:torch.Tensor, lbl_repr:torch.Tensor):\n",
    "        self.data_repr.data = data_repr\n",
    "        self.lbl_repr.data = lbl_repr\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_idx:torch.Tensor,\n",
    "        lbl2data_idx:torch.Tensor,\n",
    "    ):\n",
    "        return TCHOutput(\n",
    "            data_repr=self.data_repr(data_idx),\n",
    "            lbl2data_repr= self.lbl_repr(lbl2data_idx),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea6134-e189-41d0-94c5-66c6aec5d586",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e206f-eb00-4c24-a68a-9bc776574ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCH001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10167607-637b-4f74-b69e-9812846a0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_embeddings(data_repr, lbl_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75b489-d2f7-409d-8d74-5cd99c737c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))\n",
    "b = prepare_batch(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d757aab-fb94-4486-9e98-32ca4bdeb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b36904-0600-4a43-bf12-7f437314aa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 768]), torch.Size([10, 768]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr.shape, o.lbl2data_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235a21e-e604-42e2-9f0f-639599a25ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228f18f-597e-416d-a37e-7c8c7e7fa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'{model_output}/teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c443ca-f592-4c61-b728-62babfbaa1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cc581-1868-481a-9a6b-e82dbcc74806",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_teacher = TCH001.from_pretrained(f'{model_output}/teacher', n_data=block.train.dset.n_data, n_lbl=block.n_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeb38b-d71d-4698-8c7c-1b29b0bef609",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d142f-ec80-4dff-a280-664602c8438f",
   "metadata": {},
   "source": [
    "### `DTL001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d66e41-0575-4708-a12a-380042c2b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DTL001(DistilBertPreTrainedModel):\n",
    "    use_representation,use_generation = True,False\n",
    "    _tied_weights_keys = [\"m_student.encoder.distilbert,m_teacher.encoder.distilbert\"]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        m_student:nn.Module,\n",
    "        m_teacher:nn.Module,\n",
    "        embed_sim_loss_weight:Optional[float]=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "        store_attr('m_student,m_teacher')\n",
    "        self.s_lw = embed_sim_loss_weight\n",
    "        \n",
    "        self.loss_fn = Cosine(reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_aug_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_aug_attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        student_o = self.m_student(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, **kwargs)\n",
    "\n",
    "        loss = None\n",
    "        if data_aug_input_ids is not None and student_o.loss is not None:\n",
    "            with torch.no_grad(): \n",
    "                teacher_o = self.m_teacher(data_input_ids=data_aug_input_ids, data_attention_mask=data_aug_attention_mask, **kwargs)\n",
    "\n",
    "            dloss = self.loss_fn(student_o.data_embed, data_attention_mask, teacher_o.data_embed, data_aug_attention_mask)\n",
    "            dloss += self.loss_fn(student_o.lbl2data_embed, kwargs['lbl2data_attention_mask'], \n",
    "                                  teacher_o.lbl2data_embed, kwargs['lbl2data_attention_mask'])\n",
    "            loss = student_o.loss + self.s_lw * dloss\n",
    "            \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=student_o.data_repr,\n",
    "            lbl2data_repr=student_o.lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f397a48-7346-479c-8ca1-6ab948fc419d",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e46769-0341-4dfb-be4b-2469e3c9ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa06359-1fda-4b39-b60d-ecbda192216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-1'\n",
    "output_dir = f\"/home/scai/phd/aiz218323/scratch/outputs/{os.path.basename(model_output)}\"\n",
    "mname = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'\n",
    "\n",
    "m_teacher = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=800, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "\n",
    "model_weight_file,model_weights = f'{mname}/model.safetensors',{}\n",
    "with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "    for k in file.keys(): model_weights[k] = file.get_tensor(k)\n",
    "\n",
    "m_teacher.load_state_dict(model_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154cb90-fb7f-4d8b-b284-592c60fc1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m_student = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=800, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                                       n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "m_student.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586e82f-2a46-447a-bb42-0dccd9aa75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DTL001(DistilBertConfig(), m_student=m_student, m_teacher=m_teacher, embed_sim_loss_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04280e-1723-4bcd-83c1-d1835b110b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))\n",
    "b = prepare_batch(model, batch, m_args=['lbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_input_ids', \n",
    "                                        'lbl2data_attention_mask', 'plbl2data_data2ptr', 'plbl2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c85d4-d2f7-4877-8d41-dda3adebf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,b = model.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd268ce2-45f0-44a2-b57e-b18e346d328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8fa61-beaa-4e52-bb4b-d7ec60d2ea90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6006, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69090f60-6bee-406d-b137-c368830a6e59",
   "metadata": {},
   "source": [
    "### `DTL002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6407a6-761f-4c36-ade3-ccde9bae9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DTL002(DistilBertPreTrainedModel):\n",
    "    use_representation,use_generation = True,False\n",
    "    _tied_weights_keys = [\"m_student.encoder.distilbert\"]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        m_student:nn.Module,\n",
    "        m_teacher:nn.Module,\n",
    "        bsz:Optional[int]=None,\n",
    "        tn_targ:Optional[int]=None,\n",
    "        margin:Optional[float]=0.3,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=False,\n",
    "        n_negatives:Optional[int]=5,\n",
    "        distil_loss_weight:Optional[float]=1.0,\n",
    "        mse_loss_weight:Optional[float]=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self.d_lw,self.m_lw = distil_loss_weight,mse_loss_weight\n",
    "        store_attr('m_student,m_teacher')\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.rep_loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, \n",
    "                                        apply_softmax=apply_softmax, reduce='mean')\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        \n",
    "        data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        student_o = self.m_student(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
    "                                   lbl2data_idx=lbl2data_idx, **kwargs)\n",
    "\n",
    "        loss = None\n",
    "        if lbl2data_idx is not None and student_o.loss is not None:\n",
    "            with torch.no_grad(): \n",
    "                teacher_o = self.m_teacher(data_idx=data_idx, lbl2data_idx=lbl2data_idx)\n",
    "\n",
    "            dloss = self.rep_loss_fn(teacher_o.data_repr, student_o.lbl2data_repr, kwargs['lbl2data_data2ptr'], lbl2data_idx, \n",
    "                                     kwargs['plbl2data_data2ptr'], kwargs['plbl2data_idx'], **kwargs)\n",
    "            \n",
    "            dloss += self.rep_loss_fn(student_o.data_repr, teacher_o.lbl2data_repr, kwargs['lbl2data_data2ptr'], lbl2data_idx, \n",
    "                                      kwargs['plbl2data_data2ptr'], kwargs['plbl2data_idx'], **kwargs)\n",
    "\n",
    "            mloss = self.mse_loss_fn(teacher_o.data_repr, student_o.data_repr) + self.mse_loss_fn(teacher_o.lbl2data_repr, student_o.lbl2data_repr)\n",
    "            \n",
    "            loss = student_o.loss + self.d_lw * dloss + self.m_lw * mloss\n",
    "            \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=student_o.data_repr,\n",
    "            lbl2data_repr=student_o.lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebb3ed-a8ee-48a1-8170-427d59cc331b",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2927897-e647-448a-a9f6-b799e4768529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-1'\n",
    "m_teacher = TCH001.from_pretrained(f'{model_output}/teacher', n_data=block.train.dset.n_data, n_lbl=block.n_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd05ec-d20a-4b66-97d3-f0f88c02f3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m_student = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=800, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "m_student.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9679da-87b5-4bf3-bce8-163c2947b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DTL002(DistilBertConfig(), m_student=m_student, m_teacher=m_teacher, bsz=1024, margin=0.3, tau=0.1, \n",
    "               n_negatives=10, apply_softmax=True, distil_loss_weight=1.0, mse_loss_weight=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0a152-18c3-44f7-be0a-969a1add9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))\n",
    "b = prepare_batch(model, batch, m_args=['lbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_input_ids', \n",
    "                                        'lbl2data_attention_mask', 'plbl2data_data2ptr', 'plbl2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc95de0-016b-4490-be25-972d7119c0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_attention_mask', 'data_idx'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb247d0-c71a-4331-b790-95e73aac01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,b = model.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f8811-04ca-4939-8cf7-5cfb484a5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55dd5a-5c77-4781-a304-50eb9976a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6356, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed098cb3-4855-4c63-8e15-8812a2f178ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
