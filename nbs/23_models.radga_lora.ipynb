{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd48060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.radga_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch, re, inspect, pickle, os, torch.nn as nn, math\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any, Union\n",
    "from transformers import (\n",
    "    PretrainedConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertModel,\n",
    "    DistilBertPreTrainedModel,\n",
    ")\n",
    "from transformers.utils.generic import ModelOutput\n",
    "from transformers.activations import get_activation\n",
    "\n",
    "from fastcore.meta import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "from xcai.losses import *\n",
    "from xcai.core import store_attr\n",
    "from xcai.learner import XCDataParallel\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c00009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from xcai.block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ede14",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc269b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb671af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scai/phd/aiz218323/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg(data_dir, 'data_metas', tfm='rm', tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data|cat2lbl2data',1,(1,3)), ('cat2data',1,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3da90-d45a-40eb-bfeb-de65aa6cde4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1d105-6978-44b6-a9c9-851220d03e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-meta_distilbert-base-uncased_rm_radga-cat.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843bd7a-d18c-4f44-bd4b-c56064ee937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(5)\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd8291-03b8-41f1-95a1-acaceb851458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'pcat2lbl_idx', 'pcat2lbl_lbl2data2ptr', 'pcat2lbl_data2ptr', 'cat2lbl_idx', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_lbl2data2ptr', 'cat2lbl_data2ptr', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'cat2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'data_idx', 'hlk2lbl2data_idx', 'hlk2lbl2data_identifier', 'hlk2lbl2data_input_text', 'hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask', 'hlk2lbl2data_data2ptr', 'hlk2lbl2data_plbl2data2ptr', 'hlk2data_idx', 'hlk2data_identifier', 'hlk2data_input_text', 'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63666b5",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class RADOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    data_fused_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_fused_repr: Optional[torch.FloatTensor] = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc40c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class EncoderOutput(ModelOutput):\n",
    "    rep: Optional[torch.FloatTensor] = None\n",
    "    fused_rep: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    fusion_weights: Optional[torch.FloatTensor] = None\n",
    "    meta_repr: Optional[torch.FloatTensor] = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pooling:\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(data_embeds:torch.FloatTensor, data_attention_mask:torch.LongTensor):\n",
    "        data_attention_mask = data_attention_mask.unsqueeze(2).expand(data_embeds.size()).float()\n",
    "        return torch.sum(data_embeds * data_attention_mask, 1) / torch.clamp(data_attention_mask.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97b11c",
   "metadata": {},
   "source": [
    "## CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CrossAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config, self.n_h, self.dim = config, config.n_heads, config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        if self.dim % self.n_h != 0:\n",
    "            raise ValueError(f\"self.n_heads: {self.n_h} must divide self.dim: {self.dim} evenly.\")\n",
    "            \n",
    "        self.q = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.o = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "    def post_init(self):\n",
    "        self.q.weight.data = torch.eye(self.q.out_features, self.q.in_features, dtype=self.q.weight.dtype)\n",
    "        self.k.weight.data = torch.eye(self.k.out_features, self.k.in_features, dtype=self.k.weight.dtype)\n",
    "        self.v.weight.data = torch.eye(self.v.out_features, self.v.in_features, dtype=self.v.weight.dtype)\n",
    "        self.o.weight.data = torch.eye(self.o.out_features, self.o.in_features, dtype=self.o.weight.dtype)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        q: torch.Tensor,\n",
    "        q_m: torch.Tensor,\n",
    "        k: torch.Tensor, \n",
    "        k_m: torch.Tensor,\n",
    "        output_attentions:Optional[bool] = False,\n",
    "    ):\n",
    "        bs, q_len, dim = q.size()\n",
    "        v, k_len = k, k.size(1) \n",
    "\n",
    "        h_dim = self.dim//self.n_h\n",
    "\n",
    "        def shape(x: torch.Tensor): return x.view(bs, -1, self.n_h, h_dim).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor): return x.transpose(1, 2).contiguous().view(bs, -1, self.n_h * h_dim)\n",
    "\n",
    "        q = shape(self.q(q))  # (bs, n_h, q_len, h_dim)\n",
    "        k = shape(self.k(k))  # (bs, n_h, k_len, h_dim)\n",
    "        v = shape(self.v(v))  # (bs, n_h, k_len, h_dim)\n",
    "\n",
    "        q = q / math.sqrt(h_dim)  # (bs, n_h, q_len, h_dim)\n",
    "        sc = torch.matmul(q, k.transpose(2, 3))  # (bs, n_h, q_len, k_len)\n",
    "        \n",
    "        q_m, k_m = q_m.view(bs, 1, -1, 1).to(q.dtype), k_m.view(bs, 1, 1, -1).to(q.dtype)\n",
    "        mask = torch.matmul(q_m, k_m).expand_as(sc)  # (bs, n_h, q_len, k_len)\n",
    "        \n",
    "        sc = sc.masked_fill(mask == 0, torch.tensor(torch.finfo(sc.dtype).min))  # (bs, n_h, q_len, k_len)\n",
    "\n",
    "        w = nn.functional.softmax(sc, dim=-1)  # (bs, n_h, q_len, k_len)\n",
    "        w = self.dropout(w)  # (bs, n_h, q_len, k_len)\n",
    "\n",
    "        o = self.o(unshape(torch.matmul(w, v))) # (bs, q_len, dim)\n",
    "        \n",
    "        if output_attentions: return (o, w)\n",
    "        else: return (o,)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258a231",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('distilbert-base-uncased')\n",
    "fuser = CrossAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec047a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, data_seq_len, n_meta, dim, dtype = 2, 3, 2, config.dim, torch.float32\n",
    "data, meta = torch.randn(bsz, data_seq_len, dim, dtype=dtype), torch.randn(bsz, n_meta, dim, dtype=dtype)\n",
    "data_mask = torch.randint(0, 2, size=(bsz,data_seq_len), dtype=dtype)\n",
    "meta_mask = torch.randint(0, 2, size=(bsz,n_meta), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = fuser(data, data_mask, meta, meta_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d426e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f75c9",
   "metadata": {},
   "source": [
    "## Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e25a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RepresentationHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Linear(config.dim, config.dim)\n",
    "        self.layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.projector = nn.Linear(config.dim, config.dim)\n",
    "        self.activation = get_activation(config.activation)\n",
    "        \n",
    "        self.post_init()\n",
    "        \n",
    "    def post_init(self):\n",
    "        self.transform.weight.data = torch.eye(self.transform.out_features, self.transform.in_features, \n",
    "                                               dtype=self.transform.weight.dtype)\n",
    "        self.projector.weight.data = torch.eye(self.projector.out_features, self.projector.in_features, \n",
    "                                               dtype=self.projector.weight.dtype)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.transform(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GenerationHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Linear(config.dim, config.dim)\n",
    "        self.layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.projector = nn.Linear(config.dim, config.vocab_size)\n",
    "        self.activation = get_activation(config.activation)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.transform(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14f380",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('distilbert-base-uncased')\n",
    "x = torch.randn(10, 20, config.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RepresentationHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ea52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GenerationHead(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9224630",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ca951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Parameters:\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_meta_aug_prefix(prefix:str, **kwargs):\n",
    "        inputs = {}\n",
    "        args = [arg for arg in kwargs if prefix is not None and re.match(f'^{prefix}.*_(input_ids|attention_mask|data2ptr|meta_repr|idx)$', arg)]\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            inputs.setdefault(meta, {})[param] = kwargs[arg]\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_feat_meta_aug_prefix(feat:str, prefix:str, **kwargs):\n",
    "        keys = ['attention_mask', 'input_ids', 'meta_repr', 'idx']\n",
    "        \n",
    "        inputs = {f'{prefix}_{k}': kwargs[f'{prefix}_{k}'] for k in keys if f'{prefix}_{k}' in kwargs}\n",
    "        if prefix is not None and f'{prefix}_{feat}2ptr' in kwargs:\n",
    "            inputs.update({f'{prefix}_data2ptr': kwargs[f'{prefix}_{feat}2ptr']})\n",
    "        return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_meta_pred_prefix(prefix:str, **kwargs):\n",
    "        inputs = {}\n",
    "        args = [arg for arg in kwargs if prefix is not None and re.match(f'^[p]?{prefix}.*', arg)]\n",
    "        for arg in args:\n",
    "            meta,param = arg.split('_', maxsplit=1)\n",
    "            if arg[0] == 'p': \n",
    "                inputs.setdefault(meta[1:], {})[f'p{param}'] = kwargs[arg]\n",
    "            else: \n",
    "                inputs.setdefault(meta, {})[param] = kwargs[arg]\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_meta_loss_weights(lw:Union[float,List], n_meta:int):\n",
    "        if isinstance(lw, float):\n",
    "            lw = lw/n_meta if n_meta else None\n",
    "            return [lw] * n_meta\n",
    "        else:\n",
    "            if len(lw) != n_meta: raise ValueError(f'length of `lw` should be equal to number of metadata.')\n",
    "            return lw\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c76edd-7068-43c8-b661-5cbb185e788c",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ed38-5df8-40e5-8587-a09aca55fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(block.train.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429b9a1-8f1f-44b1-ba3f-e7c01e04b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat2lbl', 'cat2data'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Parameters.from_meta_aug_prefix('cat', **b); p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bf4d9-4748-422b-a6a1-a0b502dcfcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat2lbl_attention_mask', 'cat2lbl_input_ids', 'cat2lbl_data2ptr'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Parameters.from_feat_meta_aug_prefix('data', 'cat2lbl', **b); p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af55d5-31aa-4b4a-8ea5-23e88d9770d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cat2lbl', 'cat2data'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Parameters.from_meta_pred_prefix('cat', **b); p.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8737b",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(DistilBertPreTrainedModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config:PretrainedConfig,\n",
    "        base_model:nn.Module, \n",
    "        resize_length:Optional[int]=None,\n",
    "\n",
    "        lora_r:Optional[int]=8,\n",
    "        lora_alpha:Optional[int]=32,\n",
    "\n",
    "        data_aug_meta_prefix:Optional[str]=None, \n",
    "        lbl2data_aug_meta_prefix:Optional[str]=None, \n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        store_attr('data_aug_meta_prefix,lbl2data_aug_meta_prefix')\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"],\n",
    "            bias='none',\n",
    "        )\n",
    "        self.distilbert = get_peft_model(base_model, lora_config, adapter_name=\"lbl2data\")\n",
    "        if self.data_aug_meta_prefix is not None: self.distilbert.add_adapter(self.data_aug_meta_prefix, lora_config)\n",
    "        if self.lbl2data_aug_meta_prefix is not None: self.distilbert.add_adapter(self.lbl2data_aug_meta_prefix, lora_config)\n",
    "        self._mark_entire_encoder_as_trainable()\n",
    "        \n",
    "        self.dr_head = RepresentationHead(config)\n",
    "        self.dr_fused_head =  RepresentationHead(config)\n",
    "        self.meta_head = RepresentationHead(config)\n",
    "        self.cross_head = CrossAttention(config)\n",
    "         \n",
    "        self.ones = torch.ones(resize_length, dtype=torch.long, device=self.device) if resize_length is not None else None\n",
    "        self.post_init()\n",
    "\n",
    "    def _mark_entire_encoder_as_trainable(self):\n",
    "        for p in self.distilbert.parameters(): p.requires_grad_(True)\n",
    "\n",
    "    def _mark_only_adapters_as_trainable(self):\n",
    "        self.distilbert.base_model._mark_only_adapters_as_trainable(self.distilbert)\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        return self.distilbert.get_position_embeddings()\n",
    "    \n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        self.distilbert.resize_position_embeddings(new_num_position_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def resize(self, inputs:torch.Tensor, mask:torch.Tensor, num_inputs:torch.Tensor):\n",
    "        if torch.any(num_inputs == 0): raise ValueError(\"`num_inputs` should be non-zero positive integer.\")\n",
    "        bsz, dim, total_num_inputs = num_inputs.shape[0], inputs.shape[-1], inputs.shape[0]\n",
    "        \n",
    "        self.ones = self.ones.to(inputs.device)\n",
    "        ones = (\n",
    "            torch.ones(total_num_inputs, dtype=torch.long, device=inputs.device) \n",
    "            if self.ones is None or self.ones.shape[0] < total_num_inputs else self.ones[:total_num_inputs]\n",
    "        )\n",
    "\n",
    "        max_num_inputs = num_inputs.max()\n",
    "        xnum_inputs = max_num_inputs-num_inputs+1\n",
    "\n",
    "        inputs_ptr = num_inputs.cumsum(dim=0)-1\n",
    "        repeat_inputs = ones.scatter(0, inputs_ptr, xnum_inputs)\n",
    "        \n",
    "        resized_inputs = inputs.repeat_interleave(repeat_inputs, dim=0)\n",
    "        resized_mask = mask.repeat_interleave(repeat_inputs, dim=0)\n",
    "        \n",
    "        ignore_mask_idx = ones.scatter(0, inputs_ptr, 0).repeat_interleave(repeat_inputs, dim=0).view(bsz, -1)\n",
    "        ignore_mask_idx[:, -1] = 1; ignore_mask_idx = ignore_mask_idx.view(-1, 1)\n",
    "        \n",
    "        resized_mask *= ignore_mask_idx\n",
    "        \n",
    "        return resized_inputs,resized_mask\n",
    "\n",
    "\n",
    "    \n",
    "    def encode(self, input_ids:torch.Tensor, attention_mask:torch.Tensor, **kwargs):\n",
    "        return self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def dr(self, embed:torch.Tensor, attention_mask:torch.Tensor):\n",
    "        embed = self.dr_head(embed)\n",
    "        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)\n",
    "\n",
    "    def dr_fused(self, embed:torch.Tensor, attention_mask:torch.Tensor):\n",
    "        embed = self.dr_fused_head(embed)\n",
    "        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)\n",
    "\n",
    "    def meta(self, embed:torch.Tensor, attention_mask:torch.Tensor):\n",
    "        embed = self.meta_head(embed)\n",
    "        return F.normalize(Pooling.mean_pooling(embed, attention_mask), dim=1)\n",
    "    \n",
    "    def meta_unnormalized(self, embed:torch.Tensor, attention_mask:torch.Tensor):\n",
    "        embed = self.meta_head(embed)\n",
    "        return Pooling.mean_pooling(embed, attention_mask)\n",
    "\n",
    "    \n",
    "    \n",
    "    def fuse_meta_into_embeddings(self, embed:torch.Tensor, attention_mask:torch.Tensor, meta_kwargs:Dict):\n",
    "        meta_repr = {}\n",
    "        \n",
    "        for m_key, m_args in meta_kwargs.items():\n",
    "            idx = torch.where(m_args['data2ptr'] > 0)[0]\n",
    "            meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
    "\n",
    "            self.distilbert.set_adapter(m_key)\n",
    "            \n",
    "            if len(idx):\n",
    "                if 'meta_repr' in m_args:\n",
    "                    m_repr,m_repr_mask = m_args['meta_repr'],torch.any(m_args['attention_mask'], dim=1).long().view(-1,1)\n",
    "                    m_repr,m_repr_mask = self.resize(m_repr, m_repr_mask, m_args['data2ptr'][idx])\n",
    "                    m_repr_mask = m_repr_mask.bool()\n",
    "                else:\n",
    "                    m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], \n",
    "                                                                m_args['data2ptr'][idx])\n",
    "                    n_meta = m_args['data2ptr'].max()\n",
    "\n",
    "                    m_embed = self.encode(m_input_ids, m_attention_mask)[0]\n",
    "\n",
    "                    m_repr = self.meta_unnormalized(m_embed, m_attention_mask)\n",
    "                    m_repr_mask = torch.any(m_attention_mask, dim=1)\n",
    "                    \n",
    "                m_repr, m_repr_mask = m_repr.view(len(idx), -1, self.config.dim), m_repr_mask.view(len(idx), -1)\n",
    "                \n",
    "                meta_repr[m_key] = F.normalize(m_repr[m_repr_mask], dim=1)\n",
    "                \n",
    "                fused_embed = self.cross_head(embed[idx], attention_mask[idx], m_repr, m_repr_mask)[0]\n",
    "                embed[idx] += fused_embed\n",
    "                \n",
    "        return embed, meta_repr\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        data_input_ids: torch.Tensor, \n",
    "        data_attention_mask: torch.Tensor,\n",
    "        data_aug_meta_prefix: Optional[str]=None,\n",
    "        data_type:Optional[str]=None,\n",
    "        data_unnormalized:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        data_o = self.encode(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        if data_type is not None and data_type == \"meta\":\n",
    "            data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
    "        else: \n",
    "            data_repr = self.dr(data_o[0], data_attention_mask)\n",
    "            \n",
    "        data_fused_repr = meta_repr = None\n",
    "        if data_aug_meta_prefix is not None:\n",
    "            meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
    "            if len(meta_kwargs):\n",
    "                if self.training: self._mark_only_adapters_as_trainable()\n",
    "                data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
    "                                                                             data_attention_mask, \n",
    "                                                                             meta_kwargs)\n",
    "                data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
    "\n",
    "                self.distilbert.set_adapter('lbl2data')\n",
    "                if self.training: self._mark_entire_encoder_as_trainable()\n",
    "        \n",
    "        return EncoderOutput(\n",
    "            rep=data_repr,\n",
    "            fused_rep=data_fused_repr,\n",
    "            meta_repr=meta_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1abf63-cd7c-413f-904b-6bb5269fd92a",
   "metadata": {},
   "source": [
    "## `RAD001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51c610-41c8-4752-b2e6-5ac4cd5492ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RAD001(DistilBertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(\n",
    "        self, config,\n",
    "\n",
    "        base_model:nn.Module, \n",
    "        resize_length:Optional[int]=None,\n",
    "        lora_r:Optional[int]=8,\n",
    "        lora_alpha:Optional[int]=32,\n",
    "        \n",
    "        num_batch_labels:Optional[int]=None, \n",
    "        batch_size:Optional[int]=None,\n",
    "        margin:Optional[float]=0.3,\n",
    "        num_negatives:Optional[int]=5,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=True,\n",
    "        \n",
    "        data_aug_meta_prefix:Optional[str]=None, \n",
    "        lbl2data_aug_meta_prefix:Optional[str]=None, \n",
    "\n",
    "        data_pred_meta_prefix:Optional[str]=None,\n",
    "        lbl2data_pred_meta_prefix:Optional[str]=None,\n",
    "\n",
    "        use_query_loss:Optional[float]=False,\n",
    "        \n",
    "        calib_margin:Optional[float]=0.3,\n",
    "        calib_num_negatives:Optional[int]=10,\n",
    "        calib_tau:Optional[float]=0.1,\n",
    "        calib_apply_softmax:Optional[bool]=True,\n",
    "        calib_loss_weight:Optional[float]=0.1,\n",
    "        use_calib_loss:Optional[float]=False,\n",
    "        \n",
    "        meta_loss_weight:Optional[Union[List,float]]=0.3,\n",
    "        use_fusion_loss:Optional[bool]=False,\n",
    "        fusion_loss_weight:Optional[float]=0.15,\n",
    "        \n",
    "        use_encoder_parallel:Optional[bool]=True,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.m_lw, self.f_lw, self.c_lw = meta_loss_weight, fusion_loss_weight,calib_loss_weight\n",
    "        store_attr('data_pred_meta_prefix,lbl2data_pred_meta_prefix')\n",
    "        store_attr('data_aug_meta_prefix,lbl2data_aug_meta_prefix')\n",
    "        store_attr('use_query_loss,use_calib_loss,use_fusion_loss,use_encoder_parallel')\n",
    "        \n",
    "        self.encoder = Encoder(config, base_model=base_model, resize_length=resize_length, lora_r=lora_r, lora_alpha=lora_alpha, \n",
    "                               data_aug_meta_prefix=data_aug_meta_prefix, lbl2data_aug_meta_prefix=lbl2data_aug_meta_prefix)\n",
    "\n",
    "        self.rep_loss_fn = MultiTriplet(bsz=batch_size, tn_targ=num_batch_labels, margin=margin, n_negatives=num_negatives, \n",
    "                                        tau=tau, apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.cab_loss_fn = Calibration(margin=calib_margin, tau=calib_tau, n_negatives=calib_num_negatives, \n",
    "                                       apply_softmax=calib_apply_softmax, reduce='mean')\n",
    "        \n",
    "    def init_retrieval_head(self):\n",
    "        if self.encoder is None: raise ValueError('`self.encoder` is not initialized.')\n",
    "        self.encoder.dr_head.post_init()\n",
    "        self.encoder.dr_fused_head.post_init()\n",
    "        self.encoder.meta_head.post_init()\n",
    "\n",
    "    def init_cross_head(self):\n",
    "        if self.encoder is None: raise ValueError('`self.encoder` is not initialized.')\n",
    "        self.encoder.cross_head.post_init()\n",
    "        \n",
    "\n",
    "    def compute_loss(self, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx):\n",
    "        return self.rep_loss_fn(inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)\n",
    "\n",
    "    def calibration_loss(self, einp_repr, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx):\n",
    "        return self.c_lw * self.cab_loss_fn(einp_repr, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)\n",
    "    \n",
    "    \n",
    "    def compute_meta_loss(self, data_repr, lbl2data_repr, **kwargs):\n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = XCDataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "            \n",
    "        data_meta_inputs = Parameters.from_meta_pred_prefix(self.data_pred_meta_prefix, **kwargs)\n",
    "        lbl2data_meta_inputs = Parameters.from_meta_pred_prefix(self.lbl2data_pred_meta_prefix, **kwargs)\n",
    "        meta_inputs = {**data_meta_inputs, **lbl2data_meta_inputs}\n",
    "\n",
    "        m_lw = Parameters.get_meta_loss_weights(self.m_lw, len(meta_inputs)) if len(meta_inputs) else []\n",
    "        \n",
    "        loss = 0.0\n",
    "        for inputs,lw in zip(meta_inputs.values(), m_lw):\n",
    "            if 'lbl2data2ptr' in inputs:\n",
    "                idx = torch.where(inputs['lbl2data2ptr'])[0]\n",
    "                if len(idx) > 0:\n",
    "                    inputs_o = encoder(data_input_ids=inputs['input_ids'], data_attention_mask=inputs['attention_mask'], \n",
    "                                       data_type=\"meta\")\n",
    "                    m_loss = self.rep_loss_fn(lbl2data_repr[idx], inputs_o.rep, inputs['lbl2data2ptr'][idx],\n",
    "                                              inputs['idx'], inputs['plbl2data2ptr'][idx], inputs['pidx'])\n",
    "                    loss += lw * m_loss\n",
    "\n",
    "            elif 'data2ptr' in inputs:\n",
    "                idx = torch.where(inputs['data2ptr'])[0]\n",
    "                if len(idx) > 0:\n",
    "                    inputs_o = encoder(data_input_ids=inputs['input_ids'], data_attention_mask=inputs['attention_mask'], \n",
    "                                       data_type=\"meta\")\n",
    "                    m_loss = self.rep_loss_fn(data_repr[idx], inputs_o.rep, inputs['data2ptr'][idx], inputs['idx'], \n",
    "                                              inputs['pdata2ptr'][idx], inputs['pidx'])\n",
    "                    loss += lw * m_loss       \n",
    "\n",
    "            else: raise ValueError('Invalid metadata input arguments.')\n",
    "        return loss\n",
    "\n",
    "    def compute_fusion_loss(self, data_repr, meta_repr:Dict, prefix:str, **kwargs):\n",
    "        meta_inputs = Parameters.from_meta_pred_prefix(prefix, **kwargs)\n",
    "        \n",
    "        loss = 0.0\n",
    "        if meta_repr is not None:\n",
    "            for key,input_repr in meta_repr.items():\n",
    "                inputs = meta_inputs[key]\n",
    "                if 'lbl2data2ptr' in inputs:\n",
    "                    idx = torch.where(inputs['lbl2data2ptr'])[0]\n",
    "                    if len(idx) > 0:\n",
    "                        m_loss = self.rep_loss_fn(data_repr[idx], input_repr, inputs['lbl2data2ptr'][idx],\n",
    "                                                  inputs['idx'], inputs['plbl2data2ptr'][idx], inputs['pidx'])\n",
    "                        loss += self.f_lw * m_loss\n",
    "    \n",
    "                elif 'data2ptr' in inputs:\n",
    "                    idx = torch.where(inputs['data2ptr'])[0]\n",
    "                    if len(idx) > 0:\n",
    "                        m_loss = self.rep_loss_fn(data_repr[idx], input_repr, inputs['data2ptr'][idx], inputs['idx'], \n",
    "                                                  inputs['pdata2ptr'][idx], inputs['pidx'])\n",
    "                        loss += self.f_lw * m_loss       \n",
    "    \n",
    "                else: raise ValueError('Invalid metadata input arguments.')\n",
    "        return loss\n",
    "\n",
    "\n",
    "    \n",
    "    def get_meta_representation(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = XCDataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "            \n",
    "        data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
    "                         data_unnormalized=True, data_type=\"meta\")\n",
    "        return RADOutput(\n",
    "            data_repr=data_o.rep,\n",
    "            data_fused_repr=data_o.fused_rep,\n",
    "        )\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = XCDataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "        \n",
    "        data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('data', self.data_aug_meta_prefix, **kwargs)\n",
    "        data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
    "                         data_aug_meta_prefix=self.data_aug_meta_prefix, **data_meta_kwargs)\n",
    "        \n",
    "        \n",
    "        loss = None; lbl2data_o = EncoderOutput()\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('lbl2data', self.lbl2data_aug_meta_prefix, **kwargs)\n",
    "            lbl2data_o = encoder(data_input_ids=lbl2data_input_ids, data_attention_mask=lbl2data_attention_mask, \n",
    "                                 data_aug_meta_prefix=self.lbl2data_aug_meta_prefix, **lbl2data_meta_kwargs)\n",
    "            \n",
    "            loss = self.compute_loss(data_o.fused_rep, lbl2data_o.rep,lbl2data_data2ptr,lbl2data_idx,\n",
    "                                     plbl2data_data2ptr,plbl2data_idx)\n",
    "            \n",
    "            if self.use_query_loss:\n",
    "                loss += self.compute_loss(data_o.rep, lbl2data_o.rep,lbl2data_data2ptr,lbl2data_idx,\n",
    "                                          plbl2data_data2ptr,plbl2data_idx)\n",
    "            \n",
    "            if self.use_calib_loss:\n",
    "                loss += self.calibration_loss(data_o.fused_rep, data_o.rep, lbl2data_o.rep,lbl2data_data2ptr,lbl2data_idx,\n",
    "                                              plbl2data_data2ptr,plbl2data_idx)\n",
    "            \n",
    "            loss += self.compute_meta_loss(data_o.fused_rep, lbl2data_o.rep, **kwargs)\n",
    "            \n",
    "            if self.use_fusion_loss:\n",
    "                loss += self.compute_fusion_loss(data_o.fused_rep, data_o.meta_repr, self.data_aug_meta_prefix, **kwargs)\n",
    "                loss += self.compute_fusion_loss(lbl2data_o.rep, lbl2data_o.meta_repr, self.lbl2data_aug_meta_prefix, **kwargs)\n",
    "            \n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_o.logits,data_o.rep,data_o.fused_rep,lbl2data_o.logits,lbl2data_o.rep,lbl2data_o.fused_rep)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "        \n",
    "        \n",
    "        return RADOutput(\n",
    "            loss=loss,\n",
    "            \n",
    "            data_repr=data_o.rep,\n",
    "            data_fused_repr=data_o.fused_rep,\n",
    "            \n",
    "            lbl2data_repr=lbl2data_o.rep,\n",
    "            lbl2data_fused_repr=lbl2data_o.fused_rep,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420983e-be51-4150-a8d7-42aacd6d3f0e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab4441-1c90-4236-b2b4-94fa9d862641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cbce4-3a9c-46b2-a3c0-cae045305976",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DistilBertModel.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7e876-9691-4663-8cd0-795716fda003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RAD001(DistilBertConfig(), resize_length=5000, base_model=base_model, lora_r=8, lora_alpha=32,\n",
    "               \n",
    "               batch_size=100, num_batch_labels=5000, margin=0.3, num_negatives=10, tau=0.1, apply_softmax=True,\n",
    "                               \n",
    "               data_aug_meta_prefix='cat2data', lbl2data_aug_meta_prefix=None, data_pred_meta_prefix=None, lbl2data_pred_meta_prefix=None,\n",
    "               \n",
    "               use_query_loss=True,\n",
    "               \n",
    "               calib_margin=0.3, calib_num_negatives=5, calib_tau=0.1, calib_apply_softmax=True, calib_loss_weight=0.1,\n",
    "               use_calib_loss=False,\n",
    "               \n",
    "               meta_loss_weight=0.0, fusion_loss_weight=0.0, use_fusion_loss=False,\n",
    "               use_encoder_parallel=False)\n",
    "\n",
    "model.init_retrieval_head()\n",
    "model.init_cross_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a47d84-1938-4192-93d6-035e4261b138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5723019-5c60-47fe-a791-3a7a8674f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=[\n",
    "    'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask', \n",
    "    'cat2data_data2ptr',\n",
    "    'pcat2lbl_idx', 'pcat2lbl_lbl2data2ptr', 'pcat2lbl_data2ptr', 'cat2lbl_idx', 'cat2lbl_input_ids', \n",
    "    'cat2lbl_attention_mask', 'cat2lbl_lbl2data2ptr', 'cat2lbl_data2ptr',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ee2e5-9710-4079-b6fd-a194c5fa96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = model(**b.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84160f70-9472-4e79-a3b4-4380a9c0e263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0638, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0175a-e11c-4e43-a3cc-7b2191bcc31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4009cc7-edb7-445d-aaa3-16bbc95dad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    return model(**b.to(model.device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97d935-1b26-4fdc-badf-c6ac05598574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3657616883.py(3)func()\n",
      "      1 def func():\n",
      "      2     import pdb; pdb.set_trace()\n",
      "----> 3     return model(**b.to(model.device))\n",
      "      4 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b model.forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 1 at /tmp/ipykernel_22396/2456184125.py:151\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  b model.encoder.forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 2 at /tmp/ipykernel_22396/3427015654.py:141\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(166)forward()\n",
      "    164         **kwargs\n",
      "    165     ):  \n",
      "--> 166         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "    167 \n",
      "    168         if self.use_encoder_parallel:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(168)forward()\n",
      "    166         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "    167 \n",
      "--> 168         if self.use_encoder_parallel:\n",
      "    169             encoder = XCDataParallel(module=self.encoder)\n",
      "    170         else: encoder = self.encoder\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(170)forward()\n",
      "    168         if self.use_encoder_parallel:\n",
      "    169             encoder = XCDataParallel(module=self.encoder)\n",
      "--> 170         else: encoder = self.encoder\n",
      "    171 \n",
      "    172         data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('data', self.data_aug_meta_prefix, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(172)forward()\n",
      "    170         else: encoder = self.encoder\n",
      "    171 \n",
      "--> 172         data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('data', self.data_aug_meta_prefix, **kwargs)\n",
      "    173         data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
      "    174                          data_aug_meta_prefix=self.data_aug_meta_prefix, **data_meta_kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(173)forward()\n",
      "    171 \n",
      "    172         data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('data', self.data_aug_meta_prefix, **kwargs)\n",
      "--> 173         data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
      "    174                          data_aug_meta_prefix=self.data_aug_meta_prefix, **data_meta_kwargs)\n",
      "    175 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data_meta_kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'cat2data_input_ids': tensor([[  101,  3803,  1011,  4092,  3032,  1998,  6500,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  7139,  3032,  1997,  1996,  2983,  1997,  1996,  4549,   102,\n",
      "             0,     0,     0,     0],\n",
      "        [  101, 17867,  2575,  4487,  8583,  2696, 16558, 21808,  2015,  1999,\n",
      "          1996,  3009,  3400,   102],\n",
      "        [  101,  4331,  1997, 13491,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2111,  1997,  1996,  3146,  4329,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2142,  2163,  2390, 11593,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2111,  1997,  1996,  6646,  2162,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  5497, 17228,  1999,  2710,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  7649,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2163,  1998,  6500,  2511,  1999,  5497,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2148,  4004,  3032,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  7726,  3032,  1998,  6500,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2266,  2163,  1997,  1996,  2148,  4004,  2523,  2005,  3164,\n",
      "          6792,   102,     0,     0]]), 'cat2data_idx': tensor([161861, 161860, 161848, 138151,  81202,  68239, 102557, 102792, 102791,\n",
      "         79395,  84876,  84875,  84871]), 'cat2data_data2ptr': tensor([3, 1, 3, 3, 3])}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data_meta_kwargs.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cat2data_attention_mask', 'cat2data_input_ids', 'cat2data_idx', 'cat2data_data2ptr'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/2456184125.py(174)forward()\n",
      "    172         data_meta_kwargs = Parameters.from_feat_meta_aug_prefix('data', self.data_aug_meta_prefix, **kwargs)\n",
      "    173         data_o = encoder(data_input_ids=data_input_ids, data_attention_mask=data_attention_mask, \n",
      "--> 174                          data_aug_meta_prefix=self.data_aug_meta_prefix, **data_meta_kwargs)\n",
      "    175 \n",
      "    176 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(150)forward()\n",
      "    148         **kwargs\n",
      "    149     ):\n",
      "--> 150         data_o = self.encode(data_input_ids, data_attention_mask)\n",
      "    151 \n",
      "    152         if data_type is not None and data_type == \"meta\":\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters()]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.embeddings.word_embeddings.weight', 'base_model.model.embeddings.position_embeddings.weight', 'base_model.model.embeddings.LayerNorm.weight', 'base_model.model.embeddings.LayerNorm.bias', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.out_lin.weight', 'base_model.model.transformer.layer.0.attention.out_lin.bias', 'base_model.model.transformer.layer.0.sa_layer_norm.weight', 'base_model.model.transformer.layer.0.sa_layer_norm.bias', 'base_model.model.transformer.layer.0.ffn.lin1.weight', 'base_model.model.transformer.layer.0.ffn.lin1.bias', 'base_model.model.transformer.layer.0.ffn.lin2.weight', 'base_model.model.transformer.layer.0.ffn.lin2.bias', 'base_model.model.transformer.layer.0.output_layer_norm.weight', 'base_model.model.transformer.layer.0.output_layer_norm.bias', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.out_lin.weight', 'base_model.model.transformer.layer.1.attention.out_lin.bias', 'base_model.model.transformer.layer.1.sa_layer_norm.weight', 'base_model.model.transformer.layer.1.sa_layer_norm.bias', 'base_model.model.transformer.layer.1.ffn.lin1.weight', 'base_model.model.transformer.layer.1.ffn.lin1.bias', 'base_model.model.transformer.layer.1.ffn.lin2.weight', 'base_model.model.transformer.layer.1.ffn.lin2.bias', 'base_model.model.transformer.layer.1.output_layer_norm.weight', 'base_model.model.transformer.layer.1.output_layer_norm.bias', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.out_lin.weight', 'base_model.model.transformer.layer.2.attention.out_lin.bias', 'base_model.model.transformer.layer.2.sa_layer_norm.weight', 'base_model.model.transformer.layer.2.sa_layer_norm.bias', 'base_model.model.transformer.layer.2.ffn.lin1.weight', 'base_model.model.transformer.layer.2.ffn.lin1.bias', 'base_model.model.transformer.layer.2.ffn.lin2.weight', 'base_model.model.transformer.layer.2.ffn.lin2.bias', 'base_model.model.transformer.layer.2.output_layer_norm.weight', 'base_model.model.transformer.layer.2.output_layer_norm.bias', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.out_lin.weight', 'base_model.model.transformer.layer.3.attention.out_lin.bias', 'base_model.model.transformer.layer.3.sa_layer_norm.weight', 'base_model.model.transformer.layer.3.sa_layer_norm.bias', 'base_model.model.transformer.layer.3.ffn.lin1.weight', 'base_model.model.transformer.layer.3.ffn.lin1.bias', 'base_model.model.transformer.layer.3.ffn.lin2.weight', 'base_model.model.transformer.layer.3.ffn.lin2.bias', 'base_model.model.transformer.layer.3.output_layer_norm.weight', 'base_model.model.transformer.layer.3.output_layer_norm.bias', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.out_lin.weight', 'base_model.model.transformer.layer.4.attention.out_lin.bias', 'base_model.model.transformer.layer.4.sa_layer_norm.weight', 'base_model.model.transformer.layer.4.sa_layer_norm.bias', 'base_model.model.transformer.layer.4.ffn.lin1.weight', 'base_model.model.transformer.layer.4.ffn.lin1.bias', 'base_model.model.transformer.layer.4.ffn.lin2.weight', 'base_model.model.transformer.layer.4.ffn.lin2.bias', 'base_model.model.transformer.layer.4.output_layer_norm.weight', 'base_model.model.transformer.layer.4.output_layer_norm.bias', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.out_lin.weight', 'base_model.model.transformer.layer.5.attention.out_lin.bias', 'base_model.model.transformer.layer.5.sa_layer_norm.weight', 'base_model.model.transformer.layer.5.sa_layer_norm.bias', 'base_model.model.transformer.layer.5.ffn.lin1.weight', 'base_model.model.transformer.layer.5.ffn.lin1.bias', 'base_model.model.transformer.layer.5.ffn.lin2.weight', 'base_model.model.transformer.layer.5.ffn.lin2.bias', 'base_model.model.transformer.layer.5.output_layer_norm.weight', 'base_model.model.transformer.layer.5.output_layer_norm.bias']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(152)forward()\n",
      "    150         data_o = self.encode(data_input_ids, data_attention_mask)\n",
      "    151 \n",
      "--> 152         if data_type is not None and data_type == \"meta\":\n",
      "    153             data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
      "    154         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(155)forward()\n",
      "    153             data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
      "    154         else:\n",
      "--> 155             data_repr = self.dr(data_o[0], data_attention_mask)\n",
      "    156 \n",
      "    157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(157)forward()\n",
      "    155             data_repr = self.dr(data_o[0], data_attention_mask)\n",
      "    156 \n",
      "--> 157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "    158 \n",
      "    159         data_fused_repr = meta_repr = None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(159)forward()\n",
      "    157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "    158 \n",
      "--> 159         data_fused_repr = meta_repr = None\n",
      "    160         if data_aug_meta_prefix is not None:\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters()]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.embeddings.word_embeddings.weight', 'base_model.model.embeddings.position_embeddings.weight', 'base_model.model.embeddings.LayerNorm.weight', 'base_model.model.embeddings.LayerNorm.bias', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.out_lin.weight', 'base_model.model.transformer.layer.0.attention.out_lin.bias', 'base_model.model.transformer.layer.0.sa_layer_norm.weight', 'base_model.model.transformer.layer.0.sa_layer_norm.bias', 'base_model.model.transformer.layer.0.ffn.lin1.weight', 'base_model.model.transformer.layer.0.ffn.lin1.bias', 'base_model.model.transformer.layer.0.ffn.lin2.weight', 'base_model.model.transformer.layer.0.ffn.lin2.bias', 'base_model.model.transformer.layer.0.output_layer_norm.weight', 'base_model.model.transformer.layer.0.output_layer_norm.bias', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.out_lin.weight', 'base_model.model.transformer.layer.1.attention.out_lin.bias', 'base_model.model.transformer.layer.1.sa_layer_norm.weight', 'base_model.model.transformer.layer.1.sa_layer_norm.bias', 'base_model.model.transformer.layer.1.ffn.lin1.weight', 'base_model.model.transformer.layer.1.ffn.lin1.bias', 'base_model.model.transformer.layer.1.ffn.lin2.weight', 'base_model.model.transformer.layer.1.ffn.lin2.bias', 'base_model.model.transformer.layer.1.output_layer_norm.weight', 'base_model.model.transformer.layer.1.output_layer_norm.bias', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.out_lin.weight', 'base_model.model.transformer.layer.2.attention.out_lin.bias', 'base_model.model.transformer.layer.2.sa_layer_norm.weight', 'base_model.model.transformer.layer.2.sa_layer_norm.bias', 'base_model.model.transformer.layer.2.ffn.lin1.weight', 'base_model.model.transformer.layer.2.ffn.lin1.bias', 'base_model.model.transformer.layer.2.ffn.lin2.weight', 'base_model.model.transformer.layer.2.ffn.lin2.bias', 'base_model.model.transformer.layer.2.output_layer_norm.weight', 'base_model.model.transformer.layer.2.output_layer_norm.bias', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.out_lin.weight', 'base_model.model.transformer.layer.3.attention.out_lin.bias', 'base_model.model.transformer.layer.3.sa_layer_norm.weight', 'base_model.model.transformer.layer.3.sa_layer_norm.bias', 'base_model.model.transformer.layer.3.ffn.lin1.weight', 'base_model.model.transformer.layer.3.ffn.lin1.bias', 'base_model.model.transformer.layer.3.ffn.lin2.weight', 'base_model.model.transformer.layer.3.ffn.lin2.bias', 'base_model.model.transformer.layer.3.output_layer_norm.weight', 'base_model.model.transformer.layer.3.output_layer_norm.bias', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.out_lin.weight', 'base_model.model.transformer.layer.4.attention.out_lin.bias', 'base_model.model.transformer.layer.4.sa_layer_norm.weight', 'base_model.model.transformer.layer.4.sa_layer_norm.bias', 'base_model.model.transformer.layer.4.ffn.lin1.weight', 'base_model.model.transformer.layer.4.ffn.lin1.bias', 'base_model.model.transformer.layer.4.ffn.lin2.weight', 'base_model.model.transformer.layer.4.ffn.lin2.bias', 'base_model.model.transformer.layer.4.output_layer_norm.weight', 'base_model.model.transformer.layer.4.output_layer_norm.bias', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.out_lin.weight', 'base_model.model.transformer.layer.5.attention.out_lin.bias', 'base_model.model.transformer.layer.5.sa_layer_norm.weight', 'base_model.model.transformer.layer.5.sa_layer_norm.bias', 'base_model.model.transformer.layer.5.ffn.lin1.weight', 'base_model.model.transformer.layer.5.ffn.lin1.bias', 'base_model.model.transformer.layer.5.ffn.lin2.weight', 'base_model.model.transformer.layer.5.ffn.lin2.bias', 'base_model.model.transformer.layer.5.output_layer_norm.weight', 'base_model.model.transformer.layer.5.output_layer_norm.bias']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self._mark_only_adapters_as_trainable()\n",
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters() if p.requires_grad]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.transformer.layer.0.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(160)forward()\n",
      "    158 \n",
      "    159         data_fused_repr = meta_repr = None\n",
      "--> 160         if data_aug_meta_prefix is not None:\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "    162             if len(meta_kwargs):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(161)forward()\n",
      "    159         data_fused_repr = meta_repr = None\n",
      "    160         if data_aug_meta_prefix is not None:\n",
      "--> 161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "    162             if len(meta_kwargs):\n",
      "    163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(162)forward()\n",
      "    160         if data_aug_meta_prefix is not None:\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "--> 162             if len(meta_kwargs):\n",
      "    163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "    164                                                                              data_attention_mask,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  meta_kwargs.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cat2data'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(163)forward()\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "    162             if len(meta_kwargs):\n",
      "--> 163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "    164                                                                              data_attention_mask,\n",
      "    165                                                                              meta_kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(164)forward()\n",
      "    162             if len(meta_kwargs):\n",
      "    163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "--> 164                                                                              data_attention_mask,\n",
      "    165                                                                              meta_kwargs)\n",
      "    166                 data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(165)forward()\n",
      "    163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "    164                                                                              data_attention_mask,\n",
      "--> 165                                                                              meta_kwargs)\n",
      "    166                 data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
      "    167 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(163)forward()\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "    162             if len(meta_kwargs):\n",
      "--> 163                 data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
      "    164                                                                              data_attention_mask,\n",
      "    165                                                                              meta_kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_22396/3427015654.py(108)fuse_meta_into_embeddings()\n",
      "    106 \n",
      "    107 \n",
      "--> 108     def fuse_meta_into_embeddings(self, embed:torch.Tensor, attention_mask:torch.Tensor, meta_kwargs:Dict):\n",
      "    109         meta_repr = {}\n",
      "    110 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(109)fuse_meta_into_embeddings()\n",
      "    107 \n",
      "    108     def fuse_meta_into_embeddings(self, embed:torch.Tensor, attention_mask:torch.Tensor, meta_kwargs:Dict):\n",
      "--> 109         meta_repr = {}\n",
      "    110 \n",
      "    111         for m_key, m_args in meta_kwargs.items():\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(111)fuse_meta_into_embeddings()\n",
      "    109         meta_repr = {}\n",
      "    110 \n",
      "--> 111         for m_key, m_args in meta_kwargs.items():\n",
      "    112             idx = torch.where(m_args['data2ptr'] > 0)[0]\n",
      "    113             meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(112)fuse_meta_into_embeddings()\n",
      "    110 \n",
      "    111         for m_key, m_args in meta_kwargs.items():\n",
      "--> 112             idx = torch.where(m_args['data2ptr'] > 0)[0]\n",
      "    113             meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
      "    114 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(113)fuse_meta_into_embeddings()\n",
      "    111         for m_key, m_args in meta_kwargs.items():\n",
      "    112             idx = torch.where(m_args['data2ptr'] > 0)[0]\n",
      "--> 113             meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
      "    114 \n",
      "    115             self.distilbert.set_adapter(m_key)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(115)fuse_meta_into_embeddings()\n",
      "    113             meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
      "    114 \n",
      "--> 115             self.distilbert.set_adapter(m_key)\n",
      "    116 \n",
      "    117             if len(idx):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  m_key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cat2data'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(117)fuse_meta_into_embeddings()\n",
      "    115             self.distilbert.set_adapter(m_key)\n",
      "    116 \n",
      "--> 117             if len(idx):\n",
      "    118                 if 'meta_repr' in m_args:\n",
      "    119                     m_repr,m_repr_mask = m_args['meta_repr'],torch.any(m_args['attention_mask'], dim=1).long().view(-1,1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters() if p.requires_grad]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.distilbert.active_adapters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat2data']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(118)fuse_meta_into_embeddings()\n",
      "    116 \n",
      "    117             if len(idx):\n",
      "--> 118                 if 'meta_repr' in m_args:\n",
      "    119                     m_repr,m_repr_mask = m_args['meta_repr'],torch.any(m_args['attention_mask'], dim=1).long().view(-1,1)\n",
      "    120                     m_repr,m_repr_mask = self.resize(m_repr, m_repr_mask, m_args['data2ptr'][idx])\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(123)fuse_meta_into_embeddings()\n",
      "    121                     m_repr_mask = m_repr_mask.bool()\n",
      "    122                 else:\n",
      "--> 123                     m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], \n",
      "    124                                                                 m_args['data2ptr'][idx])\n",
      "    125                     n_meta = m_args['data2ptr'].max()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(124)fuse_meta_into_embeddings()\n",
      "    122                 else:\n",
      "    123                     m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], \n",
      "--> 124                                                                 m_args['data2ptr'][idx])\n",
      "    125                     n_meta = m_args['data2ptr'].max()\n",
      "    126 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(123)fuse_meta_into_embeddings()\n",
      "    121                     m_repr_mask = m_repr_mask.bool()\n",
      "    122                 else:\n",
      "--> 123                     m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], \n",
      "    124                                                                 m_args['data2ptr'][idx])\n",
      "    125                     n_meta = m_args['data2ptr'].max()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(125)fuse_meta_into_embeddings()\n",
      "    123                     m_input_ids, m_attention_mask = self.resize(m_args['input_ids'], m_args['attention_mask'], \n",
      "    124                                                                 m_args['data2ptr'][idx])\n",
      "--> 125                     n_meta = m_args['data2ptr'].max()\n",
      "    126 \n",
      "    127                     m_embed = self.encode(m_input_ids, m_attention_mask)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(127)fuse_meta_into_embeddings()\n",
      "    125                     n_meta = m_args['data2ptr'].max()\n",
      "    126 \n",
      "--> 127                     m_embed = self.encode(m_input_ids, m_attention_mask)[0]\n",
      "    128 \n",
      "    129                     m_repr = self.meta_unnormalized(m_embed, m_attention_mask)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(129)fuse_meta_into_embeddings()\n",
      "    127                     m_embed = self.encode(m_input_ids, m_attention_mask)[0]\n",
      "    128 \n",
      "--> 129                     m_repr = self.meta_unnormalized(m_embed, m_attention_mask)\n",
      "    130                     m_repr_mask = torch.any(m_attention_mask, dim=1)\n",
      "    131 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(130)fuse_meta_into_embeddings()\n",
      "    128 \n",
      "    129                     m_repr = self.meta_unnormalized(m_embed, m_attention_mask)\n",
      "--> 130                     m_repr_mask = torch.any(m_attention_mask, dim=1)\n",
      "    131 \n",
      "    132                 m_repr, m_repr_mask = m_repr.view(len(idx), -1, self.config.dim), m_repr_mask.view(len(idx), -1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(132)fuse_meta_into_embeddings()\n",
      "    130                     m_repr_mask = torch.any(m_attention_mask, dim=1)\n",
      "    131 \n",
      "--> 132                 m_repr, m_repr_mask = m_repr.view(len(idx), -1, self.config.dim), m_repr_mask.view(len(idx), -1)\n",
      "    133 \n",
      "    134                 meta_repr[m_key] = F.normalize(m_repr[m_repr_mask], dim=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(134)fuse_meta_into_embeddings()\n",
      "    132                 m_repr, m_repr_mask = m_repr.view(len(idx), -1, self.config.dim), m_repr_mask.view(len(idx), -1)\n",
      "    133 \n",
      "--> 134                 meta_repr[m_key] = F.normalize(m_repr[m_repr_mask], dim=1)\n",
      "    135 \n",
      "    136                 fused_embed = self.cross_head(embed[idx], attention_mask[idx], m_repr, m_repr_mask)[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(136)fuse_meta_into_embeddings()\n",
      "    134                 meta_repr[m_key] = F.normalize(m_repr[m_repr_mask], dim=1)\n",
      "    135 \n",
      "--> 136                 fused_embed = self.cross_head(embed[idx], attention_mask[idx], m_repr, m_repr_mask)[0]\n",
      "    137                 embed[idx] += fused_embed\n",
      "    138 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(137)fuse_meta_into_embeddings()\n",
      "    135 \n",
      "    136                 fused_embed = self.cross_head(embed[idx], attention_mask[idx], m_repr, m_repr_mask)[0]\n",
      "--> 137                 embed[idx] += fused_embed\n",
      "    138 \n",
      "    139         return embed, meta_repr\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(111)fuse_meta_into_embeddings()\n",
      "    109         meta_repr = {}\n",
      "    110 \n",
      "--> 111         for m_key, m_args in meta_kwargs.items():\n",
      "    112             idx = torch.where(m_args['data2ptr'] > 0)[0]\n",
      "    113             meta_repr[m_key] = torch.empty(0, self.config.dim).to(embed)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(139)fuse_meta_into_embeddings()\n",
      "    137                 embed[idx] += fused_embed\n",
      "    138 \n",
      "--> 139         return embed, meta_repr\n",
      "    140 \n",
      "2   141     def forward(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "(tensor([[[-1....PutBackward0>), {'cat2data': tensor([[-0.0...DivBackward0>)})\n",
      "> /tmp/ipykernel_22396/3427015654.py(139)fuse_meta_into_embeddings()\n",
      "    137                 embed[idx] += fused_embed\n",
      "    138 \n",
      "--> 139         return embed, meta_repr\n",
      "    140 \n",
      "2   141     def forward(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(166)forward()\n",
      "    164                                                                              data_attention_mask,\n",
      "    165                                                                              meta_kwargs)\n",
      "--> 166                 data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
      "    167 \n",
      "    168         if self.training:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(168)forward()\n",
      "    166                 data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
      "    167 \n",
      "--> 168         if self.training:\n",
      "    169             self.distilbert.set_adapter('lbl2data')\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(169)forward()\n",
      "    167 \n",
      "    168         if self.training:\n",
      "--> 169             self.distilbert.set_adapter('lbl2data')\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(170)forward()\n",
      "    168         if self.training:\n",
      "    169             self.distilbert.set_adapter('lbl2data')\n",
      "--> 170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "    172         return EncoderOutput(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(172)forward()\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "--> 172         return EncoderOutput(\n",
      "    173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters() if p.requires_grad]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.embeddings.word_embeddings.weight', 'base_model.model.embeddings.position_embeddings.weight', 'base_model.model.embeddings.LayerNorm.weight', 'base_model.model.embeddings.LayerNorm.bias', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.out_lin.weight', 'base_model.model.transformer.layer.0.attention.out_lin.bias', 'base_model.model.transformer.layer.0.sa_layer_norm.weight', 'base_model.model.transformer.layer.0.sa_layer_norm.bias', 'base_model.model.transformer.layer.0.ffn.lin1.weight', 'base_model.model.transformer.layer.0.ffn.lin1.bias', 'base_model.model.transformer.layer.0.ffn.lin2.weight', 'base_model.model.transformer.layer.0.ffn.lin2.bias', 'base_model.model.transformer.layer.0.output_layer_norm.weight', 'base_model.model.transformer.layer.0.output_layer_norm.bias', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.out_lin.weight', 'base_model.model.transformer.layer.1.attention.out_lin.bias', 'base_model.model.transformer.layer.1.sa_layer_norm.weight', 'base_model.model.transformer.layer.1.sa_layer_norm.bias', 'base_model.model.transformer.layer.1.ffn.lin1.weight', 'base_model.model.transformer.layer.1.ffn.lin1.bias', 'base_model.model.transformer.layer.1.ffn.lin2.weight', 'base_model.model.transformer.layer.1.ffn.lin2.bias', 'base_model.model.transformer.layer.1.output_layer_norm.weight', 'base_model.model.transformer.layer.1.output_layer_norm.bias', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.out_lin.weight', 'base_model.model.transformer.layer.2.attention.out_lin.bias', 'base_model.model.transformer.layer.2.sa_layer_norm.weight', 'base_model.model.transformer.layer.2.sa_layer_norm.bias', 'base_model.model.transformer.layer.2.ffn.lin1.weight', 'base_model.model.transformer.layer.2.ffn.lin1.bias', 'base_model.model.transformer.layer.2.ffn.lin2.weight', 'base_model.model.transformer.layer.2.ffn.lin2.bias', 'base_model.model.transformer.layer.2.output_layer_norm.weight', 'base_model.model.transformer.layer.2.output_layer_norm.bias', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.out_lin.weight', 'base_model.model.transformer.layer.3.attention.out_lin.bias', 'base_model.model.transformer.layer.3.sa_layer_norm.weight', 'base_model.model.transformer.layer.3.sa_layer_norm.bias', 'base_model.model.transformer.layer.3.ffn.lin1.weight', 'base_model.model.transformer.layer.3.ffn.lin1.bias', 'base_model.model.transformer.layer.3.ffn.lin2.weight', 'base_model.model.transformer.layer.3.ffn.lin2.bias', 'base_model.model.transformer.layer.3.output_layer_norm.weight', 'base_model.model.transformer.layer.3.output_layer_norm.bias', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.out_lin.weight', 'base_model.model.transformer.layer.4.attention.out_lin.bias', 'base_model.model.transformer.layer.4.sa_layer_norm.weight', 'base_model.model.transformer.layer.4.sa_layer_norm.bias', 'base_model.model.transformer.layer.4.ffn.lin1.weight', 'base_model.model.transformer.layer.4.ffn.lin1.bias', 'base_model.model.transformer.layer.4.ffn.lin2.weight', 'base_model.model.transformer.layer.4.ffn.lin2.bias', 'base_model.model.transformer.layer.4.output_layer_norm.weight', 'base_model.model.transformer.layer.4.output_layer_norm.bias', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.out_lin.weight', 'base_model.model.transformer.layer.5.attention.out_lin.bias', 'base_model.model.transformer.layer.5.sa_layer_norm.weight', 'base_model.model.transformer.layer.5.sa_layer_norm.bias', 'base_model.model.transformer.layer.5.ffn.lin1.weight', 'base_model.model.transformer.layer.5.ffn.lin1.bias', 'base_model.model.transformer.layer.5.ffn.lin2.weight', 'base_model.model.transformer.layer.5.ffn.lin2.bias', 'base_model.model.transformer.layer.5.output_layer_norm.weight', 'base_model.model.transformer.layer.5.output_layer_norm.bias']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.distilbert.active_adapters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lbl2data']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(150)forward()\n",
      "    148         **kwargs\n",
      "    149     ):\n",
      "--> 150         data_o = self.encode(data_input_ids, data_attention_mask)\n",
      "    151 \n",
      "    152         if data_type is not None and data_type == \"meta\":\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(152)forward()\n",
      "    150         data_o = self.encode(data_input_ids, data_attention_mask)\n",
      "    151 \n",
      "--> 152         if data_type is not None and data_type == \"meta\":\n",
      "    153             data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
      "    154         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xx = [n for n,p in self.distilbert.named_parameters() if p.requires_grad]\n",
      "ipdb>  xx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.embeddings.word_embeddings.weight', 'base_model.model.embeddings.position_embeddings.weight', 'base_model.model.embeddings.LayerNorm.weight', 'base_model.model.embeddings.LayerNorm.bias', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.0.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.0.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.0.attention.out_lin.weight', 'base_model.model.transformer.layer.0.attention.out_lin.bias', 'base_model.model.transformer.layer.0.sa_layer_norm.weight', 'base_model.model.transformer.layer.0.sa_layer_norm.bias', 'base_model.model.transformer.layer.0.ffn.lin1.weight', 'base_model.model.transformer.layer.0.ffn.lin1.bias', 'base_model.model.transformer.layer.0.ffn.lin2.weight', 'base_model.model.transformer.layer.0.ffn.lin2.bias', 'base_model.model.transformer.layer.0.output_layer_norm.weight', 'base_model.model.transformer.layer.0.output_layer_norm.bias', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.1.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.1.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.1.attention.out_lin.weight', 'base_model.model.transformer.layer.1.attention.out_lin.bias', 'base_model.model.transformer.layer.1.sa_layer_norm.weight', 'base_model.model.transformer.layer.1.sa_layer_norm.bias', 'base_model.model.transformer.layer.1.ffn.lin1.weight', 'base_model.model.transformer.layer.1.ffn.lin1.bias', 'base_model.model.transformer.layer.1.ffn.lin2.weight', 'base_model.model.transformer.layer.1.ffn.lin2.bias', 'base_model.model.transformer.layer.1.output_layer_norm.weight', 'base_model.model.transformer.layer.1.output_layer_norm.bias', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.2.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.2.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.2.attention.out_lin.weight', 'base_model.model.transformer.layer.2.attention.out_lin.bias', 'base_model.model.transformer.layer.2.sa_layer_norm.weight', 'base_model.model.transformer.layer.2.sa_layer_norm.bias', 'base_model.model.transformer.layer.2.ffn.lin1.weight', 'base_model.model.transformer.layer.2.ffn.lin1.bias', 'base_model.model.transformer.layer.2.ffn.lin2.weight', 'base_model.model.transformer.layer.2.ffn.lin2.bias', 'base_model.model.transformer.layer.2.output_layer_norm.weight', 'base_model.model.transformer.layer.2.output_layer_norm.bias', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.3.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.3.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.3.attention.out_lin.weight', 'base_model.model.transformer.layer.3.attention.out_lin.bias', 'base_model.model.transformer.layer.3.sa_layer_norm.weight', 'base_model.model.transformer.layer.3.sa_layer_norm.bias', 'base_model.model.transformer.layer.3.ffn.lin1.weight', 'base_model.model.transformer.layer.3.ffn.lin1.bias', 'base_model.model.transformer.layer.3.ffn.lin2.weight', 'base_model.model.transformer.layer.3.ffn.lin2.bias', 'base_model.model.transformer.layer.3.output_layer_norm.weight', 'base_model.model.transformer.layer.3.output_layer_norm.bias', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.4.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.4.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.4.attention.out_lin.weight', 'base_model.model.transformer.layer.4.attention.out_lin.bias', 'base_model.model.transformer.layer.4.sa_layer_norm.weight', 'base_model.model.transformer.layer.4.sa_layer_norm.bias', 'base_model.model.transformer.layer.4.ffn.lin1.weight', 'base_model.model.transformer.layer.4.ffn.lin1.bias', 'base_model.model.transformer.layer.4.ffn.lin2.weight', 'base_model.model.transformer.layer.4.ffn.lin2.bias', 'base_model.model.transformer.layer.4.output_layer_norm.weight', 'base_model.model.transformer.layer.4.output_layer_norm.bias', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.q_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.q_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.k_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.k_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.weight', 'base_model.model.transformer.layer.5.attention.v_lin.base_layer.bias', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_A.cat2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.lbl2data.weight', 'base_model.model.transformer.layer.5.attention.v_lin.lora_B.cat2data.weight', 'base_model.model.transformer.layer.5.attention.out_lin.weight', 'base_model.model.transformer.layer.5.attention.out_lin.bias', 'base_model.model.transformer.layer.5.sa_layer_norm.weight', 'base_model.model.transformer.layer.5.sa_layer_norm.bias', 'base_model.model.transformer.layer.5.ffn.lin1.weight', 'base_model.model.transformer.layer.5.ffn.lin1.bias', 'base_model.model.transformer.layer.5.ffn.lin2.weight', 'base_model.model.transformer.layer.5.ffn.lin2.bias', 'base_model.model.transformer.layer.5.output_layer_norm.weight', 'base_model.model.transformer.layer.5.output_layer_norm.bias']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(155)forward()\n",
      "    153             data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
      "    154         else:\n",
      "--> 155             data_repr = self.dr(data_o[0], data_attention_mask)\n",
      "    156 \n",
      "    157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(157)forward()\n",
      "    155             data_repr = self.dr(data_o[0], data_attention_mask)\n",
      "    156 \n",
      "--> 157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "    158 \n",
      "    159         data_fused_repr = meta_repr = None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(159)forward()\n",
      "    157         if self.training: self._mark_only_adapters_as_trainable()\n",
      "    158 \n",
      "--> 159         data_fused_repr = meta_repr = None\n",
      "    160         if data_aug_meta_prefix is not None:\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(160)forward()\n",
      "    158 \n",
      "    159         data_fused_repr = meta_repr = None\n",
      "--> 160         if data_aug_meta_prefix is not None:\n",
      "    161             meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
      "    162             if len(meta_kwargs):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(168)forward()\n",
      "    166                 data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
      "    167 \n",
      "--> 168         if self.training:\n",
      "    169             self.distilbert.set_adapter('lbl2data')\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(169)forward()\n",
      "    167 \n",
      "    168         if self.training:\n",
      "--> 169             self.distilbert.set_adapter('lbl2data')\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(170)forward()\n",
      "    168         if self.training:\n",
      "    169             self.distilbert.set_adapter('lbl2data')\n",
      "--> 170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "    172         return EncoderOutput(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(172)forward()\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "--> 172         return EncoderOutput(\n",
      "    173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(173)forward()\n",
      "    171 \n",
      "    172         return EncoderOutput(\n",
      "--> 173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "    175             meta_repr=meta_repr,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(174)forward()\n",
      "    172         return EncoderOutput(\n",
      "    173             rep=data_repr,\n",
      "--> 174             fused_rep=data_fused_repr,\n",
      "    175             meta_repr=meta_repr,\n",
      "    176         )\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(175)forward()\n",
      "    173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "--> 175             meta_repr=meta_repr,\n",
      "    176         )\n",
      "    177 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22396/3427015654.py(172)forward()\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "--> 172         return EncoderOutput(\n",
      "    173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "EncoderOutput...eta_repr=None)\n",
      "> /tmp/ipykernel_22396/3427015654.py(172)forward()\n",
      "    170             self._mark_entire_encoder_as_trainable()\n",
      "    171 \n",
      "--> 172         return EncoderOutput(\n",
      "    173             rep=data_repr,\n",
      "    174             fused_rep=data_fused_repr,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RADOutput(loss=tensor(0.0592, grad_fn=<AddBackward0>), logits=None, data_repr=tensor([[-0.0355, -0.0144, -0.0295,  ...,  0.0476,  0.0050, -0.0291],\n",
       "        [-0.0242, -0.0198, -0.0323,  ...,  0.0314, -0.0336,  0.0125],\n",
       "        [ 0.0014,  0.0163, -0.0380,  ...,  0.0723, -0.0179,  0.0352],\n",
       "        [ 0.0704,  0.0167,  0.0023,  ...,  0.0334, -0.0045,  0.0316],\n",
       "        [-0.0218, -0.0240, -0.0293,  ...,  0.0250, -0.0241, -0.0328]],\n",
       "       grad_fn=<DivBackward0>), data_fused_repr=tensor([[-2.3747e-02, -1.5425e-02, -2.3315e-02,  ...,  2.7843e-02,\n",
       "         -9.0777e-03, -2.3724e-02],\n",
       "        [-1.9169e-02, -1.8406e-02, -2.0055e-02,  ...,  2.4618e-02,\n",
       "         -1.9030e-02, -5.9703e-05],\n",
       "        [-1.3714e-02,  3.5124e-03, -2.3359e-02,  ...,  1.6399e-02,\n",
       "         -2.4352e-02,  3.2127e-03],\n",
       "        [ 8.0003e-02,  7.5907e-03, -1.1098e-02,  ...,  4.2235e-02,\n",
       "         -1.7850e-02,  3.7884e-02],\n",
       "        [-2.4064e-02, -2.0427e-02, -1.9721e-02,  ...,  1.0924e-02,\n",
       "         -1.3290e-02, -2.3074e-02]], grad_fn=<DivBackward0>), lbl2data_repr=tensor([[-0.0361, -0.0120, -0.0351,  ...,  0.0376,  0.0508,  0.0069],\n",
       "        [-0.0247, -0.0194, -0.0323,  ...,  0.0331, -0.0337,  0.0138],\n",
       "        [ 0.0174,  0.0030, -0.0333,  ...,  0.0005, -0.0298,  0.0626],\n",
       "        [ 0.0393,  0.0082,  0.0729,  ..., -0.0266, -0.0162,  0.0747],\n",
       "        [ 0.0299,  0.0185,  0.0083,  ...,  0.0263, -0.0114,  0.0083]],\n",
       "       grad_fn=<DivBackward0>), lbl2data_fused_repr=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [... skipped 1 hidden frame]\n",
      "\n",
      "    [... skipped 1 hidden frame]\n",
      "\n",
      "    [... skipped 1 hidden frame]\n",
      "\n",
      "    [... skipped 1 hidden frame]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d462d-f5b4-4770-931e-d9c517146087",
   "metadata": {},
   "source": [
    "## `RAD002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff121ba-d820-42d9-a5e2-bc332fc18675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder002(Encoder):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        config:PretrainedConfig,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        data_input_ids: torch.Tensor, \n",
    "        data_attention_mask: torch.Tensor,\n",
    "        data_aug_meta_prefix: Optional[str]=None,\n",
    "        data_type:Optional[str]=None,\n",
    "        data_unnormalized:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        data_o = self.encode(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        if data_type is not None and data_type == \"meta\":\n",
    "            data_repr = self.meta_unnormalized(data_o[0], data_attention_mask) if data_unnormalized else self.meta(data_o[0], data_attention_mask)\n",
    "        else: \n",
    "            data_repr = self.dr(data_o[0], data_attention_mask)\n",
    "            \n",
    "        data_fused_repr = meta_repr = None\n",
    "        if data_aug_meta_prefix is not None:\n",
    "            meta_kwargs = Parameters.from_meta_aug_prefix(data_aug_meta_prefix, **kwargs)\n",
    "            if len(meta_kwargs):\n",
    "                data_fused_embed, meta_repr = self.fuse_meta_into_embeddings(data_o[0], \n",
    "                                                                             data_attention_mask, \n",
    "                                                                             meta_kwargs)\n",
    "                data_fused_repr = self.dr_fused(data_fused_embed, data_attention_mask)\n",
    "\n",
    "                self.distilbert.set_adapter('lbl2data')\n",
    "        \n",
    "        return EncoderOutput(\n",
    "            rep=data_repr,\n",
    "            fused_rep=data_fused_repr,\n",
    "            meta_repr=meta_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6baf34-a280-4ced-9c6f-4f19204bba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RAD002(RAD001):\n",
    "    \n",
    "    def __init__(\n",
    "        self, config,\n",
    "\n",
    "        base_model:nn.Module, \n",
    "        resize_length:Optional[int]=None,\n",
    "        lora_r:Optional[int]=8,\n",
    "        lora_alpha:Optional[int]=32,\n",
    "\n",
    "        data_aug_meta_prefix:Optional[str]=None, \n",
    "        lbl2data_aug_meta_prefix:Optional[str]=None, \n",
    "        \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, base_model=base_model, resize_length=resize_length, lora_r=lora_r, lora_alpha=lora_alpha, \n",
    "                         data_aug_meta_prefix=data_aug_meta_prefix, lbl2data_aug_meta_prefix=lbl2data_aug_meta_prefix, **kwargs)\n",
    "        \n",
    "        self.encoder = Encoder002(config, base_model=base_model, resize_length=resize_length, lora_r=lora_r, lora_alpha=lora_alpha, \n",
    "                                  data_aug_meta_prefix=data_aug_meta_prefix, lbl2data_aug_meta_prefix=lbl2data_aug_meta_prefix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4b450-6a10-420a-9983-2973d43f00bc",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d1cdb-e1d8-4d5a-805a-5a0d0a09f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b846c-7071-45cb-9b75-5d251bfb4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DistilBertModel.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e462fce-014d-4bf8-8810-d2d3f0fb7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RAD002(DistilBertConfig(), resize_length=5000, base_model=base_model, lora_r=8, lora_alpha=32,\n",
    "               \n",
    "               batch_size=100, num_batch_labels=5000, margin=0.3, num_negatives=10, tau=0.1, apply_softmax=True,\n",
    "                               \n",
    "               data_aug_meta_prefix='cat2data', lbl2data_aug_meta_prefix=None, data_pred_meta_prefix=None, lbl2data_pred_meta_prefix=None,\n",
    "               \n",
    "               use_query_loss=True,\n",
    "               \n",
    "               calib_margin=0.3, calib_num_negatives=5, calib_tau=0.1, calib_apply_softmax=True, calib_loss_weight=0.1,\n",
    "               use_calib_loss=False,\n",
    "               \n",
    "               meta_loss_weight=0.0, fusion_loss_weight=0.0, use_fusion_loss=False,\n",
    "               use_encoder_parallel=False)\n",
    "\n",
    "model.init_retrieval_head()\n",
    "model.init_cross_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce618f-ef17-42d4-a65a-2a25914d1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=[\n",
    "    'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask', \n",
    "    'cat2data_data2ptr',\n",
    "    'pcat2lbl_idx', 'pcat2lbl_lbl2data2ptr', 'pcat2lbl_data2ptr', 'cat2lbl_idx', 'cat2lbl_input_ids', \n",
    "    'cat2lbl_attention_mask', 'cat2lbl_lbl2data2ptr', 'cat2lbl_data2ptr',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1650ec6-2e89-4f6c-8a67-db54a406e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = model(**b.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51d3a9-cfa9-42c9-aa54-b3fce2d7283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0643, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9f9de-f719-447b-b2fd-0762570e39c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ba025-5a93-4a0a-a095-b1d7617b19bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5b153-a686-4c7e-8c2d-94cfafc1d3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
