{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8c456-5339-43aa-baed-b9eb2114b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.MMM00X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f76551-93c3-40cf-915f-5a4d3c04cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc422-47c6-4c49-a719-e83d23acf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any\n",
    "from transformers import (\n",
    "    BertLMHeadModel, \n",
    "    BatchEncoding, \n",
    "    BertPreTrainedModel, \n",
    "    BertModel, \n",
    "    RobertaForCausalLM, \n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertModel,\n",
    "    DistilBertPreTrainedModel,\n",
    ")\n",
    "from transformers.utils.generic import ModelOutput\n",
    "\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae88304-b2a5-4988-b6e8-d30a2bd79f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "from xcai.block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3b726-0f16-46f1-ae48-b34febc784b0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f118491-1eb9-4e38-a63f-395612689d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# block = XCBlock.from_cfg('train', tokz='bert-base-uncased')\n",
    "block = XCBlock.from_cfg('train', tokz='distilbert/distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734cfb6-162b-42b7-8ddb-f5a85bc3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 20\n",
    "batch = block.train.one_batch(bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8142ce8-4ddd-4ab7-b791-7343d1ae64f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ace8df-a4bb-4f06-9f12-eb703f3cab3b",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d52e6c-596d-4b1e-9ffc-497fd58fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class XCModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    lm_loss: Optional[torch.FloatTensor] = None\n",
    "    dr_loss: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74690ee7-2bd5-469a-9b1e-547a5399c1d3",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab186b-0be7-45d0-a10e-762f41fb46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pooling:\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(data_embeds:torch.FloatTensor, data_attention_mask:torch.LongTensor):\n",
    "        data_attention_mask = data_attention_mask.unsqueeze(2).expand(data_embeds.size()).float()\n",
    "        return torch.sum(data_embeds * data_attention_mask, 1) / torch.clamp(data_attention_mask.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0dbd5-94f0-4da3-bff8-224f68db6330",
   "metadata": {},
   "source": [
    "## XCModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04965f07-b85e-4146-9bb7-b2f7e19d5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XCModel(BertLMHeadModel):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf63b9-b42e-48ef-afc0-5f7e4e56d8bb",
   "metadata": {},
   "source": [
    "## BT0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5948a-5fec-4663-b4a4-2d75387023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0001(BertLMHeadModel):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        data_logits = self.cls(data_o[0])\n",
    "        data_repr = data_o[0].mean(dim=1)\n",
    "        \n",
    "        if lbl2data_input_ids is not None and lbl2data_data2ptr is not None:\n",
    "            lbl2data_o = self.bert(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict\n",
    "            )\n",
    "            lbl2data_repr = lbl2data_o[0].mean(dim=1)\n",
    "            return data_logits, lbl2data_input_ids, lbl2data_data2ptr, lbl2data_idx, data_repr, lbl2data_repr, kwargs\n",
    "\n",
    "        return data_logits, lbl2data_input_ids, lbl2data_data2ptr, lbl2data_idx, kwargs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b57869-5bf0-4b58-b782-57c436150913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "m = BT0001.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43542a-c62b-4189-b4b4-3522983a43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083cc97-7978-4450-b9c2-e9dc868d99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 18, 30522])\n",
      "torch.Size([37, 16])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 768])\n",
      "torch.Size([37, 768])\n"
     ]
    }
   ],
   "source": [
    "for o in out: print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f2018-78bd-4216-b25d-210994859696",
   "metadata": {},
   "source": [
    "## BT0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2252a1-c6b2-4a21-a6d7-83f6b295c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0002(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,False \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e45035-b68c-4da3-bed5-dbfa547c0e55",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3c66a-017e-429b-931f-221329b04e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565efc8-ca0a-4ab5-ac81-815b4941d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc2bb0-c675-460c-8ff6-abe4e26dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274c98-da35-4b9d-8f12-405117ecca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50b8fa-3ece-4f98-9556-c8b4c98c1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.2184, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f7667-a0be-47c3-abe2-b3d3eb7fd35a",
   "metadata": {},
   "source": [
    "## BT0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cfb86-7439-443b-a494-220c67f8a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0003(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.8,\n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, ig_tok=ig_tok, reduce='mean')\n",
    "        self.post_init()\n",
    "\n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, Pooling.mean_pooling(o[0], attention_mask)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(data_input_ids, \n",
    "                                          data_attention_mask, \n",
    "                                          data_token_type_ids, \n",
    "                                          output_attentions=output_attentions, \n",
    "                                          output_hidden_states=output_hidden_states,\n",
    "                                          return_dict=return_dict)\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(lbl2data_input_ids, \n",
    "                                                      lbl2data_attention_mask, \n",
    "                                                      lbl2data_token_type_ids, \n",
    "                                                      output_attentions=output_attentions, \n",
    "                                                      output_hidden_states=output_hidden_states,\n",
    "                                                      return_dict=return_dict)\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb35ee-a55e-4567-8416-3b1b08d7a695",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb553f-30cd-430e-b71e-7c4d8a6b70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BT0003 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0003.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa33df-0968-4c9b-88f8-588855ea91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f649083-ff41-4718-ad01-e1c5f7b5271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b92693-31e7-4df3-83b9-9d80bc1bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bab768-2fe2-4855-b517-87604c84a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659148e-9406-47c5-b744-bad69c721aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032054db-666b-4bcd-aaf5-7ecbdf54a7aa",
   "metadata": {},
   "source": [
    "## BT0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056009-fe22-400b-a10d-1bf379a53ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0004(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,True \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 lw:Optional[int]=0.5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.lw, self.dr_loss_fn = lw, SoupCon(bsz=bsz, reduce='mean')\n",
    "        self.lm_loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, F.normalize(o[0].mean(dim=1), dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss, lm_loss, dr_loss, lbl2data_repr = None, None, None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "            lm_loss = self.lm_loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr)\n",
    "            dr_loss = self.dr_loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, **kwargs)\n",
    "            loss = lm_loss + self.lw*dr_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdffa25-f966-46d2-bb01-3cb7502dd180",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffd983-5470-41bd-9530-3f32464d6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6a2e5-f5f8-4d60-85c9-65802898bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = b['data_input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28092e0b-b7df-4d2d-99b2-ab47fbe6c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0004 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dr_loss_fn.t', 'lm_loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0004.from_pretrained('bert-base-uncased', lw=0.5, bsz=bsz, tn_targ=10_000, ig_tok=0)\n",
    "m, b = m.to('cuda'), b.to('cuda')\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751d634-8bd0-44ef-8193-3f5f72614925",
   "metadata": {},
   "source": [
    "## RT0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a5d32-cd5d-40f2-b5e0-49c746ebd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RT0005(RobertaForCausalLM):\n",
    "    use_generation,use_representation = True,False \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.roberta(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.lm_head(data_o[0])\n",
    "        \n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451df7a-7cba-48cb-bdd0-4e1a2c41bd41",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f820-fcc4-43b6-8c19-c2dc4f25e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RT0005 were not initialized from the model checkpoint at roberta-base and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = RT0005.from_pretrained('roberta-base', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb3894-846a-4469-8d8b-a678cf241dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fe2b2-83ec-4fe7-990b-c5ac27d03e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401a949-9d60-4e7e-8f28-ed8402c4b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6a870-c6fa-4292-8d0c-2c8fdd930907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.3047, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4b0d1-3e53-4104-8ef8-645a117df46e",
   "metadata": {},
   "source": [
    "## BT0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538bea7-3016-47a9-97f3-0721b0ef70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0006(BT0002):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = SoupCon(bsz=bsz, reduce='mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92254fc-edee-4dc3-ad68-afaf39af898f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380f86d-3b53-4a6a-b9bc-ac7ae37e35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BT0006 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.t']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0006.from_pretrained('bert-base-uncased', bsz=bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e8219-358b-4850-aa27-bd75bc3f9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662472f-145c-4876-8ed6-144d9dfc89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691091a-c4f4-4bc7-b4d3-4b7e9daf4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce35d5-9fc8-4679-9ae3-308490ef71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0530, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94731a98-20f1-4174-a257-65d8ceff067a",
   "metadata": {},
   "source": [
    "## DBT007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d6e93-0608-44a8-82e3-d502ff5d93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT007(DistilBertForMaskedLM):\n",
    "    use_generation,use_representation = True,False \n",
    "    \n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0):\n",
    "        super().__init__(config)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        data_o = self.distilbert(\n",
    "            input_ids=data_input_ids,\n",
    "            attention_mask=data_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hs = data_o[0]\n",
    "        data_logits = self.vocab_transform(hs)\n",
    "        data_logits = self.activation(data_logits)\n",
    "        data_logits = self.vocab_layer_norm(data_logits)\n",
    "        data_logits = self.vocab_projector(data_logits)\n",
    "\n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d243fbd-894b-4728-a822-125364de7f40",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09538455-b52b-434c-9603-3c450881597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBT007.from_pretrained('distilbert-base-uncased', ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938e939-4d6b-4cbd-ad01-9d9d0d1cf364",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629afdde-6d2b-43ea-944c-f13c77d94d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCModelOutput(loss=tensor(15.3677, grad_fn=<SumBackward0>), logits=tensor([[[ -6.1614,  -6.1267,  -6.1175,  ...,  -5.4209,  -5.2192,  -3.5195],\n",
       "         [-13.9941, -14.0756, -13.8388,  ..., -12.1233, -11.9061,  -9.5880],\n",
       "         [ -5.8230,  -5.9046,  -5.8874,  ...,  -5.2475,  -4.5833,  -5.0284],\n",
       "         ...,\n",
       "         [ -6.8535,  -7.2897,  -6.7291,  ...,  -6.0015,  -8.1558,  -4.2692],\n",
       "         [ -6.6621,  -7.0612,  -6.3850,  ...,  -5.4423,  -8.1755,  -2.0556],\n",
       "         [ -6.4021,  -6.5815,  -5.9944,  ...,  -4.7292,  -7.1593,  -2.0416]],\n",
       "\n",
       "        [[ -6.0825,  -6.0533,  -6.0468,  ...,  -5.3831,  -5.2671,  -3.3423],\n",
       "         [ -8.2382,  -8.3393,  -8.1442,  ...,  -7.4286,  -7.2308,  -5.8274],\n",
       "         [-11.7931, -11.6936, -11.6976,  ..., -10.0092, -10.2047,  -7.6660],\n",
       "         ...,\n",
       "         [ -6.2854,  -6.4565,  -6.1392,  ...,  -6.3359,  -6.2492,  -5.3608],\n",
       "         [ -6.8786,  -7.0738,  -6.7573,  ...,  -6.6838,  -7.0853,  -4.0643],\n",
       "         [ -5.4824,  -5.5097,  -5.3867,  ...,  -5.1087,  -5.5105,  -0.5052]],\n",
       "\n",
       "        [[ -6.1706,  -6.1630,  -6.1429,  ...,  -5.3969,  -5.2062,  -3.2983],\n",
       "         [-12.4496, -12.6647, -12.5046,  ..., -11.0783, -10.4722, -10.5690],\n",
       "         [ -6.3923,  -6.1496,  -6.2356,  ...,  -5.9965,  -6.2985,  -6.9746],\n",
       "         ...,\n",
       "         [ -7.0119,  -7.0440,  -6.8198,  ...,  -6.6488,  -8.6825,  -2.5195],\n",
       "         [ -6.3214,  -6.4319,  -6.2705,  ...,  -6.2586,  -7.9314,  -3.2719],\n",
       "         [ -6.7384,  -6.8091,  -6.6240,  ...,  -6.3949,  -8.1799,  -3.1651]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -6.9355,  -6.8644,  -6.8413,  ...,  -5.9922,  -5.9253,  -3.5768],\n",
       "         [ -8.4714,  -8.3853,  -8.4928,  ...,  -7.8767,  -7.9238,  -5.9795],\n",
       "         [-11.9227, -11.8946, -12.0610,  ..., -10.4939, -10.7369,  -8.8868],\n",
       "         ...,\n",
       "         [-10.9604, -10.9185, -10.8954,  ...,  -8.8836,  -9.3923,  -7.1013],\n",
       "         [ -7.6124,  -7.6196,  -7.7997,  ...,  -6.7293,  -8.5463,   0.3102],\n",
       "         [ -7.5928,  -7.6106,  -7.7640,  ...,  -6.7841,  -8.5374,   0.1817]],\n",
       "\n",
       "        [[ -5.8819,  -5.8609,  -5.8495,  ...,  -5.1583,  -5.0107,  -3.1658],\n",
       "         [ -4.8145,  -4.8128,  -4.7097,  ...,  -4.3043,  -5.2534,  -3.8823],\n",
       "         [ -9.0838,  -9.2793,  -9.2492,  ...,  -7.3009,  -7.7543,  -6.2853],\n",
       "         ...,\n",
       "         [ -4.8654,  -4.9697,  -4.8728,  ...,  -4.4019,  -5.9218,  -2.6209],\n",
       "         [ -5.0449,  -5.1053,  -4.9870,  ...,  -4.3800,  -6.3811,  -1.1554],\n",
       "         [ -5.4221,  -5.4533,  -5.2811,  ...,  -4.6441,  -6.1889,  -1.5495]],\n",
       "\n",
       "        [[ -7.3127,  -7.1634,  -7.1640,  ...,  -6.2303,  -6.3459,  -3.8150],\n",
       "         [ -8.7297,  -8.7701,  -8.4374,  ...,  -7.3610,  -7.3423,  -4.9145],\n",
       "         [ -9.0872,  -9.1849,  -8.8044,  ...,  -6.8373,  -7.6222,  -5.2026],\n",
       "         ...,\n",
       "         [ -7.6688,  -7.6267,  -7.3707,  ...,  -5.2835,  -8.0364,  -2.8653],\n",
       "         [ -8.0767,  -7.9402,  -7.6707,  ...,  -5.2994,  -8.3981,  -2.6359],\n",
       "         [ -8.1632,  -7.8561,  -7.6684,  ...,  -4.8841,  -8.4551,  -2.1259]]],\n",
       "       grad_fn=<ViewBackward0>), lm_loss=None, dr_loss=None, data_repr=None, lbl2data_repr=None, data_hidden_states=None, data_attentions=None, data_cross_attentions=None, lbl2data_hidden_states=None, lbl2data_attentions=None, lbl2data_cross_attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69838806-7d60-4ad0-a9f4-38191419e7f8",
   "metadata": {},
   "source": [
    "## DBT008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9d017-71e1-4b2a-8d3b-345a7bf3f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT008(DistilBertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.8,\n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, ig_tok=ig_tok, reduce='mean')\n",
    "        self.post_init()\n",
    "\n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, Pooling.mean_pooling(o[0], attention_mask)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(data_input_ids, \n",
    "                                          data_attention_mask, \n",
    "                                          output_attentions=output_attentions, \n",
    "                                          output_hidden_states=output_hidden_states,\n",
    "                                          return_dict=return_dict)\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(lbl2data_input_ids, \n",
    "                                                      lbl2data_attention_mask,  \n",
    "                                                      output_attentions=output_attentions, \n",
    "                                                      output_hidden_states=output_hidden_states,\n",
    "                                                      return_dict=return_dict)\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6d96-4040-45db-99c0-435815d47717",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f128af-39d0-46b3-a116-c33bc7a0f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBT008.from_pretrained('sentence-transformers/msmarco-distilbert-dot-v5', ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d2071-17de-478d-b6be-54de07f5d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afdfeb-524a-475f-a758-2b881eb5cd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCModelOutput(loss=tensor(0.4810, grad_fn=<SumBackward0>), logits=None, lm_loss=None, dr_loss=None, data_repr=tensor([[ 0.0372, -0.0094,  0.1272,  ...,  0.2118, -0.5175, -0.0071],\n",
       "        [-0.3130, -0.2676,  0.2495,  ...,  0.4102, -0.4511, -0.0603],\n",
       "        [-0.3817,  0.3488, -0.2542,  ...,  0.1895, -0.0678, -0.2233],\n",
       "        ...,\n",
       "        [ 0.1648,  0.1906,  0.5809,  ...,  0.3762,  0.0511, -0.3078],\n",
       "        [ 0.0934,  0.6205,  0.0835,  ...,  0.1357,  0.1037, -0.2269],\n",
       "        [-0.4758,  0.0118,  0.1701,  ...,  0.1738,  0.1395, -0.1342]],\n",
       "       grad_fn=<DivBackward0>), lbl2data_repr=tensor([[-0.2418,  0.2394,  0.2001,  ...,  0.0800,  0.2445, -0.3035],\n",
       "        [-0.0580,  0.3176, -0.1738,  ..., -0.0049, -0.1210, -0.1995],\n",
       "        [-0.0683,  0.4309, -0.1003,  ..., -0.1094, -0.1286, -0.2189],\n",
       "        ...,\n",
       "        [-0.4602,  0.1432, -0.1938,  ..., -0.1724, -0.0637,  0.2525],\n",
       "        [-0.2790,  0.4353,  0.2206,  ..., -0.0257, -0.2271,  0.0971],\n",
       "        [-0.3544,  0.2026, -0.1465,  ...,  0.2404, -0.0090, -0.3358]],\n",
       "       grad_fn=<DivBackward0>), data_hidden_states=None, data_attentions=None, data_cross_attentions=None, lbl2data_hidden_states=None, lbl2data_attentions=None, lbl2data_cross_attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dba60c-1bcb-4184-b5a0-06f88c108422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
