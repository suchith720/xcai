{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8c456-5339-43aa-baed-b9eb2114b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.MMM0XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f76551-93c3-40cf-915f-5a4d3c04cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc422-47c6-4c49-a719-e83d23acf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect, pickle, os, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any, Union\n",
    "from transformers import (\n",
    "    PretrainedConfig,\n",
    "    BertLMHeadModel, \n",
    "    BatchEncoding, \n",
    "    BertPreTrainedModel, \n",
    "    BertModel, \n",
    "    RobertaForCausalLM, \n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertModel,\n",
    "    DistilBertPreTrainedModel,\n",
    ")\n",
    "from transformers.utils.generic import ModelOutput\n",
    "\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.losses import *\n",
    "from xcai.core import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae88304-b2a5-4988-b6e8-d30a2bd79f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.block import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3b726-0f16-46f1-ae48-b34febc784b0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f118491-1eb9-4e38-a63f-395612689d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'data_metas', tfm='xcnlg', \n",
    "                         tokenizer='distilbert-base-uncased', \n",
    "                         smp_features=[('lbl2data',1, 2), ('cat2data',1, 1), ('cat2lbl2data',2,1), \n",
    "                                       ('hlk2data', 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22220375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/aiscuser/scratch/datasets'\n",
    "pkl_dir = f'{data_dir}/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_xcnlg_radga.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb47f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734cfb6-162b-42b7-8ddb-f5a85bc3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(5)\n",
    "\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ace8df-a4bb-4f06-9f12-eb703f3cab3b",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d52e6c-596d-4b1e-9ffc-497fd58fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class XCModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    lm_loss: Optional[torch.FloatTensor] = None\n",
    "    dr_loss: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74690ee7-2bd5-469a-9b1e-547a5399c1d3",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab186b-0be7-45d0-a10e-762f41fb46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pooling:\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(data_embeds:torch.FloatTensor, data_attention_mask:torch.LongTensor):\n",
    "        data_attention_mask = data_attention_mask.unsqueeze(2).expand(data_embeds.size()).float()\n",
    "        return torch.sum(data_embeds * data_attention_mask, 1) / torch.clamp(data_attention_mask.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf63b9-b42e-48ef-afc0-5f7e4e56d8bb",
   "metadata": {},
   "source": [
    "## BT0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5948a-5fec-4663-b4a4-2d75387023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0001(BertLMHeadModel):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        data_logits = self.cls(data_o[0])\n",
    "        data_repr = data_o[0].mean(dim=1)\n",
    "        \n",
    "        if lbl2data_input_ids is not None and lbl2data_data2ptr is not None:\n",
    "            lbl2data_o = self.bert(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict\n",
    "            )\n",
    "            lbl2data_logits = self.cls(lbl2data_o[0][lbl2data_data2ptr.cumsum(dim=0)-1])\n",
    "            lbl2data_repr = lbl2data_o[0].mean(dim=1)\n",
    "            \n",
    "            return data_logits, lbl2data_input_ids, lbl2data_data2ptr, lbl2data_idx, lbl2data_logits, data_input_ids, data_repr, lbl2data_repr, data_o[0], data_attention_mask, lbl2data_o[0], lbl2data_attention_mask, kwargs\n",
    "\n",
    "        return data_logits, lbl2data_input_ids, lbl2data_data2ptr, lbl2data_idx, kwargs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b57869-5bf0-4b58-b782-57c436150913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "m = BT0001.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43542a-c62b-4189-b4b4-3522983a43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083cc97-7978-4450-b9c2-e9dc868d99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 18, 30522])\n",
      "torch.Size([37, 16])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 768])\n",
      "torch.Size([37, 768])\n"
     ]
    }
   ],
   "source": [
    "for o in out: print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f2018-78bd-4216-b25d-210994859696",
   "metadata": {},
   "source": [
    "## BT0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2252a1-c6b2-4a21-a6d7-83f6b295c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0002(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,False \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e45035-b68c-4da3-bed5-dbfa547c0e55",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3c66a-017e-429b-931f-221329b04e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565efc8-ca0a-4ab5-ac81-815b4941d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc2bb0-c675-460c-8ff6-abe4e26dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274c98-da35-4b9d-8f12-405117ecca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50b8fa-3ece-4f98-9556-c8b4c98c1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.6697, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f7667-a0be-47c3-abe2-b3d3eb7fd35a",
   "metadata": {},
   "source": [
    "## BT0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cfb86-7439-443b-a494-220c67f8a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0003(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.8,\n",
    "                 tau:Optional[float]=0.1,\n",
    "                 apply_softmax=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, tau=tau, apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.post_init()\n",
    "\n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, Pooling.mean_pooling(o[0], attention_mask)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(data_input_ids, \n",
    "                                          data_attention_mask, \n",
    "                                          data_token_type_ids, \n",
    "                                          output_attentions=output_attentions, \n",
    "                                          output_hidden_states=output_hidden_states,\n",
    "                                          return_dict=return_dict)\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(lbl2data_input_ids, \n",
    "                                                      lbl2data_attention_mask, \n",
    "                                                      lbl2data_token_type_ids, \n",
    "                                                      output_attentions=output_attentions, \n",
    "                                                      output_hidden_states=output_hidden_states,\n",
    "                                                      return_dict=return_dict)\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb35ee-a55e-4567-8416-3b1b08d7a695",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb553f-30cd-430e-b71e-7c4d8a6b70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BT0003 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0003.from_pretrained('bert-base-uncased', tn_targ=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa33df-0968-4c9b-88f8-588855ea91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f649083-ff41-4718-ad01-e1c5f7b5271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b92693-31e7-4df3-83b9-9d80bc1bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bab768-2fe2-4855-b517-87604c84a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:21: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659148e-9406-47c5-b744-bad69c721aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032054db-666b-4bcd-aaf5-7ecbdf54a7aa",
   "metadata": {},
   "source": [
    "## BT0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056009-fe22-400b-a10d-1bf379a53ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0004(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,True \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tau:Optional[float]=1.0,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 lw:Optional[int]=0.5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.lw, self.dr_loss_fn = lw, SoupCon(tau=tau, reduce='mean')\n",
    "        self.lm_loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, F.normalize(o[0].mean(dim=1), dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss, lm_loss, dr_loss, lbl2data_repr = None, None, None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "            lm_loss = self.lm_loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr)\n",
    "            dr_loss = self.dr_loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, **kwargs)\n",
    "            loss = lm_loss + self.lw*dr_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdffa25-f966-46d2-bb01-3cb7502dd180",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffd983-5470-41bd-9530-3f32464d6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28092e0b-b7df-4d2d-99b2-ab47fbe6c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0004 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dr_loss_fn.tau', 'lm_loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0004.from_pretrained('bert-base-uncased', lw=0.5, tn_targ=10_000, ig_tok=0)\n",
    "m, b = m.to('cuda'), b.to('cuda')\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751d634-8bd0-44ef-8193-3f5f72614925",
   "metadata": {},
   "source": [
    "## RT0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a5d32-cd5d-40f2-b5e0-49c746ebd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RT0005(RobertaForCausalLM):\n",
    "    use_generation,use_representation = True,False \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.roberta(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.lm_head(data_o[0])\n",
    "        \n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451df7a-7cba-48cb-bdd0-4e1a2c41bd41",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f820-fcc4-43b6-8c19-c2dc4f25e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RT0005 were not initialized from the model checkpoint at roberta-base and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = RT0005.from_pretrained('roberta-base', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb3894-846a-4469-8d8b-a678cf241dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fe2b2-83ec-4fe7-990b-c5ac27d03e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401a949-9d60-4e7e-8f28-ed8402c4b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6a870-c6fa-4292-8d0c-2c8fdd930907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.3047, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4b0d1-3e53-4104-8ef8-645a117df46e",
   "metadata": {},
   "source": [
    "## BT0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538bea7-3016-47a9-97f3-0721b0ef70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0006(BT0003):\n",
    "    use_generation,use_representation = False,True\n",
    "\n",
    "    @delegates(BT0003.__init__)\n",
    "    def __init__(self, config, tau:Optional[int]=1.0, *args, **kwargs):\n",
    "        super().__init__(config, tau=tau, *args, **kwargs)\n",
    "        self.loss_fn = SoupCon(tau=tau, reduce='mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92254fc-edee-4dc3-ad68-afaf39af898f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380f86d-3b53-4a6a-b9bc-ac7ae37e35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BT0006 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.tau']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0006.from_pretrained('bert-base-uncased', tau=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e8219-358b-4850-aa27-bd75bc3f9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662472f-145c-4876-8ed6-144d9dfc89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691091a-c4f4-4bc7-b4d3-4b7e9daf4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce35d5-9fc8-4679-9ae3-308490ef71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.9917, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94731a98-20f1-4174-a257-65d8ceff067a",
   "metadata": {},
   "source": [
    "## DBT007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d6e93-0608-44a8-82e3-d502ff5d93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT007(DistilBertForMaskedLM):\n",
    "    use_generation,use_representation = True,False \n",
    "    \n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 vocab_weights:Optional[torch.Tensor]=None,\n",
    "                 reduction:Optional[str]='mean',\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, vocab_weights=vocab_weights, reduce=reduction)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        data_o = self.distilbert(\n",
    "            input_ids=data_input_ids,\n",
    "            attention_mask=data_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        data_logits = self.vocab_transform(data_o[0])\n",
    "        data_logits = self.activation(data_logits)\n",
    "        data_logits = self.vocab_layer_norm(data_logits)\n",
    "        data_logits = self.vocab_projector(data_logits)\n",
    "\n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'cat2data_data2ptr', 'pcat2lbl2data_idx', 'pcat2lbl2data_data2ptr', 'cat2lbl2data_idx', 'cat2lbl2data_identifier', 'cat2lbl2data_input_text', 'cat2lbl2data_input_ids', 'cat2lbl2data_attention_mask', 'cat2lbl2data_data2ptr', 'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_identifier', 'hlk2data_input_text', 'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'hlk2lbl2data_idx', 'hlk2lbl2data_identifier', 'hlk2lbl2data_input_text', 'hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask', 'hlk2lbl2data_data2ptr', 'hlk2lbl2data_plbl2data2ptr'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d243fbd-894b-4728-a822-125364de7f40",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09538455-b52b-434c-9603-3c450881597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBT007.from_pretrained('distilbert-base-uncased', ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938e939-4d6b-4cbd-ad01-9d9d0d1cf364",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629afdde-6d2b-43ea-944c-f13c77d94d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.4827, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69838806-7d60-4ad0-a9f4-38191419e7f8",
   "metadata": {},
   "source": [
    "## DBT008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9d017-71e1-4b2a-8d3b-345a7bf3f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT008(DistilBertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.8,\n",
    "                 tau:Optional[float]=0.1,\n",
    "                 apply_softmax:Optional[bool]=False,\n",
    "                 n_negatives:Optional[int]=5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dr_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.dr_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dr_projector = nn.Linear(config.dim, config.dim)\n",
    "        \n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, \n",
    "                                    apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.post_init()\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.dr_transform.weight.data = torch.eye(self.dr_transform.out_features, self.dr_transform.in_features, \n",
    "                                                  dtype=self.dr_transform.weight.dtype)\n",
    "        self.dr_projector.weight.data = torch.eye(self.dr_projector.out_features, self.dr_projector.in_features, \n",
    "                                                  dtype=self.dr_projector.weight.dtype)\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        return o, Pooling.mean_pooling(rep, attention_mask)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(data_input_ids, \n",
    "                                          data_attention_mask, \n",
    "                                          output_attentions=output_attentions, \n",
    "                                          output_hidden_states=output_hidden_states,\n",
    "                                          return_dict=return_dict)\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(lbl2data_input_ids, \n",
    "                                                      lbl2data_attention_mask,  \n",
    "                                                      output_attentions=output_attentions, \n",
    "                                                      output_hidden_states=output_hidden_states,\n",
    "                                                      return_dict=return_dict)\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6d96-4040-45db-99c0-435815d47717",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f128af-39d0-46b3-a116-c33bc7a0f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT008 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT008.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=1024, margin=0.3, tau=0.1, \n",
    "                           n_negatives=5, apply_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d2071-17de-478d-b6be-54de07f5d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afdfeb-524a-475f-a758-2b881eb5cd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6046, grad_fn=<DivBackward0>),\n",
       " torch.Size([5, 768]),\n",
       " torch.Size([6, 768]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss, o.data_repr.shape, o.lbl2data_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29db4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6046, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce8c7d",
   "metadata": {},
   "source": [
    "## DBT009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT009(DBT008):\n",
    "\n",
    "    @delegates(DBT008.__init__)\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o,rep = super().get_repr(input_ids, attention_mask, **kwargs)\n",
    "        return o,F.normalize(rep, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f2c1d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT009 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=1024, margin=0.3, tau=0.1, \n",
    "                           n_negatives=5, apply_softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f017625",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e546f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0319, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e839ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 0.3, 0.1, 5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.loss_fn.bsz, m.loss_fn.margin, m.loss_fn.tau, m.loss_fn.n_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1968eb",
   "metadata": {},
   "source": [
    "## DBT010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT010(DBT009):\n",
    "\n",
    "    @delegates(DBT009.__init__)\n",
    "    def __init__(self, config, tau:Optional[int]=1.0, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = SoupCon(tau=tau, reduce='mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcc5c6",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41feb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['loss_fn.tau']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', tau=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76f313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.2000, requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.loss_fn.tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5622fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3032, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d585f-e8f8-48d0-89f2-62d60495fd98",
   "metadata": {},
   "source": [
    "## DBT011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a838c-d152-4df1-b9fd-a8bd61d9032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT011(DBT008):\n",
    "\n",
    "    @delegates(DBT008.__init__)\n",
    "    def __init__(self, config, \n",
    "                 margin:Optional[float]=0.3, \n",
    "                 tau:Optional[float]=0.1, \n",
    "                 apply_softmax:Optional[bool]=True,\n",
    "                 n_negatives:Optional[int]=5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.lfn, self.loss_fn = Triplet(margin=margin, tau=tau, n_negatives=n_negatives, apply_softmax=apply_softmax, reduce='mean'), None\n",
    "        \n",
    "    def loss_fn(self,\n",
    "                data_repr:torch.Tensor, \n",
    "                lbl2data_repr:torch.Tensor, \n",
    "                lbl2data_data2ptr:torch.Tensor, \n",
    "                lbl2data_idx:torch.Tensor, \n",
    "                plbl2data_data2ptr:torch.Tensor, \n",
    "                plbl2data_idx:torch.Tensor, \n",
    "                **kwargs):\n",
    "        return self.lfn(data_repr, lbl2data_repr, lbl2data_idx, plbl2data_data2ptr, plbl2data_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d67701-41d4-41a4-837c-c83658e04378",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f425fe9-1016-4069-86e0-d3cc6c733f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBT011.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', margin=0.4, tau=0.7, \n",
    "                           apply_softmax=True, n_negatives=5)\n",
    "b = prepare_batch(m, batch, m_args=['plbl2data_idx', 'plbl2data_data2ptr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d2248-0ca8-4a43-ac28-525371fbdd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/scratch/Projects/xcai/xcai/losses.py:21: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73457bb-9adf-4c4e-acf4-5748a55713ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8743, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737146c-8b28-4dd1-8558-b66824037912",
   "metadata": {},
   "source": [
    "## DBT012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544a4ad-c0f2-48ee-89ea-cf3c5bd652ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT012(DBT009):\n",
    "\n",
    "    @delegates(DBT009.__init__)\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 margin:Optional[float]=0.3, \n",
    "                 tau:Optional[float]=0.1, \n",
    "                 apply_softmax:Optional[bool]=True,\n",
    "                 n_negatives:Optional[int]=5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.lfn, self.loss_fn = Triplet(margin=margin, tau=tau, n_negatives=n_negatives, apply_softmax=apply_softmax, reduce='mean'), None\n",
    "        \n",
    "    def loss_fn(self,\n",
    "                data_repr:torch.Tensor, \n",
    "                lbl2data_repr:torch.Tensor, \n",
    "                lbl2data_data2ptr:torch.Tensor, \n",
    "                lbl2data_idx:torch.Tensor, \n",
    "                plbl2data_data2ptr:torch.Tensor, \n",
    "                plbl2data_idx:torch.Tensor, \n",
    "                **kwargs):\n",
    "        return self.lfn(data_repr, lbl2data_repr, lbl2data_idx, plbl2data_data2ptr, plbl2data_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcff636-af69-47c0-af7a-1be2880162b8",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94bb44-a692-4114-a3ec-93286eb6256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DBT012.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', margin=0.4, tau=0.7, \n",
    "                           apply_softmax=True, n_negatives=5)\n",
    "b = prepare_batch(m, batch, m_args=['plbl2data_idx', 'plbl2data_data2ptr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59aa3e-c393-4118-afc3-3b92609bebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124a5af-cf6b-4e66-a6ae-5785b002b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0652, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffd0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.7, True, 5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.lfn.margin, m.lfn.tau, m.lfn.apply_softmax, m.lfn.n_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b599620",
   "metadata": {},
   "source": [
    "## DBT013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT013(DistilBertForMaskedLM):\n",
    "    use_generation,use_representation = True,True \n",
    "    \n",
    "    def __init__(\n",
    "        self, config,\n",
    "        tn_targ:Optional[int]=None, \n",
    "        ig_tok:Optional[int]=0,\n",
    "        bsz:Optional[int]=None,\n",
    "        margin:Optional[int]=0.3,\n",
    "        n_negatives:Optional[int]=5,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=True,\n",
    "        lw:Optional[float]=0.8,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.dr_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.dr_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dr_projector = nn.Linear(config.dim, config.dim)\n",
    "        \n",
    "        self.lw = lw\n",
    "        self.gen_lfn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "        self.rep_lfn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau,\n",
    "                                    apply_softmax=apply_softmax, reduce='mean')\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.dr_transform.weight.data = torch.eye(self.dr_transform.out_features, self.dr_transform.in_features, \n",
    "                                                  dtype=self.dr_transform.weight.dtype)\n",
    "        self.dr_projector.weight.data = torch.eye(self.dr_projector.out_features, self.dr_projector.in_features, \n",
    "                                                  dtype=self.dr_projector.weight.dtype)\n",
    "        \n",
    "    def get_representation(self, input_ids:Optional[torch.Tensor]=None, attention_mask:Optional[torch.Tensor]=None, **kwargs):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        return o, F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "    \n",
    "    def get_generation(self, input_ids:Optional[torch.Tensor]=None, attention_mask:Optional[torch.Tensor]=None, **kwargs):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        return o, self.vocab_projector(logits)\n",
    "        \n",
    "    def get_genrep(self, input_ids:Optional[torch.Tensor]=None, attention_mask:Optional[torch.Tensor]=None, **kwargs):    \n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        rep = F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "        \n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        logits = self.vocab_projector(logits)\n",
    "        return o,logits,rep\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        data_o, data_logits, data_repr = self.get_genrep(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            \n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3e7ad",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT013 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT013.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0,\n",
    "                           margin=0.4, tau=0.7, apply_softmax=True, n_negatives=5, lw=0.8)\n",
    "b = prepare_batch(m, batch, m_args='lbl2data_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d696f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbl2data_idx :  torch.Size([8])\n",
      "plbl2data_data2ptr :  torch.Size([5])\n",
      "lbl2data_idx :  torch.Size([6])\n",
      "lbl2data_input_ids :  torch.Size([6, 13])\n",
      "lbl2data_attention_mask :  torch.Size([6, 13])\n",
      "lbl2data_data2ptr :  torch.Size([5])\n",
      "data_input_ids :  torch.Size([5, 10])\n",
      "data_attention_mask :  torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "for k,v in b.items(): print(k, ': ', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e1066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3832, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eec68a",
   "metadata": {},
   "source": [
    "## DBT014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b849559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT014(DBT013):\n",
    "    \n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[Union[float,List]]=0.2, meta_prefix:Optional[str]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.m_lw, self.meta_prefix = m_lw, meta_prefix\n",
    "        \n",
    "    def _get_meta_inputs(self, **kwargs):\n",
    "        inputs = {}\n",
    "        for t in [o for o in kwargs if self.meta_prefix is not None and re.match(f'^[p]?{self.meta_prefix}.*', o)]:\n",
    "            p,q = t.split('_', maxsplit=1)\n",
    "            if t[0] == 'p': inputs.setdefault(p[1:], {})[f'p{q}'] = kwargs[t]\n",
    "            else: inputs.setdefault(p, {})[q] = kwargs[t]\n",
    "        return inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        data_o, data_logits, data_repr = self.get_genrep(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(**kwargs)\n",
    "            if isinstance(self.m_lw, float):\n",
    "                meta_lw = self.m_lw/len(meta_inputs) if len(meta_inputs) else None\n",
    "                meta_lw = [meta_lw]*len(meta_inputs)\n",
    "            else:\n",
    "                if len(self.m_lw) != len(meta_inputs): raise ValueError(f'length of `m_lw` should be equal to number of metadata.')\n",
    "                meta_lw = self.m_lw\n",
    "                \n",
    "            for m,m_lw in zip(meta_inputs.values(),meta_lw):\n",
    "                valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                if len(valid_idx) > 0:\n",
    "                    o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                    m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                    m_drl = self.rep_lfn(data_repr[valid_idx], rep, m['data2ptr'][valid_idx], m['idx'], \n",
    "                                         m['pdata2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                    loss += m_lw * (m_drl + self.lw*m_lml)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651ad06",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT014 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT014.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0, margin=0.4, tau=0.7, \n",
    "                           apply_softmax=True, n_negatives=5, lw=0.8, m_lw=[0.2], meta_prefix='hlk')\n",
    "\n",
    "b = prepare_batch(m, batch, m_args=['hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', \n",
    "                                    'hlk2data_idx', 'phlk2data_data2ptr', 'phlk2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55606fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10f02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.9105, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c3717",
   "metadata": {},
   "source": [
    "## DBT015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd04618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT015(DBT013):\n",
    "    \n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[float]=0.2, meta_prefix:Optional[str]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.mdr_lfn = Triplet(margin=kwargs.get('margin'), tau=kwargs.get('tau'), n_negatives=kwargs.get('n_negatives'), \n",
    "                               apply_softmax=kwargs.get('apply_softmax'), reduce='mean')\n",
    "        self.m_lw, self.meta_prefix = m_lw, meta_prefix \n",
    "        \n",
    "    def _get_meta_inputs(self, **kwargs):\n",
    "        inputs = {}\n",
    "        for t in [o for o in kwargs if self.meta_prefix is not None and re.match(f'^[p]?{self.meta_prefix}.*', o)]:\n",
    "            p,q = t.split('_', maxsplit=1)\n",
    "            if t[0] == 'p': inputs.setdefault(p[1:], {})[f'p{q}'] = kwargs[t]\n",
    "            else: inputs.setdefault(p, {})[q] = kwargs[t]\n",
    "        return inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        data_o, data_logits, data_repr = self.get_genrep(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, **kwargs)\n",
    "            loss = (lm_loss + self.lw*dr_loss) * (1-self.m_lw)\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(**kwargs)\n",
    "            m_lw = self.m_lw/len(meta_inputs)\n",
    "            for m in meta_inputs.values():\n",
    "                valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                if len(valid_idx) > 0:\n",
    "                    o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                    m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                    m_drl = self.mdr_lfn(data_repr[valid_idx], rep, m['idx'], m['pdata2ptr'][valid_idx], \n",
    "                                         m['pidx'], **kwargs)\n",
    "                    loss += m_lw * (m_lml + self.lw*m_drl)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad75c76",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT015 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'gen_lfn.o', 'rep_lfn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT015.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0, margin=0.4, tau=0.7, \n",
    "                           apply_softmax=True, n_negatives=5, lw=100.0, m_lw=0.2, meta_prefix='hlk')\n",
    "\n",
    "b = prepare_batch(m, batch, m_args=['hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask', 'hlk2lbl2data_data2ptr', \n",
    "                                    'hlk2lbl2data_idx', 'phlk2lbl2data_idx', 'phlk2lbl2data_data2ptr', \n",
    "                                    'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', \n",
    "                                    'hlk2data_idx', 'phlk2data_idx', 'phlk2data_data2ptr', \n",
    "                                    'lbl2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a42c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hlk2data_idx', 'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_identifier', 'hlk2data_input_text', 'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', 'hlk2lbl2data_idx', 'phlk2lbl2data_idx', 'phlk2lbl2data_data2ptr', 'hlk2lbl2data_identifier', 'hlk2lbl2data_input_text', 'hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask', 'hlk2lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09496973",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0a7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.0921, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48fd72",
   "metadata": {},
   "source": [
    "## DBT016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT016(DBT013):\n",
    "\n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "\n",
    "    def get_generation(self, input_ids:Optional[torch.Tensor]=None, attention_mask:Optional[torch.Tensor]=None, **kwargs):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        logits = o[0].detach()\n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        return o, self.vocab_projector(logits)\n",
    "        \n",
    "    def get_genrep(self, input_ids:Optional[torch.Tensor]=None, attention_mask:Optional[torch.Tensor]=None, **kwargs):    \n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        rep = F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "        \n",
    "        logits = o[0].detach()\n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        logits = self.vocab_projector(logits)\n",
    "        return o,logits,rep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee023db9",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b69f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DBT016 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'gen_lfn.o', 'rep_lfn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT016.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0,\n",
    "                           margin=0.4, tau=0.7, apply_softmax=True, n_negatives=5, lw=0.8)\n",
    "b = prepare_batch(m, batch, m_args='lbl2data_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c804ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiscuser/scratch/Projects/xcai/xcai/losses.py:21: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ce88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7218, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e5a6d",
   "metadata": {},
   "source": [
    "## DBT017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT017(DBT013):\n",
    "    \n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[Union[float,List]]=0.2, meta_prefix:Optional[str]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.m_lw, self.meta_prefix = m_lw, meta_prefix\n",
    "        \n",
    "    def _get_meta_inputs(self, **kwargs):\n",
    "        inputs = {}\n",
    "        for t in [o for o in kwargs if self.meta_prefix is not None and re.match(f'^[p]?{self.meta_prefix}.*', o)]:\n",
    "            p,q = t.split('_', maxsplit=1)\n",
    "            if t[0] == 'p': inputs.setdefault(p[1:], {})[f'p{q}'] = kwargs[t]\n",
    "            else: inputs.setdefault(p, {})[q] = kwargs[t]\n",
    "        return inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        data_o, data_logits, data_repr = self.get_genrep(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_logits, lbl2data_repr = self.get_genrep(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            \n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(**kwargs)\n",
    "            if isinstance(self.m_lw, float):\n",
    "                meta_lw = self.m_lw/len(meta_inputs) if len(meta_inputs) else None\n",
    "                meta_lw = [meta_lw]*len(meta_inputs)\n",
    "            else:\n",
    "                if len(self.m_lw) != len(meta_inputs): raise ValueError(f'length of `m_lw` should be equal to number of metadata.')\n",
    "                meta_lw = self.m_lw\n",
    "                \n",
    "            for m,m_lw in zip(meta_inputs.values(), meta_lw):\n",
    "                if 'lbl2data2ptr' in m:\n",
    "                    valid_idx = torch.where(m['lbl2data2ptr'])[0]\n",
    "                    if len(valid_idx) > 0:\n",
    "                        o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                        m_lml = self.gen_lfn(lbl2data_logits[valid_idx], m['input_ids'], m['lbl2data2ptr'][valid_idx], **kwargs)\n",
    "                        m_drl = self.rep_lfn(lbl2data_repr[valid_idx], rep, m['lbl2data2ptr'][valid_idx], m['idx'], \n",
    "                                             m['plbl2data2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                        loss += m_lw * (m_drl + self.lw* m_lml)\n",
    "                        \n",
    "                elif 'data2ptr' in m:\n",
    "                    valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                    if len(valid_idx) > 0:\n",
    "                        o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                        m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                        m_drl = self.rep_lfn(data_repr[valid_idx], rep, m['data2ptr'][valid_idx], m['idx'], \n",
    "                                             m['pdata2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                        loss += m_lw * (m_drl + self.lw*m_lml) \n",
    "                        \n",
    "                else: raise ValueError('Invalid metadata input arguments.')\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4754ea",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fa2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT017 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = DBT017.from_pretrained('distilbert-base-uncased', tn_targ=10_000, ig_tok=0, margin=0.4, tau=0.7, \n",
    "                           apply_softmax=True, n_negatives=5, lw=0.8, m_lw=[0.1], meta_prefix='hlk')\n",
    "\n",
    "b = prepare_batch(m, batch, m_args=['phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_input_ids', \n",
    "                                    'hlk2data_attention_mask', 'hlk2data_data2ptr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea348aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6465, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56999aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask', 'hlk2data_data2ptr', 'data_input_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549c850",
   "metadata": {},
   "source": [
    "## Fuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Fuser(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config, self.n_h, self.dim = config, config.n_heads, config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        if self.dim % self.n_h != 0:\n",
    "            raise ValueError(f\"self.n_heads: {self.n_h} must divide self.dim: {self.dim} evenly.\")\n",
    "            \n",
    "        self.q = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.o = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        data: torch.Tensor,\n",
    "        data_mask: torch.Tensor,\n",
    "        meta: torch.Tensor, \n",
    "        meta_mask: torch.Tensor,\n",
    "        output_attentions:Optional[bool] = False,\n",
    "    ):\n",
    "        q, k, v, q_m, k_m = data, meta, meta, data_mask, meta_mask\n",
    "        \n",
    "        bs, q_len, dim = q.size()\n",
    "        k_len = k.size(1)\n",
    "\n",
    "        h_dim = self.dim//self.n_h\n",
    "\n",
    "        def shape(x: torch.Tensor): return x.view(bs, -1, self.n_h, h_dim).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor): return x.transpose(1, 2).contiguous().view(bs, -1, self.n_h * h_dim)\n",
    "\n",
    "        q = shape(self.q(q))  # (bs, n_h, q_len, h_dim)\n",
    "        k = shape(self.k(k))  # (bs, n_h, k_len, h_dim)\n",
    "        v = shape(self.v(v))  # (bs, n_h, k_len, h_dim)\n",
    "\n",
    "        q = q / math.sqrt(h_dim)  # (bs, n_h, q_len, h_dim)\n",
    "        sc = torch.matmul(q, k.transpose(2, 3))  # (bs, n_h, q_len, k_len)\n",
    "        \n",
    "        q_m, k_m = q_m.view(bs, 1, -1, 1).to(q.dtype), k_m.view(bs, 1, 1, -1).to(q.dtype)\n",
    "        mask = torch.matmul(q_m, k_m).expand_as(sc)  # (bs, n_h, q_len, k_len)\n",
    "        \n",
    "        sc = sc.masked_fill(mask.bool(), torch.tensor(torch.finfo(sc.dtype).min))  # (bs, n_h, q_len, k_len)\n",
    "\n",
    "        w = nn.functional.softmax(sc, dim=-1)  # (bs, n_h, q_len, k_len)\n",
    "        w = self.dropout(w)  # (bs, n_h, q_len, k_len)\n",
    "\n",
    "        o = self.o(unshape(torch.matmul(w, v))) # (bs, q_len, dim)\n",
    "        \n",
    "        if output_attentions: return (o, w)\n",
    "        else: return (o,)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d95ac",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25562854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuser = Fuser(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79134f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, data_seq_len, meta_seq_len, dim, dtype = 10, 14, 17, config.dim, torch.float32\n",
    "data, meta = torch.randn(bsz, data_seq_len, dim, dtype=dtype), torch.randn(bsz, meta_seq_len, dim, dtype=dtype)\n",
    "data_mask = torch.randint(0, 2, size=(bsz,data_seq_len), dtype=dtype)\n",
    "meta_mask = torch.randint(0, 2, size=(bsz,meta_seq_len), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = fuser(data, data_mask, meta, meta_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71865882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 14, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7377af",
   "metadata": {},
   "source": [
    "## DBT018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a484b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT018(DBT013):\n",
    "    \n",
    "    @delegates(DBT013.__init__)\n",
    "    def __init__(self, config, aug_meta_prefix:Optional[List]=None, tn_meta:Optional[int]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('aug_meta_prefix')\n",
    "        self.fuser = Fuser(config)\n",
    "        self.ln = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.o = torch.ones(tn_meta, dtype=torch.long, device=self.device) if tn_meta is not None else None\n",
    "        \n",
    "    def _get_meta_inputs(self, meta_prefix:Optional[str], **kwargs):\n",
    "        inputs = {}\n",
    "        for t in [o for o in kwargs if meta_prefix is not None and re.match(f'^[p]?{meta_prefix}.*', o)]:\n",
    "            p,q = t.split('_', maxsplit=1)\n",
    "            if t[0] == 'p': inputs.setdefault(p[1:], {})[f'p{q}'] = kwargs[t]\n",
    "            else: inputs.setdefault(p, {})[q] = kwargs[t]\n",
    "        return inputs\n",
    "    \n",
    "    def get_output(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        o = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_logits, data_repr = self.get_meta_fused_genrep(data_input_ids, data_attention_mask, **kwargs)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o[0],\n",
    "        )\n",
    "    \n",
    "    def resize_meta(self, meta:torch.Tensor, mask:torch.Tensor, n_data2meta:torch.Tensor):\n",
    "        bsz, dim, tn_data2meta = n_data2meta.shape[0], meta.shape[-1], meta.shape[0]\n",
    "        self.o = self.o.to(meta.device)\n",
    "        o = (\n",
    "            torch.ones(tn_data2meta, dtype=torch.long, device=meta.device) \n",
    "            if self.o is None or len(self.o) < tn_data2meta else self.o[:tn_data2meta]\n",
    "        )\n",
    "\n",
    "        max_n_data2meta = n_data2meta.max()\n",
    "        xn_data2meta = max_n_data2meta-n_data2meta+1\n",
    "\n",
    "        data2meta_ptr = n_data2meta.cumsum(dim=0)-1\n",
    "        r_data2meta = o.scatter(0, data2meta_ptr, xn_data2meta)\n",
    "\n",
    "        xmeta,xmask = meta.repeat_interleave(r_data2meta, dim=0),mask.repeat_interleave(r_data2meta, dim=0)\n",
    "        m = o.scatter(0, data2meta_ptr, 0).repeat_interleave(r_data2meta, dim=0).view(bsz, -1)\n",
    "        m[:, -1] = 1; m = m.view(-1, 1)\n",
    "        xmask *= m\n",
    "\n",
    "        return xmeta,xmask\n",
    "\n",
    "    def get_meta_fused_output(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        data_h = self.get_output(input_ids, attention_mask)[0]\n",
    "\n",
    "        meta_inputs = self._get_meta_inputs(self.aug_meta_prefix, **kwargs)\n",
    "        for m in meta_inputs.values():\n",
    "            valid_idx = torch.where(m['data2ptr'] > 0)[0]\n",
    "            if len(valid_idx):\n",
    "                bsz = len(valid_idx)\n",
    "                meta_input_ids, meta_attention_mask = self.resize_meta(m['input_ids'], m['attention_mask'], \n",
    "                                                                       m['data2ptr'][valid_idx])\n",
    "                meta_h = self.get_output(meta_input_ids, meta_attention_mask)[0]\n",
    "\n",
    "                meta_h,meta_attention_mask = meta_h.view(bsz, -1, self.config.dim), meta_attention_mask.view(bsz, -1)\n",
    "\n",
    "                data_h[valid_idx] += self.fuser(data_h[valid_idx], attention_mask[valid_idx], \n",
    "                                                meta_h, meta_attention_mask)[0]\n",
    "\n",
    "        data_h = self.ln(data_h)\n",
    "        return (data_h,)\n",
    "\n",
    "    def get_meta_fused_genrep(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None, \n",
    "        **kwargs\n",
    "    ):    \n",
    "        o = self.get_meta_fused_output(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        rep = self.dr_transform(o[0])\n",
    "        rep = self.dr_layer_norm(rep)\n",
    "        rep = self.dr_projector(rep)\n",
    "        rep = F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "\n",
    "        logits = self.vocab_transform(o[0])\n",
    "        logits = self.activation(logits)\n",
    "        logits = self.vocab_layer_norm(logits)\n",
    "        logits = self.vocab_projector(logits)\n",
    "        return o,logits,rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03db6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7231798",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f42120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT018 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'fuser.k.bias', 'fuser.k.weight', 'fuser.o.bias', 'fuser.o.weight', 'fuser.q.bias', 'fuser.q.weight', 'fuser.v.bias', 'fuser.v.weight', 'ln.bias', 'ln.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "model = DBT018.from_pretrained('distilbert-base-uncased', ig_tok=0, tn_targ=10_000, tn_meta=10_000, \n",
    "                               margin=0.3, tau=0.1, n_negatives=5, apply_softmax=True, lw=0.01,\n",
    "                               aug_meta_prefix='hlk', init_drh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f009721",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=[\n",
    "    'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask', \n",
    "    'cat2data_data2ptr', \n",
    "    'pcat2lbl2data_idx', 'pcat2lbl2data_data2ptr', 'cat2lbl2data_idx', 'cat2lbl2data_input_ids', \n",
    "    'cat2lbl2data_attention_mask', 'cat2lbl2data_data2ptr',\n",
    "    \n",
    "    'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask', \n",
    "    'hlk2data_data2ptr'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebad312",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b490f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1434, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc6acc",
   "metadata": {},
   "source": [
    "## DBT019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8853954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT019(DBT018):\n",
    "    \n",
    "    @delegates(DBT018.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[Union[List,float]]=0.2, pred_meta_prefix:Optional[List]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('m_lw,pred_meta_prefix')\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_logits, data_repr = self.get_meta_fused_genrep(data_input_ids, data_attention_mask, **kwargs)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_representation(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(self.pred_meta_prefix, **kwargs)\n",
    "            if isinstance(self.m_lw, float):\n",
    "                meta_lw = self.m_lw/len(meta_inputs) if len(meta_inputs) else None\n",
    "                meta_lw = [meta_lw]*len(meta_inputs)\n",
    "            else:\n",
    "                if len(self.m_lw) != len(meta_inputs): raise ValueError(f'length of `m_lw` should be equal to number of metadata.')\n",
    "                meta_lw = self.m_lw\n",
    "            \n",
    "            for m,m_lw in zip(meta_inputs.values(),meta_lw):\n",
    "                valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                if len(valid_idx) > 0:\n",
    "                    o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                    m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                    m_drl = self.rep_lfn(data_repr[valid_idx], rep, m['data2ptr'][valid_idx], m['idx'], \n",
    "                                         m['pdata2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                    loss += m_lw * (m_drl + self.lw*m_lml)\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o[0],\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70860a79",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT019 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'fuser.k.bias', 'fuser.k.weight', 'fuser.o.bias', 'fuser.o.weight', 'fuser.q.bias', 'fuser.q.weight', 'fuser.v.bias', 'fuser.v.weight', 'ln.bias', 'ln.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DBT019.from_pretrained('distilbert-base-uncased', ig_tok=0, tn_targ=10_000, tn_meta=10_000, \n",
    "                               margin=0.3, tau=0.1, n_negatives=5, apply_softmax=True, lw=0.01,\n",
    "                               aug_meta_prefix='hlk', pred_meta_prefix='cat', m_lw=0.1, init_drh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=[\n",
    "    'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask', \n",
    "    'cat2data_data2ptr', \n",
    "    'pcat2lbl2data_idx', 'pcat2lbl2data_data2ptr', 'cat2lbl2data_idx', 'cat2lbl2data_input_ids', \n",
    "    'cat2lbl2data_attention_mask', 'cat2lbl2data_data2ptr',\n",
    "    \n",
    "    'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask', \n",
    "    'hlk2data_data2ptr'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff35da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f06e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1621, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef4ddd",
   "metadata": {},
   "source": [
    "## DBT020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DBT020(DBT018):\n",
    "    \n",
    "    @delegates(DBT018.__init__)\n",
    "    def __init__(self, config, m_lw:Optional[Union[List,float]]=0.2, pred_meta_prefix:Optional[List]=None, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('m_lw,pred_meta_prefix')\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_logits, data_repr = self.get_meta_fused_genrep(data_input_ids, data_attention_mask, **kwargs)\n",
    "        \n",
    "        loss = lm_loss = dr_loss = lbl2data_repr = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_logits, lbl2data_repr = self.get_genrep(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            lm_loss = self.gen_lfn(data_logits, lbl2data_input_ids, lbl2data_data2ptr, **kwargs)\n",
    "            dr_loss = self.rep_lfn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                   plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "            loss = dr_loss + self.lw*lm_loss\n",
    "            \n",
    "            meta_inputs = self._get_meta_inputs(self.pred_meta_prefix, **kwargs)\n",
    "            if isinstance(self.m_lw, float):\n",
    "                meta_lw = self.m_lw/len(meta_inputs) if len(meta_inputs) else None\n",
    "                meta_lw = [meta_lw]*len(meta_inputs)\n",
    "            else:\n",
    "                if len(self.m_lw) != len(meta_inputs): raise ValueError(f'length of `m_lw` should be equal to number of metadata.')\n",
    "                meta_lw = self.m_lw\n",
    "            \n",
    "            for m,m_lw in zip(meta_inputs.values(),meta_lw):\n",
    "                if 'lbl2data2ptr' in m:\n",
    "                    valid_idx = torch.where(m['lbl2data2ptr'])[0]\n",
    "                    if len(valid_idx) > 0:\n",
    "                        o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                        m_lml = self.gen_lfn(lbl2data_logits[valid_idx], m['input_ids'], m['lbl2data2ptr'][valid_idx], **kwargs)\n",
    "                        m_drl = self.rep_lfn(lbl2data_repr[valid_idx], rep, m['lbl2data2ptr'][valid_idx], m['idx'], \n",
    "                                             m['plbl2data2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                        loss += m_lw * (m_drl + self.lw* m_lml)\n",
    "                        \n",
    "                elif 'data2ptr' in m:\n",
    "                    valid_idx = torch.where(m['data2ptr'])[0]\n",
    "                    if len(valid_idx) > 0:\n",
    "                        o, rep = self.get_representation(m['input_ids'], m['attention_mask'])\n",
    "                        m_lml = self.gen_lfn(data_logits[valid_idx], m['input_ids'], m['data2ptr'][valid_idx], **kwargs)\n",
    "                        m_drl = self.rep_lfn(data_repr[valid_idx], rep, m['data2ptr'][valid_idx], m['idx'], \n",
    "                                             m['pdata2ptr'][valid_idx], m['pidx'], **kwargs)\n",
    "                        loss += m_lw * (m_drl + self.lw*m_lml)       \n",
    "                \n",
    "                else: raise ValueError('Invalid metadata input arguments.')\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o[0],\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c785436",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83048ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT020 were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dr_layer_norm.bias', 'dr_layer_norm.weight', 'dr_projector.bias', 'dr_projector.weight', 'dr_transform.bias', 'dr_transform.weight', 'fuser.k.bias', 'fuser.k.weight', 'fuser.o.bias', 'fuser.o.weight', 'fuser.q.bias', 'fuser.q.weight', 'fuser.v.bias', 'fuser.v.weight', 'ln.bias', 'ln.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DBT020.from_pretrained('distilbert-base-uncased', ig_tok=0, tn_targ=10_000, tn_meta=10_000, \n",
    "                               margin=0.3, tau=0.1, n_negatives=5, apply_softmax=True, lw=0.01,\n",
    "                               aug_meta_prefix='hlk', pred_meta_prefix='cat', m_lw=0.1, init_drh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4653b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=[\n",
    "    'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', 'cat2data_attention_mask', \n",
    "    'cat2data_data2ptr', \n",
    "    'pcat2lbl2data_idx', 'pcat2lbl2data_data2ptr', 'cat2lbl2data_idx', 'cat2lbl2data_input_ids', \n",
    "    'cat2lbl2data_attention_mask', 'cat2lbl2data_data2ptr',\n",
    "    \n",
    "    'phlk2data_idx', 'phlk2data_data2ptr', 'hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask', \n",
    "    'hlk2data_data2ptr'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82645e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08292ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1624, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07504e77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
