{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc7979-df70-4741-ad55-849062b9b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6ade5-8db6-4207-b435-3b958cc5a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41bd69-580d-4598-9ec5-39639b8fea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from xcai.core import store_attr\n",
    "from xcai.losses import MultiTriplet\n",
    "from xcai.models.modeling_utils import XCModelOutput\n",
    "\n",
    "from transformers.utils.generic import ModelOutput\n",
    "from transformers import DistilBertPreTrainedModel,DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60d1bd-3967-4f0c-8cdb-903d20153a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import os,torch, pickle, numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from xcai.block import *\n",
    "from xcai.basics import *\n",
    "from xcai.models.PPP0XX import DBT010\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fee19e-3c78-48df-9e69-c61576670948",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf829af-b3d7-41ea-a8cf-53d9e1e5dccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scai/phd/aiz218323/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "\n",
    "block = XCBlock.from_cfg(data_dir, 'data_meta', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                         sampling_features=[('lbl2data,cat2lbl2data',1), ('cat2data',1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4118c7-4fe8-4746-b6a0-7e3384d49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b007a-fc1d-4815-b508-c78e7176caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d7285-d298-4031-ba1e-e9fc986902ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db258f4a-9544-4c0f-800d-79a0340b7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/69-distillation-for-wikiseealso-1-4',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_steps=3000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=25,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=2,\n",
    "    maximum_cluster_size=1600,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None,\n",
    "    fp16=True,\n",
    "    label_names=['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask'],\n",
    "    use_data_metadata_for_representation=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775718d0-7d35-47a2-a9bc-29e0e83da06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-4'\n",
    "output_dir = f\"/home/scai/phd/aiz218323/scratch/outputs/{os.path.basename(model_output)}\"\n",
    "mname = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'\n",
    "\n",
    "model = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=800, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                               n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "\n",
    "model_weight_file,model_weights = f'{mname}/model.safetensors',{}\n",
    "with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "    for k in file.keys(): model_weights[k] = file.get_tensor(k)\n",
    "\n",
    "model.load_state_dict(model_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542dbc0-3935-458b-8cd3-2bea10ff7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d427c89-fc6a-40bb-956f-5216b702c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dc207-056c-4c24-8b75-4d91c5282bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bff941065c41e697f18dd79ac44d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4c6c49c28340bcadd18246572223ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_rep, lbl_rep = learn.get_data_and_lbl_representation(learn.train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa990a6-5b1b-4dc0-a03c-d7c8c0f9ada5",
   "metadata": {},
   "source": [
    "## `CLS001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae5b96-7f75-4487-832a-ddefa056fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLS001(DistilBertPreTrainedModel):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        config, \n",
    "        n_data:int, \n",
    "        n_lbl:int, \n",
    "        num_batch_labels:Optional[int]=None, \n",
    "        batch_size:Optional[int]=None,\n",
    "        margin:Optional[float]=0.3,\n",
    "        num_negatives:Optional[int]=5,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "        store_attr('n_data,n_lbl')\n",
    "        self.data_repr = nn.Embedding(self.n_data, config.dim)\n",
    "        self.lbl_repr = nn.Embedding(self.n_lbl, config.dim)\n",
    "        self.lbl_embeddings = nn.Embedding(self.n_lbl, config.dim)\n",
    "\n",
    "        self.rep_loss_fn = MultiTriplet(bsz=batch_size, tn_targ=num_batch_labels, margin=margin, n_negatives=num_negatives, \n",
    "                                        tau=tau, apply_softmax=apply_softmax, reduce='mean')\n",
    "\n",
    "    def get_lbl_representation(self):\n",
    "        return self.lbl_repr.weight\n",
    "\n",
    "    def get_data_representation(self):\n",
    "        return self.data_repr.weight\n",
    "\n",
    "    def init_representation(self, data_repr:torch.Tensor, lbl_repr:torch.Tensor):\n",
    "        self.data_repr.weight.data = data_repr\n",
    "        self.lbl_repr.weight.data = lbl_repr\n",
    "\n",
    "    def freeze_representation(self):\n",
    "        self.data_repr.requires_grad_(False)\n",
    "        self.lbl_repr.requires_grad_(False)\n",
    "\n",
    "    def freeze_data_representation(self):\n",
    "        self.data_repr.requires_grad_(False)\n",
    "\n",
    "    def unfreeze_representation(self):\n",
    "        self.data_repr.requires_grad_(True)\n",
    "        self.lbl_repr.requires_grad_(True)\n",
    "\n",
    "    def init_meta_embeddings(self):\n",
    "        self.lbl_embeddings.weight.data = torch.zeros_like(self.lbl_embeddings.weight.data)\n",
    "\n",
    "    def compute_loss(self, inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx):\n",
    "        return self.rep_loss_fn(inp_repr, targ_repr, targ_ptr, targ_idx, ptarg_ptr, ptarg_idx)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_idx:torch.Tensor,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "    ):\n",
    "        data_rep = self.data_repr(data_idx)\n",
    "\n",
    "        loss = lbl2data_rep = None\n",
    "        if lbl2data_idx is not None:\n",
    "            lbl2data_rep = F.normalize(self.lbl_repr(lbl2data_idx) + self.lbl_embeddings(lbl2data_idx))\n",
    "\n",
    "            loss = self.compute_loss(data_rep, lbl2data_rep,lbl2data_data2ptr,lbl2data_idx,\n",
    "                                     plbl2data_data2ptr,plbl2data_idx)\n",
    "            \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_rep,\n",
    "            lbl2data_repr=lbl2data_rep,\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52156f0e-0dc9-4b2c-8fb4-ddc5163bf7e1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f269a-a320-42ce-a111-467c0077ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLS001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl, batch_size=100, num_batch_labels=5000, \n",
    "               margin=0.3, num_negatives=10, tau=0.1, apply_softmax=True)\n",
    "model.init_representation(data_rep, lbl_rep)\n",
    "model.freeze_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e39ac-75b8-4a9a-a2f2-02b9ec391c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))\n",
    "b = prepare_batch(model, batch, m_args=['lbl2data_data2ptr', 'lbl2data_idx', 'plbl2data_data2ptr', 'plbl2data_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d1b15-0f1e-4b4e-a064-223d8f33bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fecc8-a330-40e3-a7f6-96ec5f0ec130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0303, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe9497-cb92-4c28-b255-ece6dff4490c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05c49a-8947-415a-b003-2d374799c54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
