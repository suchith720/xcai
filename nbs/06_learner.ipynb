{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b937d98-9beb-445c-bc68-c8d71a8c1ae0",
   "metadata": {},
   "source": [
    "# learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714cf2-0372-49f6-a39c-24752a28145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b498e-a0a3-4bdd-8741-ca516ddc64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ac07c-66a4-40d0-a39b-e5220c20fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from packaging import version\n",
    "import torch, re, math, numpy as np, os, time, datasets, pickle\n",
    "from typing import Any, Tuple, Optional, Sequence, Union, Dict, List, NamedTuple\n",
    "from transformers import AutoTokenizer, BatchEncoding, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.parallel._functions import Scatter\n",
    "from torch.nn.parallel.scatter_gather import _is_namedtuple\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.data import *\n",
    "from xcai.representation.search import *\n",
    "from xcai.generation.trie import *\n",
    "from xcai.generation.generate import *\n",
    "from xcai.clustering.cluster import *\n",
    "from xcai.transform import PadFeatTfm\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from fastcore.dispatch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821000f3-31ca-44ed-a2a5-e060f900ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers.trainer_pt_utils import (\n",
    "    find_batch_size, \n",
    "    nested_concat, nested_numpify, \n",
    "    IterableDatasetShard, \n",
    "    get_dataloader_sampler, \n",
    "    get_model_param_count,\n",
    "    LengthGroupedSampler\n",
    ")\n",
    "from transformers.trainer_utils import has_length, denumpify_detensorize, speed_metrics, TrainOutput, HPSearchBackend, seed_worker\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import _is_peft_model\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.utils import is_sagemaker_mp_enabled, is_accelerate_available, is_torch_tpu_available, logging, is_datasets_available\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "\n",
    "from transformers.integrations import hp_params\n",
    "from transformers.integrations.tpu import tpu_spmd_dataloader\n",
    "from transformers.integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n",
    "\n",
    "if is_accelerate_available():\n",
    "    from accelerate import Accelerator, skip_first_batches\n",
    "    from accelerate import __version__ as accelerate_version\n",
    "    from accelerate.utils import (\n",
    "        DistributedDataParallelKwargs,\n",
    "        DistributedType,\n",
    "        GradientAccumulationPlugin,\n",
    "        load_fsdp_model,\n",
    "        load_fsdp_optimizer,\n",
    "        save_fsdp_model,\n",
    "        save_fsdp_optimizer,\n",
    "    )\n",
    "\n",
    "    DATA_SAMPLERS = [RandomSampler]\n",
    "    if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "        from accelerate.data_loader import SeedableRandomSampler\n",
    "\n",
    "        DATA_SAMPLERS += [SeedableRandomSampler]\n",
    "\n",
    "    if is_deepspeed_available():\n",
    "        from accelerate.utils import DeepSpeedSchedulerWrapper\n",
    "\n",
    "if is_accelerate_available(\"0.28.0\"):\n",
    "    from accelerate.utils import DataLoaderConfiguration\n",
    "\n",
    "TRAINING_ARGS_NAME = \"training_args.bin\"\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "OPTIMIZER_NAME = \"optimizer.pt\"\n",
    "OPTIMIZER_NAME_BIN = \"optimizer.bin\"\n",
    "SCHEDULER_NAME = \"scheduler.pt\"\n",
    "SCALER_NAME = \"scaler.pt\"\n",
    "FSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547561-f805-40c1-a6b2-d89e772ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1b571-0043-4b86-acce-cb6cb0ef598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from xcai.block import *\n",
    "from xcai.models.PPP0XX import *\n",
    "from xcai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9985262",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab5fa-1f75-44e8-b533-525d577f4209",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188e3a7-9458-456f-8a19-104fd139a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4d23b-4525-46bd-9a75-19ffca285235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch = block.train.one_batch(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47e1ed-ad70-475f-a01f-cd9320705796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee43b109060489393f1b4531512f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c7e7b7f64465ea007044a1668c98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf42c7-5471-4ff7-bc65-4020a5eb5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95cdc4-ff76-4cd7-aa20-a54a03f7c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "b = prepare_batch(m, batch, m_args='lbl2data_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef492419-4e5a-4f08-bb1b-3ba3283703d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7a04d-81d7-402d-9a2d-b449b3acd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = m.to('cuda')\n",
    "b = b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64be93-2e54-4fbd-841b-c5d2828f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5e531-8f3c-438e-ac10-b46c42e90093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.9452, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133a0cc-6ce3-4310-a2f4-5e936ac5ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data_distilbert-base-uncased_xcnlg_ngame.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3638794-1429-44d7-9707-1d0e4bf54788",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d9190-682e-4017-954e-fad35c79ab13",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54c7fe-18b7-4502-a68d-5bbca3a75a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scatter(inputs, target_gpus, chunk_sizes=None, dim=0):\n",
    "    def scatter_map(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n",
    "        if _is_namedtuple(obj):\n",
    "            return [type(obj)(*args) for args in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, tuple) and len(obj) > 0:\n",
    "            return list(zip(*map(scatter_map, obj)))\n",
    "        if isinstance(obj, list) and len(obj) > 0:\n",
    "            return [list(i) for i in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, dict) and len(obj) > 0:\n",
    "            return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
    "        return [obj for _ in target_gpus] \n",
    "    try:\n",
    "        res = scatter_map(inputs)\n",
    "    finally:\n",
    "        scatter_map = None\n",
    "    return res\n",
    "    \n",
    "def scatter_kwargs(\n",
    "    inputs: Tuple[Any, ...],\n",
    "    kwargs: Optional[Dict[str, Any]],\n",
    "    target_gpus: Sequence[Union[int, torch.device]],\n",
    "    chunk_sizes: Optional[Sequence[int]]=None,\n",
    "    dim: int = 0,\n",
    ") -> Tuple[Tuple[Any, ...], Tuple[Dict[str, Any], ...]]:\n",
    "    scattered_inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n",
    "    scattered_kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n",
    "    if len(scattered_inputs) < len(scattered_kwargs):\n",
    "        scattered_inputs.extend(() for _ in range(len(scattered_kwargs) - len(scattered_inputs)))\n",
    "    elif len(scattered_kwargs) < len(inputs):\n",
    "        scattered_kwargs.extend({} for _ in range(len(scattered_inputs) - len(scattered_kwargs)))\n",
    "    return scattered_inputs, scattered_kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420b5aa-6d09-47c1-809d-af19e02f800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCDataParallel(DataParallel):\n",
    "\n",
    "    @delegates(DataParallel.__init__)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def _get_feat_name(self, x:Optional[Dict[str, Any]]):\n",
    "        return list(set([k.split('_', maxsplit=1)[0] for k in x]))\n",
    "    \n",
    "    def _extract_feat(self, x:Optional[Dict[str, Any]], prefix:str):\n",
    "        return {k:v for k,v in x.items() if re.match(f'^{prefix}_(?!.*2ptr)', k) or re.match(f'^.*_{prefix}2ptr$', k)}\n",
    "\n",
    "    def scatter(\n",
    "        self,\n",
    "        inputs: Tuple[Any, ...],\n",
    "        kwargs: Optional[Dict[str, Any]],\n",
    "        device_ids: Sequence[Union[int, torch.device]],\n",
    "    ) ->Any:\n",
    "        if len(inputs): raise ValueError('`inputs` should be empty.')    \n",
    "        feat_name = self._get_feat_name(kwargs)\n",
    "        \n",
    "        data_feat = self._extract_feat(kwargs, 'data')\n",
    "        scattered_inputs, scattered_kwargs = scatter_kwargs(inputs, data_feat, device_ids, None, dim=self.dim)\n",
    "        feat_name.remove('data')\n",
    "        \n",
    "        for k in feat_name:\n",
    "            ptr_name = f'{k}_data2ptr'\n",
    "            if ptr_name in scattered_kwargs[0] and scattered_kwargs[0][ptr_name] is not None:\n",
    "                chunk_sz = [o[ptr_name].sum().item() for o in scattered_kwargs]\n",
    "                if len(chunk_sz) < len(device_ids): \n",
    "                    chunk_sz.extend([0 for _ in range(len(device_ids) - len(chunk_sz))])\n",
    "                \n",
    "                feat = self._extract_feat(kwargs, k)\n",
    "                _, o = scatter_kwargs(inputs, feat, device_ids, chunk_sz, dim=self.dim)\n",
    "                for p,q in zip(scattered_kwargs, o): p.update(q)\n",
    "                    \n",
    "        return tuple(scattered_inputs), tuple(scattered_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553913e-875d-44c7-b58e-3b77ab8c4ae7",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, BatchEncoding\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae19912",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/aiscuser/scratch/datasets'\n",
    "pkl_dir = f'{data_dir}/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_rm_radga.pkl', 'rb') as file: \n",
    "    block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_xcnlg_radga.pkl', 'rb') as file: \n",
    "    block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = block.train.one_batch(4)\n",
    "bb = BatchEncoding({k:v for k,v in b.items() if isinstance(v, torch.Tensor)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a78059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbl2data_data2ptr :  torch.Size([4])\n",
      "lbl2data_data2ptr :  torch.Size([4])\n",
      "pcat2data_data2ptr :  torch.Size([4])\n",
      "cat2data_data2ptr :  torch.Size([4])\n",
      "pcat2lbl2data_data2ptr :  torch.Size([4])\n",
      "cat2lbl2data_data2ptr :  torch.Size([4])\n",
      "phlk2data_data2ptr :  torch.Size([4])\n",
      "hlk2data_data2ptr :  torch.Size([4])\n",
      "hlk2lbl2data_data2ptr :  torch.Size([4])\n",
      "hlk2lbl2data_plbl2data2ptr :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for k,v in bb.items():\n",
    "    if 'ptr' in k: print(k, ': ', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484462a0-4806-461a-957c-6ae5d1bd1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        for k,v in kwargs.items(): \n",
    "            if isinstance(v, torch.Tensor): print(k, ': ', v, ', ', v.device)\n",
    "        return kwargs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9a39d-bff3-4588-9fd4-2899208a6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = XCDataParallel(module=MyModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bff3f-e395-4486-9865-19d0f5bff763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbl2data_data2ptrplbl2data_data2ptr :   :  tensor([2, 1], device='cuda:0')tensor([1, 2], device='cuda:1')  , ,   cuda:0cuda:1\n",
      "\n",
      "lbl2data_data2ptrlbl2data_data2ptr  : :   tensor([1, 2], device='cuda:1')tensor([2, 1], device='cuda:0') ,   , cuda:1 \n",
      "cuda:0pcat2data_data2ptr\n",
      " pcat2data_data2ptr:   : tensor([14,  6], device='cuda:1')  tensor([13,  6], device='cuda:0'),   cuda:1,  \n",
      "cat2data_data2ptrcuda:0\n",
      " : cat2data_data2ptr  :  tensor([1, 1], device='cuda:1') tensor([1, 1], device='cuda:0') , ,   cuda:0cuda:1\n",
      "\n",
      "pcat2lbl2data_data2ptrpcat2lbl2data_data2ptr :  :   tensor([4, 7], device='cuda:1')tensor([0, 4], device='cuda:0') ,   , cuda:0 \n",
      "cat2lbl2data_data2ptrcuda:1\n",
      " cat2lbl2data_data2ptr:   :  tensor([0, 1], device='cuda:0') tensor([1, 1], device='cuda:1'),   , cuda:0 \n",
      "cuda:1phlk2data_data2ptr\n",
      " phlk2data_data2ptr:   : tensor([16, 18], device='cuda:0')  , tensor([15, 40], device='cuda:1') cuda:0 \n",
      ", hlk2data_data2ptr  cuda:1: \n",
      "hlk2data_data2ptr  tensor([3, 3], device='cuda:0'):   , tensor([3, 3], device='cuda:1')  , cuda:0 \n",
      "cuda:1data_input_ids\n",
      " data_input_ids:   :  tensor([[ 101, 9808, 4270, 2314,  102,    0,    0],\n",
      "        [ 101, 4748, 3909, 4049,  102,    0,    0]], device='cuda:0') ,  cuda:0tensor([[  101,  2957, 14135,  1006,  2236,  1007,   102],\n",
      "        [  101,  9805,  3676,  2221,  1010,  2662,   102]], device='cuda:1')\n",
      " data_attention_mask,   : cuda:1\n",
      " data_attention_mask : tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0]], device='cuda:0')  ,  cuda:0tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:1')\n",
      "hlk2lbl2data_data2ptr  , :   cuda:1\n",
      "tensor([0, 5], device='cuda:0')hlk2lbl2data_data2ptr  : ,   cuda:0tensor([5, 9], device='cuda:1')\n",
      "cat2data_idx ,  :   cuda:1\n",
      "tensor([152298,  55068], device='cuda:0')cat2data_idx  , :   cuda:0\n",
      "cat2data_input_idstensor([ 54425, 104014], device='cuda:1')  : ,   cuda:1\n",
      "tensor([[  101,  5485,  1997, 18685,  2221,  1010,  5284,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2309,  1011, 21235,  5245,  2121,  2948,   102,     0,     0,\n",
      "             0,     0]], device='cuda:0')cat2data_input_ids  , :   cuda:0\n",
      "cat2data_attention_mask : tensor([[  101,  8055,  2163,  2510,  5073,  2730,  1999,  1996,  2137,  2942,\n",
      "          2162,   102],\n",
      "        [  101,  7973, 17228,  1999,  2662,   102,     0,     0,     0,     0,\n",
      "             0,     0]], device='cuda:1')  ,  cuda:1tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0') \n",
      ", cat2data_attention_mask  cuda:0: \n",
      " cat2lbl2data_idx tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:1'):   ,  tensor([273427], device='cuda:0')cuda:1 , \n",
      "cat2lbl2data_idx  cuda:0\n",
      ": cat2lbl2data_input_ids  tensor([490081, 104014], device='cuda:1'):   ,  tensor([[ 101, 7201, 1997, 2510, 2948,  102]], device='cuda:0') cuda:1\n",
      ", cat2lbl2data_input_ids cuda:0 : \n",
      "cat2lbl2data_attention_mask  : tensor([[  101,  7201,  1997, 11593,   102,     0],\n",
      "        [  101,  7973, 17228,  1999,  2662,   102]], device='cuda:1')  , tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')  cuda:1, \n",
      " cat2lbl2data_attention_maskcuda:0 \n",
      ":  pcat2lbl2data_idx : tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1]], device='cuda:1')  , tensor([498200, 496349, 273427,  92979], device='cuda:0')  , cuda:1 cuda:0\n",
      "\n",
      "pcat2lbl2data_idxlbl2data_idx  :  :  tensor([395355, 107556, 395354, 490081, 547792, 545725, 355196, 153765,  98839,\n",
      "        104014, 119469], device='cuda:1')tensor([101316,  71037,  99923], device='cuda:0') ,   cuda:0, \n",
      " lbl2data_input_idscuda:1\n",
      " lbl2data_idx:   : tensor([[ 101, 2862, 1997, 5284, 5485,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2862, 1997, 5111, 5485,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2862, 1997, 2948, 1997, 1996, 2548, 3987, 2250, 2326,  102,    0,\n",
      "            0,    0]], device='cuda:0')  , tensor([  269, 55151, 55150], device='cuda:1') cuda:0 , \n",
      "lbl2data_attention_mask  cuda:1\n",
      ":  lbl2data_input_ids :  tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], device='cuda:0') ,  tensor([[  101,  2862,  1997,  2137,  2942,  2162, 11593,  1006,  8055,  1007,\n",
      "           102,     0,     0,     0],\n",
      "        [  101,  9805,  3676,  2221,  1010,  2662,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182, 26213,  1999,  9805,  3676,\n",
      "          2221,  1010,  2662,   102]], device='cuda:1')cuda:0\n",
      " pcat2data_idx,   : cuda:1 \n",
      "lbl2data_attention_mask : tensor([ 64717, 159252, 152298, 202322, 242932, 225043, 183785, 191189, 183855,\n",
      "        218989, 242907, 228216, 188424,  54866,  56102,  55068,  55069,  56101,\n",
      "         56100], device='cuda:0')  ,  cuda:0tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')\n",
      " , plbl2data_idx  : cuda:1\n",
      " pcat2data_idxtensor([ 71037, 101316,  99923], device='cuda:0')  :  ,  tensor([ 68276, 106638, 102346, 102550, 367802, 207931,  66312, 133998, 171930,\n",
      "         62824,  54423,  54425, 108939, 281131, 153767, 104014,  98839, 355196,\n",
      "        119469, 153765], device='cuda:1')cuda:0 \n",
      "hlk2lbl2data_plbl2data2ptr,   : cuda:1 \n",
      "plbl2data_idx : tensor([0, 0, 5], device='cuda:0')  ,  tensor([  269, 55150, 55151], device='cuda:1')cuda:0\n",
      " , phlk2data_idx  cuda:1:  \n",
      "hlk2lbl2data_plbl2data2ptr :  tensor([ 113865,    8637,   48624, 1397758,  794132,   13068,     254,   24213,\n",
      "         808587,  310460,  846531,  997655, 1221759,  916835, 1210975,  345267,\n",
      "          77253,   25418,  467468,    2667,  145886,  179309,  467472,   52621,\n",
      "         467465,  467466,    3004,  467473,    2736,  467471,  467464,  467470,\n",
      "         433167,  467469], device='cuda:0') , tensor([5, 4, 5], device='cuda:1')  cuda:0\n",
      ",  hlk2data_idxcuda:1 : \n",
      " phlk2data_idx tensor([ 48624, 794132,    254, 179309, 467472, 433167], device='cuda:0'):   ,  cuda:0\n",
      "tensor([   1571,  241461,    8805,    2582,   89844,  460143,    9824,     297,\n",
      "        1283293,   19288,  567212,   40308,    1464, 1282940,   14059,  948811,\n",
      "          61257,  517280,    4927,   15729,  304807,  243045,  815271,  716531,\n",
      "           7839,    8646,  441558,   20720,  147120,    2932, 2337137,  817356,\n",
      "          46779,   15335,   15675,  484300,  815267,     254,  834837, 1914233,\n",
      "          38369,  948613,  847391, 2455908,  438723,     147,  511595,  561003,\n",
      "          14452,    2894,     317,  836235,   39074,  511775,  403957],\n",
      "       device='cuda:1')hlk2data_input_ids ,  :   cuda:1\n",
      "hlk2data_idx :  tensor([[  101,  9808,  4270,  3842,   102,     0,     0,     0,     0],\n",
      "        [  101, 15237,  8071,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2142,  2163,   102,     0,     0,     0,     0,     0],\n",
      "        [  101, 28667, 11514,  3217, 18252,  3194,   102,     0,     0],\n",
      "        [  101, 11409,  2669,  3246,   102,     0,     0,     0,     0],\n",
      "        [  101, 24185,  4877, 22352, 17947,   102,     0,     0,     0]],\n",
      "       device='cuda:0') tensor([  19288, 1283293,    2582,    7839,    2894,  511595], device='cuda:1'),   cuda:0\n",
      "hlk2data_attention_mask,   : cuda:1 \n",
      "hlk2data_input_ids :  tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0]], device='cuda:0') ,  cuda:0\n",
      "tensor([[  101,  2236,  3738,  1999,  1996,  8055,  2163,  2390,   102],\n",
      "        [  101,  5900,  2110, 10211,   102,     0,     0,     0,     0],\n",
      "        [  101,  9991, 15049,  2808,   102,     0,     0,     0,     0],\n",
      "        [  101,  3534,  2051,  4224,   102,     0,     0,     0,     0],\n",
      "        [  101,  3009,  2653,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  4956,  7778,  2181,   102,     0,     0,     0,     0]],\n",
      "       device='cuda:1')hlk2lbl2data_idx  , :   cuda:1tensor([  40156,  527144, 1560778,  552110,  757054], device='cuda:0') \n",
      ", hlk2data_attention_mask  : cuda:0 \n",
      "hlk2lbl2data_input_ids : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:1')  ,  cuda:1\n",
      "tensor([[  101,  2061, 28400,  8939, 20720, 13714,  2358, 22134,  3334,   102],\n",
      "        [  101,  2460,  2828, 19681,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2632, 13959,  7464,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1054, 22394,  1011,  2465, 27636,   102,     0,     0,     0],\n",
      "        [  101, 20704,  3217, 23475,   102,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')hlk2lbl2data_idx ,  :   cuda:0\n",
      "tensor([  13617, 1644592,   12853,  656719,   42218,  948160,     254,    7140,\n",
      "           2932,   46779,  441558, 1914233, 2337137, 2455908], device='cuda:1')hlk2lbl2data_attention_mask  ,  :  cuda:1\n",
      "hlk2lbl2data_input_ids : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:0')  ,  cuda:0\n",
      "tensor([[  101,  2343,  1997,  1996,  8055,  2163,  1997,  2637,   102,     0],\n",
      "        [  101,  4557, 21863,  5420,  3044,   102,     0,     0,     0,     0],\n",
      "        [  101,  2390,  1997,  5900,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  5340, 20082,  3483,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  3448,  6627,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182, 26213,  1999,  2662,   102],\n",
      "        [  101,  2142,  2163,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182,   102,     0,     0,     0],\n",
      "        [  101,  2662,   102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 11932,  3028,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20287,  5063,  2653,   102,     0,     0,     0,     0,     0],\n",
      "        [  101, 25540,  2571, 26614,  2697,   102,     0,     0,     0,     0],\n",
      "        [  101,  9805,  3676,  1011, 10514, 12079,  6671,   102,     0,     0],\n",
      "        [  101,  9805,  3676,  2221,  3075,   102,     0,     0,     0,     0]],\n",
      "       device='cuda:1') ,  cuda:1\n",
      "hlk2lbl2data_attention_mask :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "o = m(**bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49800f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all([torch.all(bb[k] == o[k].to('cpu')) for k in o.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60df7f-8004-49ec-8cce-eb36a285c8e2",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7b634-3ec5-4b5c-9d1f-75f32fa47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCEvalLoopOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    targ_idx: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    targ_ptr: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    gen_output: Optional[Dict]\n",
    "    repr_output: Optional[Dict]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "\n",
    "class XCPredictionOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    gen_output: Optional[Dict]\n",
    "    repr_output: Optional[Dict]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fb71-7eb7-43c6-96ba-71759aa2b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearningArguments(Seq2SeqTrainingArguments):\n",
    "\n",
    "    @delegates(Seq2SeqTrainingArguments.__init__)\n",
    "    def __init__(self, \n",
    "                 use_encoder_parallel:Optional[bool]=False,\n",
    "                 generation_length_penalty:Optional[float]=1.0,\n",
    "                 generation_eos_token:Optional[int]=102,\n",
    "                 generation_num_beams:Optional[int]=5,\n",
    "                 generation_max_info:Optional[int]=None,\n",
    "                 representation_accumulation_steps:Optional[int]=None,\n",
    "                 representation_attribute:Optional[str]='data_repr',\n",
    "                 representation_num_beams:Optional[int]=5,\n",
    "                 representation_search_type:Optional[str]='INDEX',\n",
    "                 index_space:Optional[str]='cosine', \n",
    "                 index_efc:Optional[int]=300, \n",
    "                 index_m:Optional[int]=100, \n",
    "                 index_efs:Optional[int]=300,\n",
    "                 index_num_threads:Optional[int]=84,\n",
    "                 predict_with_generation:Optional[bool]=False,\n",
    "                 predict_with_representation:Optional[bool]=False,\n",
    "                 output_concatenation_weight:Optional[float]=1.0,\n",
    "                 group_by_cluster:Optional[bool]=False,\n",
    "                 num_clustering_warmup_epochs:Optional[int]=None,\n",
    "                 num_cluster_update_epochs:Optional[int]=1,\n",
    "                 num_cluster_size_update_epochs:Optional[int]=1,\n",
    "                 clustering_type:Optional[str]='EXPO',\n",
    "                 minimum_clusters:Optional[int]=3,\n",
    "                 maximum_clusters:Optional[int]=None,\n",
    "                 minimum_cluster_size:Optional[int]=1,\n",
    "                 maximum_cluster_size:Optional[int]=None,\n",
    "                 clustering_devices:Optional[List]=None,\n",
    "                 target_indices_key:Optional[str]='lbl2data_idx',\n",
    "                 target_pointer_key:Optional[str]='lbl2data_data2ptr',\n",
    "                 data_aug_meta_name:Optional[str]=None,\n",
    "                 augmentation_num_beams:Optional[int]=3,\n",
    "                 predict_with_augmentation:Optional[bool]=False,\n",
    "                 use_augmentation_index_representation:Optional[bool]=False,\n",
    "                 metadata_representation_attribute:Optional[str]='data_repr',\n",
    "                 data_augmentation_attribute:Optional[str]='data_repr',\n",
    "                 use_distributional_representation:Optional[bool]=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('generation_num_beams,generation_length_penalty,generation_max_info,generation_eos_token')\n",
    "        store_attr('representation_accumulation_steps,representation_attribute,representation_num_beams,representation_search_type')\n",
    "        store_attr('index_space,index_efc,index_m,index_efs,index_num_threads')\n",
    "        store_attr('predict_with_generation,predict_with_representation,output_concatenation_weight')\n",
    "        store_attr('group_by_cluster,num_cluster_update_epochs,num_cluster_size_update_epochs,num_clustering_warmup_epochs')\n",
    "        store_attr('clustering_devices,clustering_type,maximum_cluster_size')\n",
    "        store_attr('target_indices_key,target_pointer_key')\n",
    "        store_attr('use_encoder_parallel')\n",
    "        store_attr('data_aug_meta_name,augmentation_num_beams,predict_with_augmentation')\n",
    "        store_attr('use_augmentation_index_representation,metadata_representation_attribute,data_augmentation_attribute')\n",
    "        store_attr('use_distributional_representation')\n",
    "        self.minimum_clusters = max(1, minimum_clusters)\n",
    "        self.maximum_clusters = max(minimum_clusters, maximum_clusters) if maximum_clusters is not None else minimum_clusters\n",
    "        self.minimum_cluster_size = max(1, minimum_cluster_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b036146-ad70-4c47-802c-763f1f2aaf45",
   "metadata": {},
   "source": [
    "### `XCLearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b4bd-cd24-4b96-af56-474dfcbdcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearner(Seq2SeqTrainer):\n",
    "\n",
    "    @delegates(Seq2SeqTrainer.__init__)\n",
    "    def __init__(self, \n",
    "                 trie:Optional[Trie]=None, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tbs = TrieBeamSearch(trie, self.args.generation_eos_token, n_bm=self.args.generation_num_beams, \n",
    "                                  len_penalty=self.args.generation_length_penalty, max_info=self.args.generation_max_info, **kwargs)\n",
    "        self.idxs = (\n",
    "            BruteForceSearch(n_bm=self.args.representation_num_beams)\n",
    "            if self.args.representation_search_type == 'BRUTEFORCE' else\n",
    "            IndexSearch(space=self.args.index_space, efc=self.args.index_efc, m=self.args.index_m, \n",
    "                        efs=self.args.index_efs, n_bm=self.args.representation_num_beams, \n",
    "                        n_threads=self.args.index_num_threads) \n",
    "        )\n",
    "        self.aug_idxs, self.aug_info = None, None \n",
    "        self.aug_pad = PadFeatTfm(pad_tok=self.model.config.pad_token_id, prefix=\"meta\")\n",
    "\n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        if unwrap_model(model) is not model:\n",
    "            return model\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            if (hasattr(model, 'encoder') and isinstance(model.encoder, nn.DataParallel)) or self.args.use_encoder_parallel: return model\n",
    "            else: return XCDataParallel(module=model)\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, eval_dataset:Optional[Dataset]=None, ignore_keys:Optional[List[str]]=None, \n",
    "             metric_key_prefix:str=\"eval\", **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "        if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
    "            gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
    "            \n",
    "        self.gather_function, self._gen_kwargs  = self.accelerator.gather, gen_kwargs\n",
    "        \n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "\n",
    "    def predict(self, test_dataset: Dataset, ignore_keys:Optional[List[str]]=None, \n",
    "            metric_key_prefix:str=\"test\", **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "        if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
    "            gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
    "    \n",
    "        self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
    "        self._memory_tracker.start()\n",
    "    \n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "        start_time = time.time()\n",
    "    \n",
    "        output = self.evaluation_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(metric_key_prefix,start_time,num_samples=output.num_samples,num_steps=math.ceil(output.num_samples / total_batch_size),)\n",
    "        )\n",
    "        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "        return XCPredictionOutput(pred_idx=output.pred_idx, pred_ptr=output.pred_ptr, pred_score=output.pred_score, \n",
    "                              gen_output=output.gen_output, repr_output=output.repr_output, metrics=output.metrics, \n",
    "                              num_samples=output.num_samples)\n",
    "    \n",
    "    def _gather_host_output(self, output, host_output):\n",
    "        if output is not None:\n",
    "            output = self.accelerator.pad_across_processes(output, dim=1, pad_index=-100)\n",
    "            output = self.gather_function((output))\n",
    "            return output if host_output is None else nested_concat(host_output, output, padding_index=-100)\n",
    "        else: return host_output\n",
    "\n",
    "    def _gather_all_output(self, host_output, all_output, to_cpu=True):\n",
    "        if host_output is not None:\n",
    "            if isinstance(host_output, torch.Tensor) and to_cpu: host_output = host_output.cpu()\n",
    "            return host_output if all_output is None else nested_concat(all_output, host_output, padding_index=-100)\n",
    "        else: return all_output\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc228b-c71e-4f8f-a51b-3e495fc5337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_aug_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
    "    dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
    "    dataset = dataset if self.train_dataset is None else self.train_dataset\n",
    "    \n",
    "    aug_meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
    "    if (\n",
    "        dataset is not None and dataset.meta is not None and aug_meta_name is not None and \n",
    "        aug_meta_name in dataset.meta\n",
    "    ):\n",
    "        self.aug_idxs = IndexSearch(space=self.args.index_space, efc=self.args.index_efc, m=self.args.index_m, \n",
    "                                    efs=self.args.index_efs, n_bm=self.args.representation_num_beams, \n",
    "                                    n_threads=self.args.index_num_threads)\n",
    "        \n",
    "        self.aug_info = getattr(dataset.meta[aug_meta_name], 'meta_info')\n",
    "        \n",
    "        aug_dset = MainXCDataset(self.aug_info)\n",
    "        aug_dl = self.get_test_dataloader(aug_dset)\n",
    "        aug_repr = self.get_meta_representation(aug_dl, to_cpu=isinstance(self.aug_idxs, IndexSearch))\n",
    "        if self.args.use_distributional_representation: aug_repr = F.log_softmax(aug_repr, dim=-1)\n",
    "            \n",
    "        self.aug_idxs.build(aug_repr)\n",
    "\n",
    "@patch\n",
    "def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
    "    dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
    "    dataset = dataset if self.train_dataset is None else self.train_dataset\n",
    "    \n",
    "    if dataset is not None:\n",
    "        lbl_dset = dataset.lbl_dset\n",
    "        \n",
    "        meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
    "        if meta_name is not None and dataset.meta is not None and meta_name in dataset.meta:\n",
    "            prefix,lbl_meta,meta_info  = dataset.meta[meta_name].prefix,dataset.meta[meta_name].lbl_meta,dataset.meta[meta_name].meta_info\n",
    "            meta_kwargs = {meta_name: MetaXCDataset(prefix, lbl_meta, lbl_meta, meta_info, n_data_meta_samples=self.args.augmentation_num_beams)}\n",
    "            lbl_dset = XCDataset(lbl_dset, **meta_kwargs)\n",
    "        \n",
    "        lbl_dl = self.get_test_dataloader(lbl_dset)\n",
    "        lbl_repr = self.get_representation(lbl_dl, to_cpu=isinstance(self.idxs, IndexSearch))\n",
    "        if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
    "            \n",
    "        self.idxs.build(lbl_repr)\n",
    "    else: raise ValueError('Failed to build `self.idxs`')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f8f9-c2c1-4391-8ec9-d6ea44346107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def generation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"gen_num_beams\") if \"gen_num_beams\" in kwargs and kwargs[\"gen_num_beams\"] is not None else self.args.generation_num_beams\n",
    "    len_penalty = kwargs.pop(\"length_penalty\") if \"length_penalty\" in kwargs and kwargs[\"length_penalty\"] is not None else self.args.generation_length_penalty\n",
    "    \n",
    "    with torch.no_grad(): o = self.tbs.proc(self.model, inputs.copy(), n_bm=n_bm, len_penalty=len_penalty)\n",
    "        \n",
    "    return {'pred_idx':o['info2seq2data_idx'], 'pred_score':o['info2seq2data_score'], 'pred_ptr':o['info2seq2data_data2ptr']}\n",
    "\n",
    "@patch\n",
    "def representation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        o = getattr(model(**inputs), self.args.representation_attribute)\n",
    "        if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
    "            \n",
    "    o = self.idxs.proc(o, n_bm=n_bm)\n",
    "        \n",
    "    return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
    "\n",
    "@patch\n",
    "def augmentation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    if self.aug_idxs is None: raise ValueError('Augmentation `aug_idx` is not initialized.')\n",
    "        \n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"aug_num_beams\") if \"aug_num_beams\" in kwargs and kwargs[\"aug_num_beams\"] is not None else self.args.augmentation_num_beams\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        o = getattr(model(**inputs), self.args.data_augmentation_attribute)\n",
    "        if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
    "            \n",
    "    o = self.aug_idxs.proc(o, n_bm=n_bm)\n",
    "    \n",
    "    aug_info = self.aug_pad({\n",
    "        'meta_input_ids':[self.aug_info['input_ids'][i] for i in o['info2data_idx']], \n",
    "        'meta_attention_mask':[self.aug_info['input_ids'][i] for i in o['info2data_idx']]\n",
    "    })\n",
    "    \n",
    "    if self.args.use_augmentation_index_representation:\n",
    "        meta_repr = torch.tensor(self.aug_idxs.index.get_items(o['info2data_idx']))\n",
    "        return {\n",
    "            f'{self.args.data_aug_meta_name}2data_meta_repr': meta_repr,\n",
    "            f'{self.args.data_aug_meta_name}2data_attention_mask': aug_info['meta_attention_mask'],\n",
    "            f'{self.args.data_aug_meta_name}2data_data2ptr': o['info2data_data2ptr'],\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            f'{self.args.data_aug_meta_name}2data_idx':o['info2data_idx'], \n",
    "            f'{self.args.data_aug_meta_name}2data_input_ids': aug_info['meta_input_ids'], \n",
    "            f'{self.args.data_aug_meta_name}2data_attention_mask': aug_info['meta_attention_mask'],\n",
    "            f'{self.args.data_aug_meta_name}2data_data2ptr': o['info2data_data2ptr']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967d99a-b531-4ff0-83a7-6808c0a8ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _perform_generation(self:XCLearner, model:nn.Module, predict_with_generation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_generation = self.args.predict_with_generation if predict_with_generation is None else predict_with_generation\n",
    "    return getattr(model,'use_generation') if hasattr(model,'use_generation') else predict_with_generation\n",
    "\n",
    "@patch\n",
    "def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
    "    return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
    "\n",
    "@patch\n",
    "def _perform_augmentation(self:XCLearner, model:nn.Module, predict_with_augmentation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_augmentation = self.args.predict_with_augmentation if predict_with_augmentation is None else predict_with_augmentation\n",
    "    return getattr(model,'use_augmentation') if hasattr(model,'use_augmentation') else predict_with_augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351dde7-b678-41d5-b710-005cb7db6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def resize_pred(cls:XCLearner, t, n_t):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    r_t = torch.ones((len(t),), dtype=xn_t.dtype, device=xn_t.device).scatter(0, t_ptr, xn_t)\n",
    "    xt = t.repeat_interleave(r_t).view(len(n_t), -1)\n",
    "    return xt\n",
    "\n",
    "@patch\n",
    "def output_mask(cls:XCLearner, n_t, l):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    mask_ptr = t_ptr+torch.arange(len(t_ptr), device=t_ptr.device)+1\n",
    "    mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype, device=mask_ptr.device).scatter(0, mask_ptr, 0)\n",
    "    r_mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype, device=mask_ptr.device).scatter(0, mask_ptr, xn_t-1)\n",
    "    mask = mask.repeat_interleave(r_mask).view(len(n_t), -1)\n",
    "    return mask\n",
    "\n",
    "@patch\n",
    "def resize_output(cls:XCLearner, pred_idx, pred_score, pred_ptr):\n",
    "    return cls.resize_pred(pred_idx, pred_ptr), cls.resize_pred(pred_score, pred_ptr), cls.output_mask(pred_ptr, len(pred_idx)), pred_ptr\n",
    "\n",
    "@patch\n",
    "def concatenate_output(cls:XCLearner, gen_o:Dict, repr_o:Dict):\n",
    "    gen_o['pred_score'] = torch.exp(gen_o['pred_score'])*cls.args.output_concatenation_weight\n",
    "    gen_o, repr_o = cls.resize_output(**gen_o), cls.resize_output(**repr_o)\n",
    "    pred_idx, pred_score, mask = [torch.hstack([gen_o[i], repr_o[i].cpu()]).flatten() for i in range(3)]\n",
    "    idx = torch.where(mask)[0]\n",
    "    return {\n",
    "        'pred_idx': pred_idx[idx],\n",
    "        'pred_score': pred_score[idx],\n",
    "        'pred_ptr': gen_o[3]+repr_o[3].cpu(),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e5ae1-6971-416b-9b11-1e6234632c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prediction_step(\n",
    "    self:XCLearner,\n",
    "    model: nn.Module,\n",
    "    inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "    prediction_loss_only: bool,\n",
    "    predict_with_generation: bool,\n",
    "    predict_with_representation: bool,\n",
    "    predict_with_augmentation:Optional[bool]=None,\n",
    "    ignore_keys: Optional[List[str]] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "    with torch.no_grad():\n",
    "        with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
    "        loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "    prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
    "    if prediction_loss_only: return loss, {}\n",
    "    \n",
    "    if self._perform_augmentation(model, predict_with_augmentation): \n",
    "        aug_inputs = self.augmentation_output(model, inputs, **kwargs)\n",
    "        inputs.update(aug_inputs)\n",
    "        \n",
    "    output, gen_o, repr_o = None, None, None\n",
    "    if self._perform_generation(model, predict_with_generation): gen_o = self.generation_output(model, inputs, **kwargs)\n",
    "    if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
    "    \n",
    "    if gen_o is not None and repr_o is not None:\n",
    "        output = {f'{k}_gen':v for k,v in gen_o.items()}\n",
    "        output.update({f'{k}_repr':v for k,v in repr_o.items()})\n",
    "        output.update(self.concatenate_output(gen_o, repr_o))\n",
    "    else:\n",
    "        output = gen_o if repr_o is None else repr_o\n",
    "        \n",
    "    labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
    "    if labels is not None: output.update(labels)\n",
    "    \n",
    "    return loss, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189c5d7-a677-43e6-a78d-7485c07d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluation_loop(\n",
    "    self:XCLearner,\n",
    "    dataloader:DataLoader,\n",
    "    description:str,\n",
    "    prediction_loss_only:Optional[bool] = None,\n",
    "    predict_with_generation:Optional[bool]=None,\n",
    "    predict_with_representation:Optional[bool]=None,\n",
    "    ignore_keys:Optional[List[str]] = None,\n",
    "    metric_key_prefix:str=\"eval\",\n",
    ") -> XCEvalLoopOutput:\n",
    "    args = self.args\n",
    "    prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "    model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "    if len(self.accelerator._models) == 0 and model is self.model:\n",
    "        model = self.accelerator.prepare(model) if self.is_deepspeed_enabled else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "        if self.is_fsdp_enabled: self.model = model\n",
    "        if model is not self.model: self.model_wrapped = model\n",
    "        if self.is_deepspeed_enabled: self.deepspeed = self.model_wrapped\n",
    "\n",
    "    batch_size = self.args.eval_batch_size\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    self.callback_handler.eval_dataloader = dataloader\n",
    "    eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "    \n",
    "    if self._perform_representation(unwrap_model(model)) and not prediction_loss_only: \n",
    "        self._build_lbl_index(eval_dataset)\n",
    "            \n",
    "    if self._perform_augmentation(unwrap_model(model)) and not prediction_loss_only: \n",
    "        self._build_aug_index(eval_dataset)\n",
    "    \n",
    "    if args.past_index >= 0: self._past = None\n",
    "\n",
    "    losses_host, all_losses = None, None\n",
    "    host_output, all_output = {}, {}\n",
    "    \n",
    "    observed_num_examples = 0\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        observed_batch_size = find_batch_size(inputs)\n",
    "        if observed_batch_size is not None:\n",
    "            observed_num_examples += observed_batch_size\n",
    "            if batch_size is None: batch_size = observed_batch_size\n",
    "                \n",
    "        loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
    "        \n",
    "        if loss is not None:\n",
    "            losses = self.gather_function((loss.repeat(batch_size)))\n",
    "            losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
    "        for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
    "            \n",
    "        self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "        \n",
    "        if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "            if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "            for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "    \n",
    "    self.gather_function = self.accelerator.gather_for_metrics\n",
    "    if args.past_index and hasattr(self, \"_past\"): delattr(self, \"_past\")\n",
    "\n",
    "    if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "    for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "        \n",
    "    if has_length(eval_dataset): num_samples = len(eval_dataset)\n",
    "    elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "        num_samples = eval_dataset.num_examples\n",
    "    else:\n",
    "        if has_length(dataloader): num_samples = self.num_examples(dataloader)\n",
    "        else: num_samples = observed_num_examples\n",
    "    if num_samples == 0 and observed_num_examples > 0: num_samples = observed_num_examples\n",
    "        \n",
    "    gen_output, repr_output = None, None\n",
    "    metric_input_keys = ['targ_idx', 'targ_ptr', 'pred_idx', 'pred_ptr', 'pred_score']\n",
    "    if 'pred_idx_gen' in all_output and all_output['pred_idx_gen'] is not None:\n",
    "        gen_output = {o:all_output[f'{o}_gen' if o.startswith('pred_') else o] for o in metric_input_keys}\n",
    "    if 'pred_idx_repr' in all_output and all_output['pred_idx_repr'] is not None:\n",
    "        repr_output = {o:all_output[f'{o}_repr' if o.startswith('pred_') else o] for o in metric_input_keys}\n",
    "    \n",
    "\n",
    "    if (self.compute_metrics is not None and \n",
    "        'targ_idx' in all_output and all_output['targ_idx'] is not None and \n",
    "        'pred_idx' in all_output and all_output['pred_idx'] is not None):\n",
    "        \n",
    "        metrics = self.compute_metrics(**{o:all_output[o] for o in metric_input_keys})\n",
    "        if gen_output is not None:\n",
    "            m = self.compute_metrics(**gen_output)\n",
    "            metrics.update({f'{k}_GEN':v for k,v in m.items()})\n",
    "        if repr_output is not None:\n",
    "            m = self.compute_metrics(**repr_output)\n",
    "            metrics.update({f'{k}_REPR':v for k,v in m.items()})      \n",
    "    else: metrics = {}\n",
    "        \n",
    "    metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "    if all_losses is not None: metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "    if hasattr(self, \"jit_compilation_time\"): metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
    "        \n",
    "    for key in list(metrics.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"): metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "    \n",
    "    return XCEvalLoopOutput(pred_idx=all_output.get('pred_idx'), pred_ptr=all_output.get('pred_ptr'), \n",
    "                            pred_score=all_output.get('pred_score'),targ_idx=all_output.get('targ_idx'), \n",
    "                            targ_ptr=all_output.get('targ_ptr'), gen_output=gen_output, repr_output=repr_output,\n",
    "                            metrics=metrics, num_samples=num_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d626f2e-22fa-4b44-82d9-af7d082cb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_meta_representation(self:XCLearner, dataloader: DataLoader, to_cpu:Optional[bool]=True):\n",
    "    data_host, all_data = None, None\n",
    "    \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        use_noise = self.model.disable_noise()\n",
    "    \n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad(): data = getattr(self.model.get_meta_representation(**inputs), self.args.metadata_representation_attribute)\n",
    "        data_host = self._gather_host_output(data, data_host)\n",
    "        if self.args.representation_accumulation_steps is not None and (step + 1) % self.args.representation_accumulation_steps == 0:\n",
    "            all_data, data_host = self._gather_all_output(data_host, all_data, to_cpu=to_cpu), None\n",
    "            \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        self.model.set_noise(use_noise)\n",
    "            \n",
    "    return self._gather_all_output(data_host, all_data, to_cpu=to_cpu)\n",
    "\n",
    "@patch\n",
    "def get_representation(self:XCLearner, dataloader: DataLoader, to_cpu:Optional[bool]=True):\n",
    "    data_host, all_data = None, None\n",
    "    \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        use_noise = self.model.disable_noise()\n",
    "    \n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad(): data = getattr(self.model(**inputs), self.args.representation_attribute)\n",
    "        data_host = self._gather_host_output(data, data_host)\n",
    "        if self.args.representation_accumulation_steps is not None and (step + 1) % self.args.representation_accumulation_steps == 0:\n",
    "            all_data, data_host = self._gather_all_output(data_host, all_data, to_cpu=to_cpu), None\n",
    "            \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        self.model.set_noise(use_noise)\n",
    "            \n",
    "    return self._gather_all_output(data_host, all_data, to_cpu=to_cpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ec06d-459a-4ba0-8b1d-74ff1d84a4ff",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b71a7-4ec2-47c5-a72d-77001b9c9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_train_sampler(self:XCLearner):\n",
    "    if self.train_dataset is None or not has_length(self.train_dataset):\n",
    "        return None\n",
    "        \n",
    "    if self.args.group_by_length:\n",
    "        if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\n",
    "            lengths = (\n",
    "                self.train_dataset[self.args.length_column_name]\n",
    "                if self.args.length_column_name in self.train_dataset.column_names\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            lengths = None\n",
    "        model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n",
    "        return LengthGroupedSampler(\n",
    "            self.args.train_batch_size * self.args.gradient_accumulation_steps,\n",
    "            dataset=self.train_dataset,\n",
    "            lengths=lengths,\n",
    "            model_input_name=model_input_name,\n",
    "        )\n",
    "\n",
    "    elif self.args.group_by_cluster:\n",
    "        return ClusterGroupedSampler(n=len(self.train_dataset))\n",
    "    else:\n",
    "        return RandomSampler(self.train_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cb389-f267-44a3-a65e-39b517b86660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_train_dataloader(self:XCLearner):\n",
    "    if self.train_dataset is None:\n",
    "        raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "    train_dataset = self.train_dataset\n",
    "    data_collator = self.data_collator\n",
    "    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "    else:\n",
    "        data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "    dataloader_params = {\n",
    "        \"batch_size\": self._train_batch_size,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"num_workers\": self.args.dataloader_num_workers,\n",
    "        \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "    }\n",
    "\n",
    "    if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "        dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "        dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "        dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "        dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "    \n",
    "    return DataLoader(train_dataset, **dataloader_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_min_cluster_sz(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    \n",
    "    if self.args.num_clustering_warmup_epochs is not None:\n",
    "        if epochs_trained < self.args.num_clustering_warmup_epochs: return None\n",
    "        else: epochs_trained -= self.args.num_clustering_warmup_epochs\n",
    "    \n",
    "    if self.args.clustering_type == 'LINEAR':\n",
    "        if self.args.maximum_clusters is None: return self.train_dataset.n_data//self.args.minimum_clusters\n",
    "        else:\n",
    "            n_cluster = (self.args.maximum_clusters-self.args.minimum_clusters)/num_train_epochs*epochs_trained\n",
    "            return self.train_dataset.n_data//int(self.args.minimum_clusters+n_cluster)\n",
    "        \n",
    "    elif self.args.clustering_type == 'EXPO':\n",
    "        mult = 2**(epochs_trained//self.args.num_cluster_size_update_epochs)\n",
    "        cluster_sz = self.args.minimum_cluster_size*mult\n",
    "        cluster_sz = (\n",
    "            self.args.maximum_cluster_size \n",
    "            if self.args.maximum_cluster_size is not None and cluster_sz > self.args.maximum_cluster_size \n",
    "            else cluster_sz\n",
    "        )\n",
    "        return cluster_sz\n",
    "    \n",
    "    else: raise ValueError(f'Invalid `clustering_type`({self.args.clustering_type}).')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447567d1-8fc1-4582-9673-34ff1c971048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_train_data_cluster(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    dataset = self.train_dataset.data_dset\n",
    "    dataloader = self.get_test_dataloader(dataset)\n",
    "    data_repr = self.get_representation(dataloader)\n",
    "    \n",
    "    if self.args.use_distributional_representation: data_repr = F.softmax(data_repr, dim=-1)\n",
    "        \n",
    "    cluster = BalancedClusters.proc(data_repr, self._get_min_cluster_sz(epochs_trained, num_train_epochs), clustering_devices=self.args.clustering_devices)\n",
    "    return cluster\n",
    "\n",
    "@patch\n",
    "def update_dataloader_sampler(self:XCLearner, dataloader:DataLoader, epochs_trained:int, num_train_epochs:int):\n",
    "    if isinstance(dataloader.sampler, ClusterGroupedSampler):\n",
    "        cluster = self._get_train_data_cluster(epochs_trained, num_train_epochs)\n",
    "        dataloader.sampler.set_cluster(cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33907726-5248-4c73-84f1-ce900f119ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _validate_group_by_cluster(self:XCLearner):\n",
    "    if self.args.group_by_cluster and (not hasattr(self.model,'use_representation') or  not getattr(unwrap_model(self.model),'use_representation')):\n",
    "        raise ValueError('Cannot use `group_by_cluster` for models without `use_representation`.')\n",
    "        self.args.group_by_cluster = False\n",
    "\n",
    "@patch\n",
    "def _inner_training_loop(\n",
    "    self:XCLearner, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "):\n",
    "    self.accelerator.free_memory()\n",
    "    self._train_batch_size = batch_size\n",
    "    if self.args.auto_find_batch_size:\n",
    "        if self.state.train_batch_size != self._train_batch_size:\n",
    "            from accelerate.utils import release_memory\n",
    "\n",
    "            (self.model_wrapped,) = release_memory(self.model_wrapped)\n",
    "            self.model_wrapped = self.model\n",
    "\n",
    "            # Check for DeepSpeed *after* the intial pass and modify the config\n",
    "            if self.is_deepspeed_enabled:\n",
    "                # Temporarily unset `self.args.train_batch_size`\n",
    "                original_bs = self.args.per_device_train_batch_size\n",
    "                self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\n",
    "                self.propagate_args_to_deepspeed(True)\n",
    "                self.args.per_device_train_batch_size = original_bs\n",
    "        self.state.train_batch_size = self._train_batch_size\n",
    "    logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n",
    "    \n",
    "    # Data loader and number of training steps\n",
    "    self._validate_group_by_cluster()\n",
    "    train_dataloader = self.get_train_dataloader()\n",
    "    \n",
    "    if self.is_fsdp_xla_v2_enabled:\n",
    "        train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
    "\n",
    "    # Setting up training control variables:\n",
    "    # number of training epochs: num_train_epochs\n",
    "    # number of training steps per epoch: num_update_steps_per_epoch\n",
    "    # total number of training steps to execute: max_steps\n",
    "    total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "\n",
    "    len_dataloader = None\n",
    "    num_train_tokens = None\n",
    "    if has_length(train_dataloader):\n",
    "        len_dataloader = len(train_dataloader)\n",
    "        num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
    "        num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "        num_examples = self.num_examples(train_dataloader)\n",
    "        if args.max_steps > 0:\n",
    "            max_steps = args.max_steps\n",
    "            num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "                args.max_steps % num_update_steps_per_epoch > 0\n",
    "            )\n",
    "            # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "            # the best we can do.\n",
    "            num_train_samples = args.max_steps * total_train_batch_size\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = (\n",
    "                    self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "                )\n",
    "        else:\n",
    "            max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "            num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "            num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
    "    elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
    "        max_steps = args.max_steps\n",
    "        # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "        num_train_epochs = sys.maxsize\n",
    "        num_update_steps_per_epoch = max_steps\n",
    "        num_examples = total_train_batch_size * args.max_steps\n",
    "        num_train_samples = args.max_steps * total_train_batch_size\n",
    "        if args.include_tokens_per_second:\n",
    "            num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n",
    "            f\" {args.max_steps}\"\n",
    "        )\n",
    "\n",
    "    if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
    "        if self.args.n_gpu > 1:\n",
    "            # nn.DataParallel(model) replicates the model, creating new variables and module\n",
    "            # references registered here no longer work on other gpus, breaking the module\n",
    "            raise ValueError(\n",
    "                \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n",
    "                \" (torchrun or torch.distributed.launch (deprecated)).\"\n",
    "            )\n",
    "        else:\n",
    "            debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
    "\n",
    "    delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n",
    "\n",
    "    # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
    "    if self._created_lr_scheduler:\n",
    "        self.lr_scheduler = None\n",
    "        self._created_lr_scheduler = False\n",
    "\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
    "\n",
    "    if not delay_optimizer_creation:\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    self.state = TrainerState()\n",
    "    self.state.is_hyper_param_search = trial is not None\n",
    "    self.state.train_batch_size = self._train_batch_size\n",
    "\n",
    "    # Compute absolute values for logging, eval, and save if given as ratio\n",
    "    if args.logging_steps is not None:\n",
    "        if args.logging_steps < 1:\n",
    "            self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
    "        else:\n",
    "            self.state.logging_steps = args.logging_steps\n",
    "    if args.eval_steps is not None:\n",
    "        if args.eval_steps < 1:\n",
    "            self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
    "        else:\n",
    "            self.state.eval_steps = args.eval_steps\n",
    "    if args.save_steps is not None:\n",
    "        if args.save_steps < 1:\n",
    "            self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
    "        else:\n",
    "            self.state.save_steps = args.save_steps\n",
    "\n",
    "    # Activate gradient checkpointing if needed\n",
    "    if args.gradient_checkpointing:\n",
    "        if args.gradient_checkpointing_kwargs is None:\n",
    "            gradient_checkpointing_kwargs = {}\n",
    "        else:\n",
    "            gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs\n",
    "\n",
    "        self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "\n",
    "    model = self._wrap_model(self.model_wrapped)\n",
    "\n",
    "    # as the model is wrapped, don't use `accelerator.prepare`\n",
    "    # this is for unhandled cases such as\n",
    "    # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n",
    "    use_accelerator_prepare = True if model is self.model else False\n",
    "\n",
    "    if delay_optimizer_creation:\n",
    "        if use_accelerator_prepare:\n",
    "            self.model = self.accelerator.prepare(self.model)\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    # prepare using `accelerator` prepare\n",
    "    if use_accelerator_prepare:\n",
    "        self.model.train()\n",
    "        if hasattr(self.lr_scheduler, \"step\"):\n",
    "            if self.use_apex:\n",
    "                model = self.accelerator.prepare(self.model)\n",
    "            else:\n",
    "                model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
    "        else:\n",
    "            # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n",
    "            model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "                self.model, self.optimizer, self.lr_scheduler\n",
    "            )\n",
    "\n",
    "    if self.is_fsdp_enabled:\n",
    "        self.model = self.model_wrapped = model\n",
    "\n",
    "    # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "    if model is not self.model:\n",
    "        self.model_wrapped = model\n",
    "\n",
    "    # backward compatibility\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.deepspeed = self.model_wrapped\n",
    "\n",
    "    # ckpt loading\n",
    "    if resume_from_checkpoint is not None:\n",
    "        if self.is_deepspeed_enabled:\n",
    "            deepspeed_load_checkpoint(\n",
    "                self.model_wrapped, resume_from_checkpoint, load_module_strict=not _is_peft_model(self.model)\n",
    "            )\n",
    "        elif is_sagemaker_mp_enabled() or self.is_fsdp_enabled:\n",
    "            self._load_from_checkpoint(resume_from_checkpoint, self.model_wrapped)\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
    "\n",
    "    # important: at this point:\n",
    "    # self.model         is the Transformers Model\n",
    "    # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),\n",
    "    # FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {num_examples:,}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
    "    if self.args.per_device_train_batch_size != self._train_batch_size:\n",
    "        logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
    "    logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
    "\n",
    "    self.state.epoch = 0\n",
    "    start_time = time.time()\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    steps_trained_progress_bar = None\n",
    "\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if resume_from_checkpoint is not None and os.path.isfile(\n",
    "        os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
    "    ):\n",
    "        self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
    "        epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "        if not args.ignore_data_skip:\n",
    "            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "            steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "        else:\n",
    "            steps_trained_in_current_epoch = 0\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "        logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "        if not args.ignore_data_skip:\n",
    "            logger.info(\n",
    "                f\"  Will skip the first {epochs_trained} epochs then the first\"\n",
    "                f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n",
    "            )\n",
    "\n",
    "    # Update the references\n",
    "    self.callback_handler.model = self.model\n",
    "    self.callback_handler.optimizer = self.optimizer\n",
    "    self.callback_handler.lr_scheduler = self.lr_scheduler\n",
    "    self.callback_handler.train_dataloader = train_dataloader\n",
    "    if self.hp_name is not None and self._trial is not None:\n",
    "        # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n",
    "        # parameter to Train when using DDP.\n",
    "        self.state.trial_name = self.hp_name(self._trial)\n",
    "    if trial is not None:\n",
    "        assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
    "        self.state.trial_params = hp_params(assignments)\n",
    "    else:\n",
    "        self.state.trial_params = None\n",
    "    # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "    # to set this after the load.\n",
    "    self.state.max_steps = max_steps\n",
    "    self.state.num_train_epochs = num_train_epochs\n",
    "    self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "    self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "    # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n",
    "    tr_loss = torch.tensor(0.0).to(args.device)\n",
    "    # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n",
    "    self._total_loss_scalar = 0.0\n",
    "    self._globalstep_last_logged = self.state.global_step\n",
    "    model.zero_grad()\n",
    "    grad_norm: Optional[float] = None\n",
    "\n",
    "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "\n",
    "    # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n",
    "    if not args.ignore_data_skip:\n",
    "        for epoch in range(epochs_trained):\n",
    "            sampler = get_dataloader_sampler(train_dataloader)\n",
    "            sampler_kinds = [RandomSampler]\n",
    "            if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "                sampler_kinds.append(SeedableRandomSampler)\n",
    "            is_random_sampler = isinstance(sampler, tuple(sampler_kinds))\n",
    "            if not is_random_sampler:\n",
    "                # We just need to begin an iteration to create the randomization of the sampler.\n",
    "                for _ in train_dataloader:\n",
    "                    break\n",
    "            else:\n",
    "                # Otherwise we need to call the whooooole sampler cause there is some random operation added\n",
    "                # AT THE VERY END!\n",
    "                sampler = sampler if sampler is not None else []\n",
    "                _ = list(sampler)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(epochs_trained, num_train_epochs):\n",
    "        if self.args.group_by_cluster and (epoch % self.args.num_cluster_update_epochs == 0 or epoch == self.args.num_clustering_warmup_epochs) and epoch >= self.args.num_clustering_warmup_epochs:\n",
    "            self.update_dataloader_sampler(train_dataloader, epoch, num_train_epochs)\n",
    "        \n",
    "        epoch_iterator = train_dataloader\n",
    "        if hasattr(epoch_iterator, \"set_epoch\"):\n",
    "            epoch_iterator.set_epoch(epoch)\n",
    "\n",
    "        # Reset the past mems state at the beginning of each epoch if necessary.\n",
    "        if args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        steps_in_epoch = (\n",
    "            len(epoch_iterator)\n",
    "            if len_dataloader is not None\n",
    "            else args.max_steps * args.gradient_accumulation_steps\n",
    "        )\n",
    "        self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n",
    "\n",
    "        if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "            self._load_rng_state(resume_from_checkpoint)\n",
    "\n",
    "        rng_to_sync = False\n",
    "        steps_skipped = 0\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n",
    "            steps_skipped = steps_trained_in_current_epoch\n",
    "            steps_trained_in_current_epoch = 0\n",
    "            rng_to_sync = True\n",
    "\n",
    "        step = -1\n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            if self.args.include_num_input_tokens_seen:\n",
    "                main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n",
    "                if main_input_name not in inputs:\n",
    "                    logger.warning(\n",
    "                        \"Tried to track the number of tokens seen, however the current model is \"\n",
    "                        \"not configured properly to know what item is the input. To fix this, add \"\n",
    "                        \"a `main_input_name` attribute to the model class you are using.\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.state.num_input_tokens_seen += self.accelerator.gather(inputs[main_input_name]).numel()\n",
    "            if rng_to_sync:\n",
    "                self._load_rng_state(resume_from_checkpoint)\n",
    "                rng_to_sync = False\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                if steps_trained_progress_bar is not None:\n",
    "                    steps_trained_progress_bar.update(1)\n",
    "                if steps_trained_in_current_epoch == 0:\n",
    "                    self._load_rng_state(resume_from_checkpoint)\n",
    "                continue\n",
    "            elif steps_trained_progress_bar is not None:\n",
    "                steps_trained_progress_bar.close()\n",
    "                steps_trained_progress_bar = None\n",
    "\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
    "\n",
    "            with self.accelerator.accumulate(model):\n",
    "                tr_loss_step = self.training_step(model, inputs)\n",
    "\n",
    "            if (\n",
    "                args.logging_nan_inf_filter\n",
    "                and not is_torch_tpu_available()\n",
    "                and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "            ):\n",
    "                # if loss is nan or inf simply add the average of previous logged losses\n",
    "                tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "            else:\n",
    "                tr_loss += tr_loss_step\n",
    "\n",
    "            self.current_flos += float(self.floating_point_ops(inputs))\n",
    "\n",
    "            is_last_step_and_steps_less_than_grad_acc = (\n",
    "                steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                total_batched_samples % args.gradient_accumulation_steps == 0\n",
    "                or\n",
    "                # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                is_last_step_and_steps_less_than_grad_acc\n",
    "            ):\n",
    "                # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\n",
    "                # in accelerate. So, explicitly enable sync gradients to True in that case.\n",
    "                if is_last_step_and_steps_less_than_grad_acc:\n",
    "                    self.accelerator.gradient_state._set_sync_gradients(True)\n",
    "\n",
    "                # Gradient clipping\n",
    "                if args.max_grad_norm is not None and args.max_grad_norm > 0:\n",
    "                    # deepspeed does its own clipping\n",
    "\n",
    "                    if is_sagemaker_mp_enabled() and args.fp16:\n",
    "                        _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n",
    "                    elif self.use_apex:\n",
    "                        # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "                        _grad_norm = nn.utils.clip_grad_norm_(\n",
    "                            amp.master_params(self.optimizer),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "                    else:\n",
    "                        _grad_norm = self.accelerator.clip_grad_norm_(\n",
    "                            model.parameters(),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "\n",
    "                    if (\n",
    "                        is_accelerate_available()\n",
    "                        and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n",
    "                    ):\n",
    "                        grad_norm = model.get_global_grad_norm()\n",
    "                    else:\n",
    "                        grad_norm = _grad_norm.item() if _grad_norm is not None else None\n",
    "\n",
    "                # Optimizer step\n",
    "                self.optimizer.step()\n",
    "                optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n",
    "                if optimizer_was_run:\n",
    "                    # Delay optimizer scheduling until metrics are generated\n",
    "                    if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                self.state.global_step += 1\n",
    "                self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n",
    "                self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
    "\n",
    "                self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "            else:\n",
    "                self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
    "\n",
    "            if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "                # PyTorch/XLA relies on the data loader to insert the mark_step for\n",
    "                # each step. Since we are breaking the loop early, we need to manually\n",
    "                # insert the mark_step here.\n",
    "                if is_torch_tpu_available():\n",
    "                    xm.mark_step()\n",
    "                break\n",
    "        if step < 0:\n",
    "            logger.warning(\n",
    "                \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "                f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n",
    "                f\" num_steps ({max_steps}) higher than the number of available samples.\"\n",
    "            )\n",
    "            self.control.should_training_stop = True\n",
    "\n",
    "        self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
    "        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            if is_torch_tpu_available():\n",
    "                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                xm.master_print(met.metrics_report())\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n",
    "                    \"configured. Check your training configuration if this is unexpected.\"\n",
    "                )\n",
    "        if self.control.should_training_stop:\n",
    "            break\n",
    "\n",
    "    if args.past_index and hasattr(self, \"_past\"):\n",
    "        # Clean the state at the end of training\n",
    "        delattr(self, \"_past\")\n",
    "\n",
    "    logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "    if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n",
    "        # Wait for everyone to get here so we are sure the model has been saved by process 0.\n",
    "        if is_torch_tpu_available():\n",
    "            xm.rendezvous(\"load_best_model_at_end\")\n",
    "        elif args.parallel_mode == ParallelMode.DISTRIBUTED:\n",
    "            dist.barrier()\n",
    "        elif is_sagemaker_mp_enabled():\n",
    "            smp.barrier()\n",
    "\n",
    "        self._load_best_model()\n",
    "\n",
    "    # add remaining tr_loss\n",
    "    self._total_loss_scalar += tr_loss.item()\n",
    "    train_loss = self._total_loss_scalar / self.state.global_step\n",
    "\n",
    "    metrics = speed_metrics(\n",
    "        \"train\",\n",
    "        start_time,\n",
    "        num_samples=num_train_samples,\n",
    "        num_steps=self.state.max_steps,\n",
    "        num_tokens=num_train_tokens,\n",
    "    )\n",
    "    self.store_flos()\n",
    "    metrics[\"total_flos\"] = self.state.total_flos\n",
    "    metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "    self.is_in_train = False\n",
    "\n",
    "    self._memory_tracker.stop_and_update_metrics(metrics)\n",
    "\n",
    "    self.log(metrics)\n",
    "\n",
    "    run_dir = self._get_output_dir(trial)\n",
    "    checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)\n",
    "\n",
    "    # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.\n",
    "    if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:\n",
    "        for checkpoint in checkpoints_sorted:\n",
    "            if not os.path.samefile(checkpoint, self.state.best_model_checkpoint):\n",
    "                logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n",
    "                shutil.rmtree(checkpoint)\n",
    "\n",
    "    self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "\n",
    "    # Wait for the checkpoint to be uploaded.\n",
    "    self._finish_current_push()\n",
    "\n",
    "    # After training we make sure to retrieve back the original forward pass method\n",
    "    # for the embedding layer by removing the forward post hook.\n",
    "    if self.neftune_noise_alpha is not None:\n",
    "        self._deactivate_neftune(self.model)\n",
    "\n",
    "    return TrainOutput(self.state.global_step, train_loss, metrics)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc27f85-b8d7-4147-9e19-2ea8f95f7d6d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90781b5d-e073-4e77-b102-cdf72a55c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b7ca-4600-4066-bfa4-a347724e9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/scratch/scai/phd/aiz218323/scratch/outputs/default/',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=50,\n",
    "    eval_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    representation_accumulation_steps=10,\n",
    "    representation_attribute='data_repr',\n",
    "    representation_search_type='INDEX',\n",
    "    evaluation_strategy='steps',\n",
    "    label_names=['lbl2data_idx'],\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=3,\n",
    "    num_cluster_size_update_epochs=2,\n",
    "    use_distributional_representation=False,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=1,\n",
    "    maximum_cluster_size=4,\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258a9bf-9a0a-4fbe-9ca9-55e375780297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT012 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "model = DBT012.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', margin=0.3, tau=0.1, psi=0.5,\n",
    "                               n_negatives=10, apply_softmax=True, use_encoder_parallel=False)\n",
    "model.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4b300-50b2-40b4-b29d-40b6d578d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb74349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, valid_dset = block.train.dset.sample(n=1000), block.test.dset.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5bd9b-3f60-4eaa-8d6d-ddc3e2e6fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, valid_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl, \n",
    "                  pk=5, rk=5, rep_pk=[1, 3, 5], rep_rk=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc998894-1a9f-4c12-8c75-2f4ff5758302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    data_collator=block.collator, \n",
    "    train_dataset=train_dset, \n",
    "    eval_dataset=valid_dset,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b369f0f-f0aa-4ba5-a311-e79ca5ff84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(16)_inner_training_loop()\n",
      "     14     #debug\n",
      "     15 \n",
      "---> 16     self.accelerator.free_memory()\n",
      "     17     self._train_batch_size = batch_size\n",
      "     18     if self.args.auto_find_batch_size:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(17)_inner_training_loop()\n",
      "     15 \n",
      "     16     self.accelerator.free_memory()\n",
      "---> 17     self._train_batch_size = batch_size\n",
      "     18     if self.args.auto_find_batch_size:\n",
      "     19         if self.state.train_batch_size != self._train_batch_size:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(18)_inner_training_loop()\n",
      "     16     self.accelerator.free_memory()\n",
      "     17     self._train_batch_size = batch_size\n",
      "---> 18     if self.args.auto_find_batch_size:\n",
      "     19         if self.state.train_batch_size != self._train_batch_size:\n",
      "     20             from accelerate.utils import release_memory\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(33)_inner_training_loop()\n",
      "     31                 self.args.per_device_train_batch_size = original_bs\n",
      "     32         self.state.train_batch_size = self._train_batch_size\n",
      "---> 33     logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n",
      "     34 \n",
      "     35     # Data loader and number of training steps\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(36)_inner_training_loop()\n",
      "     34 \n",
      "     35     # Data loader and number of training steps\n",
      "---> 36     self._validate_group_by_cluster()\n",
      "     37     train_dataloader = self.get_train_dataloader()\n",
      "     38 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(37)_inner_training_loop()\n",
      "     35     # Data loader and number of training steps\n",
      "     36     self._validate_group_by_cluster()\n",
      "---> 37     train_dataloader = self.get_train_dataloader()\n",
      "     38 \n",
      "     39     if self.is_fsdp_xla_v2_enabled:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(39)_inner_training_loop()\n",
      "     37     train_dataloader = self.get_train_dataloader()\n",
      "     38 \n",
      "---> 39     if self.is_fsdp_xla_v2_enabled:\n",
      "     40         train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
      "     41 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(46)_inner_training_loop()\n",
      "     44     # number of training steps per epoch: num_update_steps_per_epoch\n",
      "     45     # total number of training steps to execute: max_steps\n",
      "---> 46     total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
      "     47 \n",
      "     48     len_dataloader = None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(48)_inner_training_loop()\n",
      "     46     total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
      "     47 \n",
      "---> 48     len_dataloader = None\n",
      "     49     num_train_tokens = None\n",
      "     50     if has_length(train_dataloader):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(49)_inner_training_loop()\n",
      "     47 \n",
      "     48     len_dataloader = None\n",
      "---> 49     num_train_tokens = None\n",
      "     50     if has_length(train_dataloader):\n",
      "     51         len_dataloader = len(train_dataloader)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(50)_inner_training_loop()\n",
      "     48     len_dataloader = None\n",
      "     49     num_train_tokens = None\n",
      "---> 50     if has_length(train_dataloader):\n",
      "     51         len_dataloader = len(train_dataloader)\n",
      "     52         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(51)_inner_training_loop()\n",
      "     49     num_train_tokens = None\n",
      "     50     if has_length(train_dataloader):\n",
      "---> 51         len_dataloader = len(train_dataloader)\n",
      "     52         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
      "     53         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(52)_inner_training_loop()\n",
      "     50     if has_length(train_dataloader):\n",
      "     51         len_dataloader = len(train_dataloader)\n",
      "---> 52         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
      "     53         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
      "     54         num_examples = self.num_examples(train_dataloader)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(53)_inner_training_loop()\n",
      "     51         len_dataloader = len(train_dataloader)\n",
      "     52         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
      "---> 53         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
      "     54         num_examples = self.num_examples(train_dataloader)\n",
      "     55         if args.max_steps > 0:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(54)_inner_training_loop()\n",
      "     52         num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
      "     53         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
      "---> 54         num_examples = self.num_examples(train_dataloader)\n",
      "     55         if args.max_steps > 0:\n",
      "     56             max_steps = args.max_steps\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(55)_inner_training_loop()\n",
      "     53         num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
      "     54         num_examples = self.num_examples(train_dataloader)\n",
      "---> 55         if args.max_steps > 0:\n",
      "     56             max_steps = args.max_steps\n",
      "     57             num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(68)_inner_training_loop()\n",
      "     66                 )\n",
      "     67         else:\n",
      "---> 68             max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
      "     69             num_train_epochs = math.ceil(args.num_train_epochs)\n",
      "     70             num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(69)_inner_training_loop()\n",
      "     67         else:\n",
      "     68             max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
      "---> 69             num_train_epochs = math.ceil(args.num_train_epochs)\n",
      "     70             num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
      "     71             if args.include_tokens_per_second:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(70)_inner_training_loop()\n",
      "     68             max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
      "     69             num_train_epochs = math.ceil(args.num_train_epochs)\n",
      "---> 70             num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
      "     71             if args.include_tokens_per_second:\n",
      "     72                 num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(71)_inner_training_loop()\n",
      "     69             num_train_epochs = math.ceil(args.num_train_epochs)\n",
      "     70             num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
      "---> 71             if args.include_tokens_per_second:\n",
      "     72                 num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
      "     73     elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(88)_inner_training_loop()\n",
      "     86         )\n",
      "     87 \n",
      "---> 88     if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
      "     89         if self.args.n_gpu > 1:\n",
      "     90             # nn.DataParallel(model) replicates the model, creating new variables and module\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(99)_inner_training_loop()\n",
      "     97             debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
      "     98 \n",
      "---> 99     delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n",
      "    100 \n",
      "    101     # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(102)_inner_training_loop()\n",
      "    100 \n",
      "    101     # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
      "--> 102     if self._created_lr_scheduler:\n",
      "    103         self.lr_scheduler = None\n",
      "    104         self._created_lr_scheduler = False\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(103)_inner_training_loop()\n",
      "    101     # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
      "    102     if self._created_lr_scheduler:\n",
      "--> 103         self.lr_scheduler = None\n",
      "    104         self._created_lr_scheduler = False\n",
      "    105 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(104)_inner_training_loop()\n",
      "    102     if self._created_lr_scheduler:\n",
      "    103         self.lr_scheduler = None\n",
      "--> 104         self._created_lr_scheduler = False\n",
      "    105 \n",
      "    106     if self.is_deepspeed_enabled:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(106)_inner_training_loop()\n",
      "    104         self._created_lr_scheduler = False\n",
      "    105 \n",
      "--> 106     if self.is_deepspeed_enabled:\n",
      "    107         self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
      "    108 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(109)_inner_training_loop()\n",
      "    107         self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
      "    108 \n",
      "--> 109     if not delay_optimizer_creation:\n",
      "    110         self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
      "    111 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(110)_inner_training_loop()\n",
      "    108 \n",
      "    109     if not delay_optimizer_creation:\n",
      "--> 110         self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
      "    111 \n",
      "    112     self.state = TrainerState()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(112)_inner_training_loop()\n",
      "    110         self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
      "    111 \n",
      "--> 112     self.state = TrainerState()\n",
      "    113     self.state.is_hyper_param_search = trial is not None\n",
      "    114     self.state.train_batch_size = self._train_batch_size\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(113)_inner_training_loop()\n",
      "    111 \n",
      "    112     self.state = TrainerState()\n",
      "--> 113     self.state.is_hyper_param_search = trial is not None\n",
      "    114     self.state.train_batch_size = self._train_batch_size\n",
      "    115 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerState(epoch=None, global_step=0, max_steps=0, logging_steps=500, eval_steps=500, save_steps=500, train_batch_size=None, num_train_epochs=0, num_input_tokens_seen=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(114)_inner_training_loop()\n",
      "    112     self.state = TrainerState()\n",
      "    113     self.state.is_hyper_param_search = trial is not None\n",
      "--> 114     self.state.train_batch_size = self._train_batch_size\n",
      "    115 \n",
      "    116     # Compute absolute values for logging, eval, and save if given as ratio\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(117)_inner_training_loop()\n",
      "    115 \n",
      "    116     # Compute absolute values for logging, eval, and save if given as ratio\n",
      "--> 117     if args.logging_steps is not None:\n",
      "    118         if args.logging_steps < 1:\n",
      "    119             self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(118)_inner_training_loop()\n",
      "    116     # Compute absolute values for logging, eval, and save if given as ratio\n",
      "    117     if args.logging_steps is not None:\n",
      "--> 118         if args.logging_steps < 1:\n",
      "    119             self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
      "    120         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(121)_inner_training_loop()\n",
      "    119             self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
      "    120         else:\n",
      "--> 121             self.state.logging_steps = args.logging_steps\n",
      "    122     if args.eval_steps is not None:\n",
      "    123         if args.eval_steps < 1:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(122)_inner_training_loop()\n",
      "    120         else:\n",
      "    121             self.state.logging_steps = args.logging_steps\n",
      "--> 122     if args.eval_steps is not None:\n",
      "    123         if args.eval_steps < 1:\n",
      "    124             self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(123)_inner_training_loop()\n",
      "    121             self.state.logging_steps = args.logging_steps\n",
      "    122     if args.eval_steps is not None:\n",
      "--> 123         if args.eval_steps < 1:\n",
      "    124             self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
      "    125         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(126)_inner_training_loop()\n",
      "    124             self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
      "    125         else:\n",
      "--> 126             self.state.eval_steps = args.eval_steps\n",
      "    127     if args.save_steps is not None:\n",
      "    128         if args.save_steps < 1:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(127)_inner_training_loop()\n",
      "    125         else:\n",
      "    126             self.state.eval_steps = args.eval_steps\n",
      "--> 127     if args.save_steps is not None:\n",
      "    128         if args.save_steps < 1:\n",
      "    129             self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(128)_inner_training_loop()\n",
      "    126             self.state.eval_steps = args.eval_steps\n",
      "    127     if args.save_steps is not None:\n",
      "--> 128         if args.save_steps < 1:\n",
      "    129             self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
      "    130         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(131)_inner_training_loop()\n",
      "    129             self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
      "    130         else:\n",
      "--> 131             self.state.save_steps = args.save_steps\n",
      "    132 \n",
      "    133     # Activate gradient checkpointing if needed\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(134)_inner_training_loop()\n",
      "    132 \n",
      "    133     # Activate gradient checkpointing if needed\n",
      "--> 134     if args.gradient_checkpointing:\n",
      "    135         if args.gradient_checkpointing_kwargs is None:\n",
      "    136             gradient_checkpointing_kwargs = {}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(142)_inner_training_loop()\n",
      "    140         self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
      "    141 \n",
      "--> 142     model = self._wrap_model(self.model_wrapped)\n",
      "    143 \n",
      "    144     # as the model is wrapped, don't use `accelerator.prepare`\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(147)_inner_training_loop()\n",
      "    145     # this is for unhandled cases such as\n",
      "    146     # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n",
      "--> 147     use_accelerator_prepare = True if model is self.model else False\n",
      "    148 \n",
      "    149     if delay_optimizer_creation:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(149)_inner_training_loop()\n",
      "    147     use_accelerator_prepare = True if model is self.model else False\n",
      "    148 \n",
      "--> 149     if delay_optimizer_creation:\n",
      "    150         if use_accelerator_prepare:\n",
      "    151             self.model = self.accelerator.prepare(self.model)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  use_accelerator_prepare \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBT012(\n",
      "  (encoder): DBT012Encoder(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dr_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dr_projector): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (loss_fn): Entropy()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.args.use_encoder_parallel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(155)_inner_training_loop()\n",
      "    153 \n",
      "    154     # prepare using `accelerator` prepare\n",
      "--> 155     if use_accelerator_prepare:\n",
      "    156         self.model.train()\n",
      "    157         if hasattr(self.lr_scheduler, \"step\"):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(156)_inner_training_loop()\n",
      "    154     # prepare using `accelerator` prepare\n",
      "    155     if use_accelerator_prepare:\n",
      "--> 156         self.model.train()\n",
      "    157         if hasattr(self.lr_scheduler, \"step\"):\n",
      "    158             if self.use_apex:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(157)_inner_training_loop()\n",
      "    155     if use_accelerator_prepare:\n",
      "    156         self.model.train()\n",
      "--> 157         if hasattr(self.lr_scheduler, \"step\"):\n",
      "    158             if self.use_apex:\n",
      "    159                 model = self.accelerator.prepare(self.model)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(158)_inner_training_loop()\n",
      "    156         self.model.train()\n",
      "    157         if hasattr(self.lr_scheduler, \"step\"):\n",
      "--> 158             if self.use_apex:\n",
      "    159                 model = self.accelerator.prepare(self.model)\n",
      "    160             else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(161)_inner_training_loop()\n",
      "    159                 model = self.accelerator.prepare(self.model)\n",
      "    160             else:\n",
      "--> 161                 model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
      "    162         else:\n",
      "    163             # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBT012(\n",
      "  (encoder): DBT012Encoder(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dr_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dr_projector): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (loss_fn): Entropy()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1151)prepare()\n",
      "   1149         return obj\n",
      "   1150 \n",
      "-> 1151     def prepare(self, *args, device_placement=None):\n",
      "   1152         \"\"\"\n",
      "   1153         Prepare all objects passed in `args` for distributed training and mixed precision, then return them in the same\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1197)prepare()\n",
      "   1195         ```\n",
      "   1196         \"\"\"\n",
      "-> 1197         if device_placement is None:\n",
      "   1198             device_placement = [None for _ in args]\n",
      "   1199         elif self.distributed_type in (DistributedType.DEEPSPEED, DistributedType.MEGATRON_LM):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1198)prepare()\n",
      "   1196         \"\"\"\n",
      "   1197         if device_placement is None:\n",
      "-> 1198             device_placement = [None for _ in args]\n",
      "   1199         elif self.distributed_type in (DistributedType.DEEPSPEED, DistributedType.MEGATRON_LM):\n",
      "   1200             raise ValueError(\"You can't customize device placements with DeepSpeed or Megatron-LM.\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1206)prepare()\n",
      "   1204             )\n",
      "   1205 \n",
      "-> 1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "   1208             if (\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  device_placement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1209)prepare()\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "   1208             if (\n",
      "-> 1209                 isinstance(obj, torch.nn.Module)\n",
      "   1210                 and self.verify_device_map(obj)\n",
      "   1211                 and self.distributed_type != DistributedType.NO\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  obj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBT012(\n",
      "  (encoder): DBT012Encoder(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dr_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dr_projector): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (loss_fn): Entropy()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1208)prepare()\n",
      "   1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "-> 1208             if (\n",
      "   1209                 isinstance(obj, torch.nn.Module)\n",
      "   1210                 and self.verify_device_map(obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1210)prepare()\n",
      "   1208             if (\n",
      "   1209                 isinstance(obj, torch.nn.Module)\n",
      "-> 1210                 and self.verify_device_map(obj)\n",
      "   1211                 and self.distributed_type != DistributedType.NO\n",
      "   1212                 and os.environ.get(\"ACCELERATE_BYPASS_DEVICE_MAP\", \"false\") != \"true\"\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1208)prepare()\n",
      "   1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "-> 1208             if (\n",
      "   1209                 isinstance(obj, torch.nn.Module)\n",
      "   1210                 and self.verify_device_map(obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1206)prepare()\n",
      "   1204             )\n",
      "   1205 \n",
      "-> 1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "   1208             if (\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1209)prepare()\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "   1208             if (\n",
      "-> 1209                 isinstance(obj, torch.nn.Module)\n",
      "   1210                 and self.verify_device_map(obj)\n",
      "   1211                 and self.distributed_type != DistributedType.NO\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  obj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1208)prepare()\n",
      "   1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "-> 1208             if (\n",
      "   1209                 isinstance(obj, torch.nn.Module)\n",
      "   1210                 and self.verify_device_map(obj)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1206)prepare()\n",
      "   1204             )\n",
      "   1205 \n",
      "-> 1206         for obj in args:\n",
      "   1207             # TODO: Look at enabling native TP training directly with a proper config\n",
      "   1208             if (\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1219)prepare()\n",
      "   1217                 )\n",
      "   1218 \n",
      "-> 1219         if self.distributed_type == DistributedType.DEEPSPEED:\n",
      "   1220             model_count = 0\n",
      "   1221             for obj in args:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1232)prepare()\n",
      "   1230         # have parameters disconnected from the model (so no training :-( ).\n",
      "   1231         # If the model and optimizer have parameters on different devices we raise an error.\n",
      "-> 1232         if self.distributed_type == DistributedType.XLA:\n",
      "   1233             model_device, optimizer_device = self._get_devices()\n",
      "   1234             if model_device is not None and optimizer_device is not None and model_device != optimizer_device:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1244)prepare()\n",
      "   1242 \n",
      "   1243         # If we're dealing with device placement, this deals with that by...\n",
      "-> 1244         tpu_should_fix_optimizer = self.device_placement and self.distributed_type == DistributedType.XLA\n",
      "   1245         if tpu_should_fix_optimizer or (self.mixed_precision == \"fp8\" and self.fp8_recipe_handler.backend == \"TE\"):\n",
      "   1246             # 1. grabbing old model parameters\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1245)prepare()\n",
      "   1243         # If we're dealing with device placement, this deals with that by...\n",
      "   1244         tpu_should_fix_optimizer = self.device_placement and self.distributed_type == DistributedType.XLA\n",
      "-> 1245         if tpu_should_fix_optimizer or (self.mixed_precision == \"fp8\" and self.fp8_recipe_handler.backend == \"TE\"):\n",
      "   1246             # 1. grabbing old model parameters\n",
      "   1247             old_named_params = self._get_named_parameters(*args)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1249)prepare()\n",
      "   1247             old_named_params = self._get_named_parameters(*args)\n",
      "   1248 \n",
      "-> 1249         if self.distributed_type in [DistributedType.MULTI_CPU, DistributedType.MULTI_XPU, DistributedType.NO]:\n",
      "   1250             if self.device.type == \"cpu\" and self.state.use_ipex:\n",
      "   1251                 args = self._prepare_ipex(*args)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1250)prepare()\n",
      "   1248 \n",
      "   1249         if self.distributed_type in [DistributedType.MULTI_CPU, DistributedType.MULTI_XPU, DistributedType.NO]:\n",
      "-> 1250             if self.device.type == \"cpu\" and self.state.use_ipex:\n",
      "   1251                 args = self._prepare_ipex(*args)\n",
      "   1252             elif self.device.type == \"xpu\" and is_xpu_available():\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1252)prepare()\n",
      "   1250             if self.device.type == \"cpu\" and self.state.use_ipex:\n",
      "   1251                 args = self._prepare_ipex(*args)\n",
      "-> 1252             elif self.device.type == \"xpu\" and is_xpu_available():\n",
      "   1253                 args = self._prepare_ipex(*args)\n",
      "   1254         if self.distributed_type == DistributedType.DEEPSPEED:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1254)prepare()\n",
      "   1252             elif self.device.type == \"xpu\" and is_xpu_available():\n",
      "   1253                 args = self._prepare_ipex(*args)\n",
      "-> 1254         if self.distributed_type == DistributedType.DEEPSPEED:\n",
      "   1255             result = self._prepare_deepspeed(*args)\n",
      "   1256         elif self.distributed_type == DistributedType.MEGATRON_LM:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1256)prepare()\n",
      "   1254         if self.distributed_type == DistributedType.DEEPSPEED:\n",
      "   1255             result = self._prepare_deepspeed(*args)\n",
      "-> 1256         elif self.distributed_type == DistributedType.MEGATRON_LM:\n",
      "   1257             result = self._prepare_megatron_lm(*args)\n",
      "   1258         else:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1259)prepare()\n",
      "   1257             result = self._prepare_megatron_lm(*args)\n",
      "   1258         else:\n",
      "-> 1259             if self.mixed_precision == \"fp8\" and self.fp8_recipe_handler.backend == \"MSAMP\":\n",
      "   1260                 args = self._prepare_msamp(*args)\n",
      "   1261                 # MS-AMP will handle the device placement\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1263)prepare()\n",
      "   1261                 # MS-AMP will handle the device placement\n",
      "   1262                 device_placement = [False for _ in args]\n",
      "-> 1263             result = tuple(\n",
      "   1264                 self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\n",
      "   1265             )\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1264)prepare()\n",
      "   1262                 device_placement = [False for _ in args]\n",
      "   1263             result = tuple(\n",
      "-> 1264                 self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\n",
      "   1265             )\n",
      "   1266             result = tuple(self._prepare_one(obj, device_placement=d) for obj, d in zip(result, device_placement))\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1263)prepare()\n",
      "   1261                 # MS-AMP will handle the device placement\n",
      "   1262                 device_placement = [False for _ in args]\n",
      "-> 1263             result = tuple(\n",
      "   1264                 self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\n",
      "   1265             )\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1266)prepare()\n",
      "   1264                 self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\n",
      "   1265             )\n",
      "-> 1266             result = tuple(self._prepare_one(obj, device_placement=d) for obj, d in zip(result, device_placement))\n",
      "   1267 \n",
      "   1268         if tpu_should_fix_optimizer or (self.mixed_precision == \"fp8\" and self.fp8_recipe_handler.backend == \"TE\"):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DBT012(\n",
      "  (encoder): DBT012Encoder(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dr_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dr_projector): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (loss_fn): Entropy()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "), AcceleratedOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1268)prepare()\n",
      "   1266             result = tuple(self._prepare_one(obj, device_placement=d) for obj, d in zip(result, device_placement))\n",
      "   1267 \n",
      "-> 1268         if tpu_should_fix_optimizer or (self.mixed_precision == \"fp8\" and self.fp8_recipe_handler.backend == \"TE\"):\n",
      "   1269             # 2. grabbing new model parameters\n",
      "   1270             new_named_params = self._get_named_parameters(*result)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1278)prepare()\n",
      "   1276                     obj._switch_parameters(mapping)\n",
      "   1277 \n",
      "-> 1278         for item in result:\n",
      "   1279             if any(\n",
      "   1280                 item in container\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1279)prepare()\n",
      "   1277 \n",
      "   1278         for item in result:\n",
      "-> 1279             if any(\n",
      "   1280                 item in container\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1281)prepare()\n",
      "   1279             if any(\n",
      "   1280                 item in container\n",
      "-> 1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "   1282             ):\n",
      "   1283                 item._is_accelerate_prepared = True\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1279)prepare()\n",
      "   1277 \n",
      "   1278         for item in result:\n",
      "-> 1279             if any(\n",
      "   1280                 item in container\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1283)prepare()\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "   1282             ):\n",
      "-> 1283                 item._is_accelerate_prepared = True\n",
      "   1284 \n",
      "   1285         return result if len(result) > 1 else result[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1278)prepare()\n",
      "   1276                     obj._switch_parameters(mapping)\n",
      "   1277 \n",
      "-> 1278         for item in result:\n",
      "   1279             if any(\n",
      "   1280                 item in container\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1279)prepare()\n",
      "   1277 \n",
      "   1278         for item in result:\n",
      "-> 1279             if any(\n",
      "   1280                 item in container\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1281)prepare()\n",
      "   1279             if any(\n",
      "   1280                 item in container\n",
      "-> 1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "   1282             ):\n",
      "   1283                 item._is_accelerate_prepared = True\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1279)prepare()\n",
      "   1277 \n",
      "   1278         for item in result:\n",
      "-> 1279             if any(\n",
      "   1280                 item in container\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1283)prepare()\n",
      "   1281                 for container in (self._dataloaders, self._models, self._optimizers, self._schedulers)\n",
      "   1282             ):\n",
      "-> 1283                 item._is_accelerate_prepared = True\n",
      "   1284 \n",
      "   1285         return result if len(result) > 1 else result[0]\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcceleratedOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1278)prepare()\n",
      "   1276                     obj._switch_parameters(mapping)\n",
      "   1277 \n",
      "-> 1278         for item in result:\n",
      "   1279             if any(\n",
      "   1280                 item in container\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1285)prepare()\n",
      "   1283                 item._is_accelerate_prepared = True\n",
      "   1284 \n",
      "-> 1285         return result if len(result) > 1 else result[0]\n",
      "   1286 \n",
      "   1287     def prepare_model(self, model: torch.nn.Module, device_placement: bool = None, evaluation_mode: bool = False):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "(DBT012(\n",
      "  (en... )\n",
      "    )\n",
      "  )\n",
      "), AcceleratedOp...t_decay: 0.0\n",
      "))\n",
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py(1285)prepare()\n",
      "   1283                 item._is_accelerate_prepared = True\n",
      "   1284 \n",
      "-> 1285         return result if len(result) > 1 else result[0]\n",
      "   1286 \n",
      "   1287     def prepare_model(self, model: torch.nn.Module, device_placement: bool = None, evaluation_mode: bool = False):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(168)_inner_training_loop()\n",
      "    166             )\n",
      "    167 \n",
      "--> 168     if self.is_fsdp_enabled:\n",
      "    169         self.model = self.model_wrapped = model\n",
      "    170 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(172)_inner_training_loop()\n",
      "    170 \n",
      "    171     # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
      "--> 172     if model is not self.model:\n",
      "    173         self.model_wrapped = model\n",
      "    174 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBT012(\n",
      "  (encoder): DBT012Encoder(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dr_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dr_projector): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (loss_fn): Entropy()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  model is self.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(176)_inner_training_loop()\n",
      "    174 \n",
      "    175     # backward compatibility\n",
      "--> 176     if self.is_deepspeed_enabled:\n",
      "    177         self.deepspeed = self.model_wrapped\n",
      "    178 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(180)_inner_training_loop()\n",
      "    178 \n",
      "    179     # ckpt loading\n",
      "--> 180     if resume_from_checkpoint is not None:\n",
      "    181         if self.is_deepspeed_enabled:\n",
      "    182             deepspeed_load_checkpoint(\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(189)_inner_training_loop()\n",
      "    187 \n",
      "    188     # Check if saved optimizer or scheduler states exist\n",
      "--> 189     self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
      "    190 \n",
      "    191     # important: at this point:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(197)_inner_training_loop()\n",
      "    195 \n",
      "    196     # Train!\n",
      "--> 197     logger.info(\"***** Running training *****\")\n",
      "    198     logger.info(f\"  Num examples = {num_examples:,}\")\n",
      "    199     logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(198)_inner_training_loop()\n",
      "    196     # Train!\n",
      "    197     logger.info(\"***** Running training *****\")\n",
      "--> 198     logger.info(f\"  Num examples = {num_examples:,}\")\n",
      "    199     logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
      "    200     logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.optimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcceleratedOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.lr_scheduler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.optim.lr_scheduler.LambdaLR object>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.lr_scheduler.state_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_lrs': [5e-05, 5e-05], 'last_epoch': 0, 'verbose': False, '_step_count': 1, '_get_lr_called_within_step': False, '_last_lr': [5e-05, 5e-05], 'lr_lambdas': [{}, {}]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(199)_inner_training_loop()\n",
      "    197     logger.info(\"***** Running training *****\")\n",
      "    198     logger.info(f\"  Num examples = {num_examples:,}\")\n",
      "--> 199     logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
      "    200     logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
      "    201     if self.args.per_device_train_batch_size != self._train_batch_size:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(200)_inner_training_loop()\n",
      "    198     logger.info(f\"  Num examples = {num_examples:,}\")\n",
      "    199     logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
      "--> 200     logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
      "    201     if self.args.per_device_train_batch_size != self._train_batch_size:\n",
      "    202         logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(201)_inner_training_loop()\n",
      "    199     logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
      "    200     logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
      "--> 201     if self.args.per_device_train_batch_size != self._train_batch_size:\n",
      "    202         logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
      "    203     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(202)_inner_training_loop()\n",
      "    200     logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
      "    201     if self.args.per_device_train_batch_size != self._train_batch_size:\n",
      "--> 202         logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
      "    203     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
      "    204     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(203)_inner_training_loop()\n",
      "    201     if self.args.per_device_train_batch_size != self._train_batch_size:\n",
      "    202         logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
      "--> 203     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
      "    204     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
      "    205     logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(204)_inner_training_loop()\n",
      "    202         logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
      "    203     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
      "--> 204     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
      "    205     logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
      "    206     logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(205)_inner_training_loop()\n",
      "    203     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
      "    204     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
      "--> 205     logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
      "    206     logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
      "    207 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(206)_inner_training_loop()\n",
      "    204     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
      "    205     logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
      "--> 206     logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
      "    207 \n",
      "    208     self.state.epoch = 0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(208)_inner_training_loop()\n",
      "    206     logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
      "    207 \n",
      "--> 208     self.state.epoch = 0\n",
      "    209     start_time = time.time()\n",
      "    210     epochs_trained = 0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(209)_inner_training_loop()\n",
      "    207 \n",
      "    208     self.state.epoch = 0\n",
      "--> 209     start_time = time.time()\n",
      "    210     epochs_trained = 0\n",
      "    211     steps_trained_in_current_epoch = 0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(210)_inner_training_loop()\n",
      "    208     self.state.epoch = 0\n",
      "    209     start_time = time.time()\n",
      "--> 210     epochs_trained = 0\n",
      "    211     steps_trained_in_current_epoch = 0\n",
      "    212     steps_trained_progress_bar = None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(211)_inner_training_loop()\n",
      "    209     start_time = time.time()\n",
      "    210     epochs_trained = 0\n",
      "--> 211     steps_trained_in_current_epoch = 0\n",
      "    212     steps_trained_progress_bar = None\n",
      "    213 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(212)_inner_training_loop()\n",
      "    210     epochs_trained = 0\n",
      "    211     steps_trained_in_current_epoch = 0\n",
      "--> 212     steps_trained_progress_bar = None\n",
      "    213 \n",
      "    214     # Check if continuing training from a checkpoint\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(215)_inner_training_loop()\n",
      "    213 \n",
      "    214     # Check if continuing training from a checkpoint\n",
      "--> 215     if resume_from_checkpoint is not None and os.path.isfile(\n",
      "    216         os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
      "    217     ):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(236)_inner_training_loop()\n",
      "    234 \n",
      "    235     # Update the references\n",
      "--> 236     self.callback_handler.model = self.model\n",
      "    237     self.callback_handler.optimizer = self.optimizer\n",
      "    238     self.callback_handler.lr_scheduler = self.lr_scheduler\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(237)_inner_training_loop()\n",
      "    235     # Update the references\n",
      "    236     self.callback_handler.model = self.model\n",
      "--> 237     self.callback_handler.optimizer = self.optimizer\n",
      "    238     self.callback_handler.lr_scheduler = self.lr_scheduler\n",
      "    239     self.callback_handler.train_dataloader = train_dataloader\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(238)_inner_training_loop()\n",
      "    236     self.callback_handler.model = self.model\n",
      "    237     self.callback_handler.optimizer = self.optimizer\n",
      "--> 238     self.callback_handler.lr_scheduler = self.lr_scheduler\n",
      "    239     self.callback_handler.train_dataloader = train_dataloader\n",
      "    240     if self.hp_name is not None and self._trial is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(239)_inner_training_loop()\n",
      "    237     self.callback_handler.optimizer = self.optimizer\n",
      "    238     self.callback_handler.lr_scheduler = self.lr_scheduler\n",
      "--> 239     self.callback_handler.train_dataloader = train_dataloader\n",
      "    240     if self.hp_name is not None and self._trial is not None:\n",
      "    241         # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(240)_inner_training_loop()\n",
      "    238     self.callback_handler.lr_scheduler = self.lr_scheduler\n",
      "    239     self.callback_handler.train_dataloader = train_dataloader\n",
      "--> 240     if self.hp_name is not None and self._trial is not None:\n",
      "    241         # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n",
      "    242         # parameter to Train when using DDP.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_881/2079912209.py(244)_inner_training_loop()\n",
      "    242         # parameter to Train when using DDP.\n",
      "    243         self.state.trial_name = self.hp_name(self._trial)\n",
      "--> 244     if trial is not None:\n",
      "    245         assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
      "    246         self.state.trial_params = hp_params(assignments)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bda65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccdb0b-e563-4ace-9aab-5cd634dfd40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(57)predict()\n",
      "     55         import pdb; pdb.set_trace()\n",
      "     56         #debug\n",
      "---> 57         gen_kwargs = gen_kwargs.copy()\n",
      "     58         if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
      "     59             gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(58)predict()\n",
      "     56         #debug\n",
      "     57         gen_kwargs = gen_kwargs.copy()\n",
      "---> 58         if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
      "     59             gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
      "     60         if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(59)predict()\n",
      "     57         gen_kwargs = gen_kwargs.copy()\n",
      "     58         if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
      "---> 59             gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
      "     60         if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
      "     61             gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(60)predict()\n",
      "     58         if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
      "     59             gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
      "---> 60         if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
      "     61             gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
      "     62         if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(61)predict()\n",
      "     59             gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
      "     60         if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
      "---> 61             gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
      "     62         if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
      "     63             gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(62)predict()\n",
      "     60         if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
      "     61             gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
      "---> 62         if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
      "     63             gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
      "     64         if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(63)predict()\n",
      "     61             gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
      "     62         if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
      "---> 63             gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
      "     64         if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
      "     65             gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(64)predict()\n",
      "     62         if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
      "     63             gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
      "---> 64         if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
      "     65             gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
      "     66 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(65)predict()\n",
      "     63             gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
      "     64         if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
      "---> 65             gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
      "     66 \n",
      "     67         self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(67)predict()\n",
      "     65             gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
      "     66 \n",
      "---> 67         self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
      "     68         self._memory_tracker.start()\n",
      "     69 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(68)predict()\n",
      "     66 \n",
      "     67         self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
      "---> 68         self._memory_tracker.start()\n",
      "     69 \n",
      "     70         if self._perform_representation(unwrap_model(self.model)) and not self.args.prediction_loss_only:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(70)predict()\n",
      "     68         self._memory_tracker.start()\n",
      "     69 \n",
      "---> 70         if self._perform_representation(unwrap_model(self.model)) and not self.args.prediction_loss_only:\n",
      "     71             self._build_lbl_index(test_dataset)\n",
      "     72 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(71)predict()\n",
      "     69 \n",
      "     70         if self._perform_representation(unwrap_model(self.model)) and not self.args.prediction_loss_only:\n",
      "---> 71             self._build_lbl_index(test_dataset)\n",
      "     72 \n",
      "     73         if self._perform_augmentation(unwrap_model(self.model)) and not self.args.prediction_loss_only:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_25061/4131283268.py(25)_build_lbl_index()\n",
      "     23         self.aug_idxs.build(aug_repr)\n",
      "     24 \n",
      "---> 25 @patch\n",
      "     26 def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
      "     27     dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(27)_build_lbl_index()\n",
      "     25 @patch\n",
      "     26 def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
      "---> 27     dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
      "     28     dataset = dataset if self.train_dataset is None else self.train_dataset\n",
      "     29 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(28)_build_lbl_index()\n",
      "     26 def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
      "     27     dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
      "---> 28     dataset = dataset if self.train_dataset is None else self.train_dataset\n",
      "     29 \n",
      "     30     if dataset is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(30)_build_lbl_index()\n",
      "     28     dataset = dataset if self.train_dataset is None else self.train_dataset\n",
      "     29 \n",
      "---> 30     if dataset is not None:\n",
      "     31         lbl_dset = dataset.lbl_dset\n",
      "     32 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(31)_build_lbl_index()\n",
      "     29 \n",
      "     30     if dataset is not None:\n",
      "---> 31         lbl_dset = dataset.lbl_dset\n",
      "     32 \n",
      "     33         meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(33)_build_lbl_index()\n",
      "     31         lbl_dset = dataset.lbl_dset\n",
      "     32 \n",
      "---> 33         meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
      "     34         if meta_name is not None and dataset.meta is not None and meta_name in dataset.meta:\n",
      "     35             prefix,lbl_meta,meta_info  = dataset.meta[meta_name].prefix,dataset.meta[meta_name].lbl_meta,dataset.meta[meta_name].meta_info\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  lbl_dset.n_data = 100\n",
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(34)_build_lbl_index()\n",
      "     32 \n",
      "     33         meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
      "---> 34         if meta_name is not None and dataset.meta is not None and meta_name in dataset.meta:\n",
      "     35             prefix,lbl_meta,meta_info  = dataset.meta[meta_name].prefix,dataset.meta[meta_name].lbl_meta,dataset.meta[meta_name].meta_info\n",
      "     36             meta_kwargs = {meta_name: MetaXCDataset(prefix, lbl_meta, lbl_meta, meta_info, n_data_meta_samples=self.args.augmentation_num_beams)}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(39)_build_lbl_index()\n",
      "     37             lbl_dset = XCDataset(lbl_dset, **meta_kwargs)\n",
      "     38 \n",
      "---> 39         lbl_dl = self.get_test_dataloader(lbl_dset)\n",
      "     40         lbl_repr = self.get_representation(lbl_dl, to_cpu=isinstance(self.idxs, IndexSearch))\n",
      "     41         if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(40)_build_lbl_index()\n",
      "     38 \n",
      "     39         lbl_dl = self.get_test_dataloader(lbl_dset)\n",
      "---> 40         lbl_repr = self.get_representation(lbl_dl, to_cpu=isinstance(self.idxs, IndexSearch))\n",
      "     41         if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
      "     42 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aaf16dc779e445c8dbaffe220bdd3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(41)_build_lbl_index()\n",
      "     39         lbl_dl = self.get_test_dataloader(lbl_dset)\n",
      "     40         lbl_repr = self.get_representation(lbl_dl, to_cpu=isinstance(self.idxs, IndexSearch))\n",
      "---> 41         if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
      "     42 \n",
      "     43         self.idxs.build(lbl_repr)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/4131283268.py(43)_build_lbl_index()\n",
      "     41         if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
      "     42 \n",
      "---> 43         self.idxs.build(lbl_repr)\n",
      "     44     else: raise ValueError('Failed to build `self.idxs`')\n",
      "     45 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> /tmp/ipykernel_25061/4131283268.py(43)_build_lbl_index()\n",
      "     41         if self.args.use_distributional_representation: lbl_repr = F.log_softmax(lbl_repr, dim=-1)\n",
      "     42 \n",
      "---> 43         self.idxs.build(lbl_repr)\n",
      "     44     else: raise ValueError('Failed to build `self.idxs`')\n",
      "     45 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(73)predict()\n",
      "     71             self._build_lbl_index(test_dataset)\n",
      "     72 \n",
      "---> 73         if self._perform_augmentation(unwrap_model(self.model)) and not self.args.prediction_loss_only:\n",
      "     74             self._build_aug_index(test_dataset)\n",
      "     75 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(76)predict()\n",
      "     74             self._build_aug_index(test_dataset)\n",
      "     75 \n",
      "---> 76         test_dataloader = self.get_test_dataloader(test_dataset)\n",
      "     77         start_time = time.time()\n",
      "     78 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(77)predict()\n",
      "     75 \n",
      "     76         test_dataloader = self.get_test_dataloader(test_dataset)\n",
      "---> 77         start_time = time.time()\n",
      "     78 \n",
      "     79         output = self.evaluation_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3536802410.py(79)predict()\n",
      "     77         start_time = time.time()\n",
      "     78 \n",
      "---> 79         output = self.evaluation_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
      "     80         total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
      "     81         if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_25061/1638443235.py(2)evaluation_loop()\n",
      "      1 #| export\n",
      "----> 2 @patch\n",
      "      3 def evaluation_loop(\n",
      "      4     self:XCLearner,\n",
      "      5     dataloader:DataLoader,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(13)evaluation_loop()\n",
      "     11     metric_key_prefix:str=\"eval\",\n",
      "     12 ) -> XCEvalLoopOutput:\n",
      "---> 13     args = self.args\n",
      "     14     prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
      "     15 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(14)evaluation_loop()\n",
      "     12 ) -> XCEvalLoopOutput:\n",
      "     13     args = self.args\n",
      "---> 14     prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
      "     15 \n",
      "     16     model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(16)evaluation_loop()\n",
      "     14     prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
      "     15 \n",
      "---> 16     model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
      "     17 \n",
      "     18     if len(self.accelerator._models) == 0 and model is self.model:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(18)evaluation_loop()\n",
      "     16     model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
      "     17 \n",
      "---> 18     if len(self.accelerator._models) == 0 and model is self.model:\n",
      "     19         model = self.accelerator.prepare(model) if self.is_deepspeed_enabled else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
      "     20         if self.is_fsdp_enabled: self.model = model\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(24)evaluation_loop()\n",
      "     22         if self.is_deepspeed_enabled: self.deepspeed = self.model_wrapped\n",
      "     23 \n",
      "---> 24     batch_size = self.args.eval_batch_size\n",
      "     25 \n",
      "     26     model.eval()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(26)evaluation_loop()\n",
      "     24     batch_size = self.args.eval_batch_size\n",
      "     25 \n",
      "---> 26     model.eval()\n",
      "     27 \n",
      "     28     self.callback_handler.eval_dataloader = dataloader\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(28)evaluation_loop()\n",
      "     26     model.eval()\n",
      "     27 \n",
      "---> 28     self.callback_handler.eval_dataloader = dataloader\n",
      "     29     eval_dataset = getattr(dataloader, \"dataset\", None)\n",
      "     30 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(29)evaluation_loop()\n",
      "     27 \n",
      "     28     self.callback_handler.eval_dataloader = dataloader\n",
      "---> 29     eval_dataset = getattr(dataloader, \"dataset\", None)\n",
      "     30 \n",
      "     31     if args.past_index >= 0: self._past = None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(31)evaluation_loop()\n",
      "     29     eval_dataset = getattr(dataloader, \"dataset\", None)\n",
      "     30 \n",
      "---> 31     if args.past_index >= 0: self._past = None\n",
      "     32 \n",
      "     33     losses_host, all_losses = None, None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(33)evaluation_loop()\n",
      "     31     if args.past_index >= 0: self._past = None\n",
      "     32 \n",
      "---> 33     losses_host, all_losses = None, None\n",
      "     34     host_output, all_output = {}, {}\n",
      "     35 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(34)evaluation_loop()\n",
      "     32 \n",
      "     33     losses_host, all_losses = None, None\n",
      "---> 34     host_output, all_output = {}, {}\n",
      "     35 \n",
      "     36     observed_num_examples = 0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(36)evaluation_loop()\n",
      "     34     host_output, all_output = {}, {}\n",
      "     35 \n",
      "---> 36     observed_num_examples = 0\n",
      "     37     for step, inputs in enumerate(dataloader):\n",
      "     38         observed_batch_size = find_batch_size(inputs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(37)evaluation_loop()\n",
      "     35 \n",
      "     36     observed_num_examples = 0\n",
      "---> 37     for step, inputs in enumerate(dataloader):\n",
      "     38         observed_batch_size = find_batch_size(inputs)\n",
      "     39         if observed_batch_size is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(38)evaluation_loop()\n",
      "     36     observed_num_examples = 0\n",
      "     37     for step, inputs in enumerate(dataloader):\n",
      "---> 38         observed_batch_size = find_batch_size(inputs)\n",
      "     39         if observed_batch_size is not None:\n",
      "     40             observed_num_examples += observed_batch_size\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(39)evaluation_loop()\n",
      "     37     for step, inputs in enumerate(dataloader):\n",
      "     38         observed_batch_size = find_batch_size(inputs)\n",
      "---> 39         if observed_batch_size is not None:\n",
      "     40             observed_num_examples += observed_batch_size\n",
      "     41             if batch_size is None: batch_size = observed_batch_size\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(40)evaluation_loop()\n",
      "     38         observed_batch_size = find_batch_size(inputs)\n",
      "     39         if observed_batch_size is not None:\n",
      "---> 40             observed_num_examples += observed_batch_size\n",
      "     41             if batch_size is None: batch_size = observed_batch_size\n",
      "     42 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(41)evaluation_loop()\n",
      "     39         if observed_batch_size is not None:\n",
      "     40             observed_num_examples += observed_batch_size\n",
      "---> 41             if batch_size is None: batch_size = observed_batch_size\n",
      "     42 \n",
      "     43         loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(43)evaluation_loop()\n",
      "     41             if batch_size is None: batch_size = observed_batch_size\n",
      "     42 \n",
      "---> 43         loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
      "     44 \n",
      "     45         if loss is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_25061/2932614700.py(2)prediction_step()\n",
      "      1 #| export\n",
      "----> 2 @patch\n",
      "      3 def prediction_step(\n",
      "      4     self:XCLearner,\n",
      "      5     model: nn.Module,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(14)prediction_step()\n",
      "     12     **kwargs,\n",
      "     13 ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
      "---> 14     with torch.no_grad():\n",
      "     15         with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
      "     16         loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(15)prediction_step()\n",
      "     13 ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
      "     14     with torch.no_grad():\n",
      "---> 15         with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
      "     16         loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "     17     prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(16)prediction_step()\n",
      "     14     with torch.no_grad():\n",
      "     15         with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
      "---> 16         loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "     17     prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
      "     18     if prediction_loss_only: return loss, {}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(17)prediction_step()\n",
      "     15         with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
      "     16         loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "---> 17     prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
      "     18     if prediction_loss_only: return loss, {}\n",
      "     19 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(18)prediction_step()\n",
      "     16         loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "     17     prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
      "---> 18     if prediction_loss_only: return loss, {}\n",
      "     19 \n",
      "     20     if self._perform_augmentation(model, predict_with_augmentation):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(20)prediction_step()\n",
      "     18     if prediction_loss_only: return loss, {}\n",
      "     19 \n",
      "---> 20     if self._perform_augmentation(model, predict_with_augmentation):\n",
      "     21         aug_inputs = self.augmentation_output(model, inputs, **kwargs)\n",
      "     22         inputs.update(aug_inputs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(24)prediction_step()\n",
      "     22         inputs.update(aug_inputs)\n",
      "     23 \n",
      "---> 24     output, gen_o, repr_o = None, None, None\n",
      "     25     if self._perform_generation(model, predict_with_generation): gen_o = self.generation_output(model, inputs, **kwargs)\n",
      "     26     if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(25)prediction_step()\n",
      "     23 \n",
      "     24     output, gen_o, repr_o = None, None, None\n",
      "---> 25     if self._perform_generation(model, predict_with_generation): gen_o = self.generation_output(model, inputs, **kwargs)\n",
      "     26     if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
      "     27 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self._perform_generation(model, predict_with_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(26)prediction_step()\n",
      "     24     output, gen_o, repr_o = None, None, None\n",
      "     25     if self._perform_generation(model, predict_with_generation): gen_o = self.generation_output(model, inputs, **kwargs)\n",
      "---> 26     if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
      "     27 \n",
      "     28     if gen_o is not None and repr_o is not None:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_25061/2264972374.py(8)_perform_representation()\n",
      "      6     return getattr(model,'use_generation') if hasattr(model,'use_generation') else predict_with_generation\n",
      "      7 \n",
      "----> 8 @patch\n",
      "      9 def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
      "     10     model = unwrap_model(model)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2264972374.py(10)_perform_representation()\n",
      "      8 @patch\n",
      "      9 def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
      "---> 10     model = unwrap_model(model)\n",
      "     11     predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
      "     12     return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2264972374.py(11)_perform_representation()\n",
      "      9 def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
      "     10     model = unwrap_model(model)\n",
      "---> 11     predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
      "     12     return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
      "     13 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2264972374.py(12)_perform_representation()\n",
      "     10     model = unwrap_model(model)\n",
      "     11     predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
      "---> 12     return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
      "     13 \n",
      "     14 @patch\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "True\n",
      "> /tmp/ipykernel_25061/2264972374.py(12)_perform_representation()\n",
      "     10     model = unwrap_model(model)\n",
      "     11     predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
      "---> 12     return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
      "     13 \n",
      "     14 @patch\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /tmp/ipykernel_25061/3649135444.py(17)representation_output()\n",
      "     15     return {'pred_idx':o['info2seq2data_idx'], 'pred_score':o['info2seq2data_score'], 'pred_ptr':o['info2seq2data_data2ptr']}\n",
      "     16 \n",
      "---> 17 @patch\n",
      "     18 def representation_output(\n",
      "     19     self:XCLearner,\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(24)representation_output()\n",
      "     22     **kwargs\n",
      "     23 ):\n",
      "---> 24     inputs = self._prepare_inputs(inputs)\n",
      "     25     n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
      "     26 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(25)representation_output()\n",
      "     23 ):\n",
      "     24     inputs = self._prepare_inputs(inputs)\n",
      "---> 25     n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
      "     26 \n",
      "     27     with torch.no_grad():\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(27)representation_output()\n",
      "     25     n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
      "     26 \n",
      "---> 27     with torch.no_grad():\n",
      "     28         o = getattr(model(**inputs), self.args.representation_attribute)\n",
      "     29         if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(28)representation_output()\n",
      "     26 \n",
      "     27     with torch.no_grad():\n",
      "---> 28         o = getattr(model(**inputs), self.args.representation_attribute)\n",
      "     29         if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
      "     30 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(29)representation_output()\n",
      "     27     with torch.no_grad():\n",
      "     28         o = getattr(model(**inputs), self.args.representation_attribute)\n",
      "---> 29         if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
      "     30 \n",
      "     31     o = self.idxs.proc(o, n_bm=n_bm)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  !o.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  !torch.norm(o, dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22.6966, 23.2682, 23.3030, 24.4900, 23.1127, 21.4881, 24.3592, 23.7730,\n",
      "        22.1580, 23.4856, 23.3100, 24.3136, 22.0395, 22.7433, 21.3868, 23.5914,\n",
      "        22.4485, 23.3996, 22.8410, 22.8833, 24.6456, 21.8179, 22.1863, 22.9824,\n",
      "        20.1039, 24.9278, 23.6765, 22.9878, 22.6013, 22.5656, 24.4056, 23.0457,\n",
      "        22.7467, 23.3570, 21.3748, 24.7137, 25.2308, 22.5590, 23.6726, 24.1197,\n",
      "        21.4324, 21.6063, 25.1573, 23.4217, 24.4976, 24.5576, 23.0131, 23.7130,\n",
      "        24.1785, 24.1275, 24.5175, 22.9784, 24.8105, 21.2234, 22.0189, 22.3022,\n",
      "        23.5721, 24.2453, 23.7139, 21.0918, 23.4735, 21.7098, 22.3643, 23.7718,\n",
      "        23.9011, 24.5701, 22.8404, 22.1328, 22.8997, 23.3237, 22.6347, 22.0182,\n",
      "        22.0808, 24.8773, 24.0185, 20.7367, 21.6830, 23.9921, 23.2274, 23.4083,\n",
      "        20.5668, 20.0808, 24.5315, 21.2350, 23.9680, 24.0947, 23.7745, 23.2031,\n",
      "        24.5928, 21.9048, 24.2429, 24.7638, 23.1487, 24.4555, 24.8517, 22.0793,\n",
      "        24.2304, 24.1107, 23.9684, 22.4279, 23.8725, 23.3250, 23.5987, 24.0598,\n",
      "        22.3220, 23.3959, 22.1930, 21.8933, 24.6099, 24.2580, 24.5157, 22.1719,\n",
      "        23.3939, 24.8435, 22.3282, 21.8416, 24.0377, 23.9407, 21.0315, 21.1543,\n",
      "        20.8249, 22.7813, 23.3429, 24.7917, 23.3441, 21.5260, 23.7057, 23.4807],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24     inputs = self._prepare_inputs(inputs)\n",
      "     25     n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
      "     26 \n",
      "     27     with torch.no_grad():\n",
      "     28         o = getattr(model(**inputs), self.args.representation_attribute)\n",
      "---> 29         if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
      "     30 \n",
      "     31     o = self.idxs.proc(o, n_bm=n_bm)\n",
      "     32 \n",
      "     33     return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
      "     34 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(31)representation_output()\n",
      "     29         if self.args.use_distributional_representation: o = F.softmax(o, dim=-1)\n",
      "     30 \n",
      "---> 31     o = self.idxs.proc(o, n_bm=n_bm)\n",
      "     32 \n",
      "     33     return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/3649135444.py(33)representation_output()\n",
      "     31     o = self.idxs.proc(o, n_bm=n_bm)\n",
      "     32 \n",
      "---> 33     return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
      "     34 \n",
      "     35 @patch\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "{'pred_idx': tensor([69,  ...,  9, 34, 53]), 'pred_ptr': tensor([5, 5,..., 5, 5, 5, 5]), 'pred_score': tensor([-6.85...528, -6.8610])}\n",
      "> /tmp/ipykernel_25061/3649135444.py(33)representation_output()\n",
      "     31     o = self.idxs.proc(o, n_bm=n_bm)\n",
      "     32 \n",
      "---> 33     return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
      "     34 \n",
      "     35 @patch\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(28)prediction_step()\n",
      "     26     if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
      "     27 \n",
      "---> 28     if gen_o is not None and repr_o is not None:\n",
      "     29         output = {f'{k}_gen':v for k,v in gen_o.items()}\n",
      "     30         output.update({f'{k}_repr':v for k,v in repr_o.items()})\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(33)prediction_step()\n",
      "     31         output.update(self.concatenate_output(gen_o, repr_o))\n",
      "     32     else:\n",
      "---> 33         output = gen_o if repr_o is None else repr_o\n",
      "     34 \n",
      "     35     labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(35)prediction_step()\n",
      "     33         output = gen_o if repr_o is None else repr_o\n",
      "     34 \n",
      "---> 35     labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
      "     36     if labels is not None: output.update(labels)\n",
      "     37 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(36)prediction_step()\n",
      "     34 \n",
      "     35     labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
      "---> 36     if labels is not None: output.update(labels)\n",
      "     37 \n",
      "     38     return loss, output\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/2932614700.py(38)prediction_step()\n",
      "     35     labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
      "     36     if labels is not None: output.update(labels)\n",
      "     37 \n",
      "---> 38     return loss, output\n",
      "     39 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "(tensor(0.0429...vice='cuda:0'), {'pred_idx': tensor([69,  ...,  9, 34, 53]), 'pred_ptr': tensor([5, 5,..., 5, 5, 5, 5]), 'pred_score': tensor([-6.85...528, -6.8610]), 'targ_idx': tensor([ 6030...vice='cuda:0'), ...})\n",
      "> /tmp/ipykernel_25061/2932614700.py(38)prediction_step()\n",
      "     35     labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
      "     36     if labels is not None: output.update(labels)\n",
      "     37 \n",
      "---> 38     return loss, output\n",
      "     39 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(45)evaluation_loop()\n",
      "     43         loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
      "     44 \n",
      "---> 45         if loss is not None:\n",
      "     46             losses = self.gather_function((loss.repeat(batch_size)))\n",
      "     47             losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(46)evaluation_loop()\n",
      "     44 \n",
      "     45         if loss is not None:\n",
      "---> 46             losses = self.gather_function((loss.repeat(batch_size)))\n",
      "     47             losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
      "     48         for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_25061/1638443235.py(47)evaluation_loop()\n",
      "     45         if loss is not None:\n",
      "     46             losses = self.gather_function((loss.repeat(batch_size)))\n",
      "---> 47             losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
      "     48         for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
      "     49 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program interrupted. (Use 'cont' to resume).\n",
      "--Call--\n",
      "> /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/llvmlite/binding/ffi.py(78)__exit__()\n",
      "     76             acq_fn()\n",
      "     77 \n",
      "---> 78     def __exit__(self, *exc_details):\n",
      "     79         # Invoke all callbacks\n",
      "     80         for acq_fn, rel_fn in self._cblist:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: Preprocessing for parfors)\n\u001b[1mNo implementation of function Function(<class 'numpy.dtype'>) found for signature:\n \n >>> dtype(Literal[str](int32))\n \nThere are 2 candidate implementations:\n\u001b[1m - Of which 1 did not match due to:\n Overload in function 'numpy_dtype': File: numba/np/npyimpl.py: Line 622.\n   With argument(s): '(unicode_type)':\u001b[0m\n\u001b[1m  Rejected as the implementation raised a specific error:\n    NumbaTypeError: \u001b[1munknown dtype descriptor: unicode_type\u001b[0m\u001b[0m\n  raised from /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/numba/np/npyimpl.py:631\n\u001b[1m - Of which 1 did not match due to:\n Overload in function 'numpy_dtype': File: numba/np/npyimpl.py: Line 622.\n   With argument(s): '(Literal[str](int32))':\u001b[0m\n\u001b[1m  Rejected as the implementation raised a specific error:\n    BdbQuit: Failed in nopython mode pipeline (step: native lowering)\n\u001b[0m\n  raised from /home/scai/phd/aiz218323/scratch/anaconda3/envs/xc_nlg_2/lib/python3.9/bdb.py:135\n\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 79\u001b[0m, in \u001b[0;36mXCLearner.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_test_dataloader(test_dataset)\n\u001b[1;32m     77\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 79\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((loss\u001b[38;5;241m.\u001b[39mrepeat(batch_size)))\n\u001b[0;32m---> 47\u001b[0m     losses_host \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;28;01mif\u001b[39;00m losses_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(losses_host, losses, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m output: host_output[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_host_output(output[k], host_output\u001b[38;5;241m.\u001b[39mget(k, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_prediction_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/metrics.py:30\u001b[0m, in \u001b[0;36mXCMetric.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/metrics.py:58\u001b[0m, in \u001b[0;36mXCMetric.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m]), output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)])\n\u001b[1;32m     57\u001b[0m pred, targ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pred(output), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_targ(output)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/metrics.py:112\u001b[0m, in \u001b[0;36mprec_recl\u001b[0;34m(inp, targ, prop, pa, pb, pk, rep_pk, rk, rep_rk)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprec_recl\u001b[39m(inp:sparse\u001b[38;5;241m.\u001b[39mcsr_matrix, \n\u001b[1;32m    104\u001b[0m               targ:sparse\u001b[38;5;241m.\u001b[39mcsr_matrix,\n\u001b[1;32m    105\u001b[0m               prop:sparse\u001b[38;5;241m.\u001b[39mcsr_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m               rk:Optional[\u001b[38;5;28mint\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    111\u001b[0m               rep_rk:Optional[List]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 112\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrep_pk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     metric\u001b[38;5;241m.\u001b[39mupdate(recall(inp, targ, k\u001b[38;5;241m=\u001b[39mrk, repk\u001b[38;5;241m=\u001b[39mrep_rk))\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/metrics.py:75\u001b[0m, in \u001b[0;36mprecision\u001b[0;34m(inp, targ, prop, k, pa, pb, repk)\u001b[0m\n\u001b[1;32m     72\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xm\u001b[38;5;241m.\u001b[39mcompute_inv_propesity(prop, A\u001b[38;5;241m=\u001b[39mpa, B\u001b[38;5;241m=\u001b[39mpb)\n\u001b[1;32m     74\u001b[0m metric \u001b[38;5;241m=\u001b[39m xm\u001b[38;5;241m.\u001b[39mMetrics(true_labels\u001b[38;5;241m=\u001b[39mtarg, inv_psp\u001b[38;5;241m=\u001b[39mprop)\n\u001b[0;32m---> 75\u001b[0m prec \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: prec[i][r\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i,n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(name) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m repk \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/evaluation/xc_metrics.py:708\u001b[0m, in \u001b[0;36mMetrics.eval\u001b[0;34m(self, pred_labels, K, sorted, use_cython)\u001b[0m\n\u001b[1;32m    704\u001b[0m         pred_labels \u001b[38;5;241m=\u001b[39m pred_labels[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_idx]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m compatible_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_labels, pred_labels), \\\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes must be compatible for ground truth and predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 708\u001b[0m indices, true_labels, ps_indices, inv_psp \u001b[38;5;241m=\u001b[39m \u001b[43m_setup_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_psp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cython\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cython\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m _total_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    713\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    714\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    715\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndcg_denominator[_total_pos \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/evaluation/xc_metrics.py:230\u001b[0m, in \u001b[0;36m_setup_metric\u001b[0;34m(X, true_labels, inv_psp, k, sorted, use_cython)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m compatible_shapes(X, true_labels), \\\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground truth and prediction matrices must have same shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m num_instances, num_labels \u001b[38;5;241m=\u001b[39m true_labels\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 230\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43m_get_topk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cython\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m ps_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inv_psp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/evaluation/xc_metrics.py:174\u001b[0m, in \u001b[0;36m_get_topk\u001b[0;34m(X, pad_indx, k, sorted, use_cython)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mGet top-k indices (row-wise); Support for\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m* csr_matirx\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m* 2 np.ndarray with indices and values\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m* np.ndarray with indices or values\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m--> 174\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43m_get_topk_sparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_indx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_indx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cython\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cython\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    180\u001b[0m     indices \u001b[38;5;241m=\u001b[39m _get_topk_array(\n\u001b[1;32m    181\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    182\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/evaluation/xc_metrics.py:105\u001b[0m, in \u001b[0;36m_get_topk_sparse\u001b[0;34m(X, pad_indx, k, use_cython)\u001b[0m\n\u001b[1;32m    103\u001b[0m X\u001b[38;5;241m.\u001b[39msort_indices()\n\u001b[1;32m    104\u001b[0m pad_indx \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 105\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_indx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cython\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cython\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/xclib-0.97-py3.9-linux-x86_64.egg/xclib/utils/sparse.py:113\u001b[0m, in \u001b[0;36mtopk\u001b[0;34m(X, k, pad_ind, pad_val, return_values, dtype, use_cython)\u001b[0m\n\u001b[1;32m    111\u001b[0m     ind, val \u001b[38;5;241m=\u001b[39m _topk(X\u001b[38;5;241m.\u001b[39mdata, X\u001b[38;5;241m.\u001b[39mindices, X\u001b[38;5;241m.\u001b[39mindptr, k, pad_ind, pad_val)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     ind, val \u001b[38;5;241m=\u001b[39m \u001b[43m_topk_nb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_values:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ind, val\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/numba/core/dispatcher.py:420\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    418\u001b[0m return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mtuple\u001b[39m(argtypes))\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mForceLiteralArg \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Received request for compiler re-entry with the list of arguments\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# indicated by e.requested_args.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# First, check if any of these args are already Literal-ized\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     already_lit_pos \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mrequested_args\n\u001b[1;32m    426\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[i], types\u001b[38;5;241m.\u001b[39mLiteral)]\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/numba/core/dispatcher.py:409\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: Preprocessing for parfors)\n\u001b[1mNo implementation of function Function(<class 'numpy.dtype'>) found for signature:\n \n >>> dtype(Literal[str](int32))\n \nThere are 2 candidate implementations:\n\u001b[1m - Of which 1 did not match due to:\n Overload in function 'numpy_dtype': File: numba/np/npyimpl.py: Line 622.\n   With argument(s): '(unicode_type)':\u001b[0m\n\u001b[1m  Rejected as the implementation raised a specific error:\n    NumbaTypeError: \u001b[1munknown dtype descriptor: unicode_type\u001b[0m\u001b[0m\n  raised from /scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/numba/np/npyimpl.py:631\n\u001b[1m - Of which 1 did not match due to:\n Overload in function 'numpy_dtype': File: numba/np/npyimpl.py: Line 622.\n   With argument(s): '(Literal[str](int32))':\u001b[0m\n\u001b[1m  Rejected as the implementation raised a specific error:\n    BdbQuit: Failed in nopython mode pipeline (step: native lowering)\n\u001b[0m\n  raised from /home/scai/phd/aiz218323/scratch/anaconda3/envs/xc_nlg_2/lib/python3.9/bdb.py:135\n\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "o = learn.predict(learn.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b7444-332d-42b7-8766-e3192a1f2f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c40ed3-6a16-47ab-b6ad-4dc832cfeca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
