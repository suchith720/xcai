{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b937d98-9beb-445c-bc68-c8d71a8c1ae0",
   "metadata": {},
   "source": [
    "# learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714cf2-0372-49f6-a39c-24752a28145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b498e-a0a3-4bdd-8741-ca516ddc64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ac07c-66a4-40d0-a39b-e5220c20fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from packaging import version\n",
    "import torch, re, math, numpy as np, os, time, datasets\n",
    "from typing import Any, Tuple, Optional, Sequence, Union, Dict, List, NamedTuple\n",
    "from transformers import AutoTokenizer, BatchEncoding, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.parallel._functions import Scatter\n",
    "from torch.nn.parallel.scatter_gather import _is_namedtuple\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.data import *\n",
    "from xcai.representation.index import *\n",
    "from xcai.generation.trie import *\n",
    "from xcai.generation.generate import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from fastcore.dispatch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821000f3-31ca-44ed-a2a5-e060f900ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers.trainer_pt_utils import (\n",
    "    find_batch_size, \n",
    "    nested_concat, nested_numpify, \n",
    "    IterableDatasetShard, \n",
    "    get_dataloader_sampler, \n",
    "    get_model_param_count,\n",
    "    LengthGroupedSampler\n",
    ")\n",
    "from transformers.trainer_utils import has_length, denumpify_detensorize, speed_metrics, TrainOutput, HPSearchBackend, seed_worker\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import _is_peft_model\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.utils import is_sagemaker_mp_enabled, is_accelerate_available, is_torch_tpu_available, logging, is_datasets_available\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "\n",
    "from transformers.integrations import hp_params\n",
    "from transformers.integrations.tpu import tpu_spmd_dataloader\n",
    "from transformers.integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n",
    "\n",
    "if is_accelerate_available():\n",
    "    from accelerate import Accelerator, skip_first_batches\n",
    "    from accelerate import __version__ as accelerate_version\n",
    "    from accelerate.utils import (\n",
    "        DistributedDataParallelKwargs,\n",
    "        DistributedType,\n",
    "        GradientAccumulationPlugin,\n",
    "        load_fsdp_model,\n",
    "        load_fsdp_optimizer,\n",
    "        save_fsdp_model,\n",
    "        save_fsdp_optimizer,\n",
    "    )\n",
    "\n",
    "    DATA_SAMPLERS = [RandomSampler]\n",
    "    if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "        from accelerate.data_loader import SeedableRandomSampler\n",
    "\n",
    "        DATA_SAMPLERS += [SeedableRandomSampler]\n",
    "\n",
    "    if is_deepspeed_available():\n",
    "        from accelerate.utils import DeepSpeedSchedulerWrapper\n",
    "\n",
    "if is_accelerate_available(\"0.28.0\"):\n",
    "    from accelerate.utils import DataLoaderConfiguration\n",
    "\n",
    "TRAINING_ARGS_NAME = \"training_args.bin\"\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "OPTIMIZER_NAME = \"optimizer.pt\"\n",
    "OPTIMIZER_NAME_BIN = \"optimizer.bin\"\n",
    "SCHEDULER_NAME = \"scheduler.pt\"\n",
    "SCALER_NAME = \"scaler.pt\"\n",
    "FSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547561-f805-40c1-a6b2-d89e772ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1b571-0043-4b86-acce-cb6cb0ef598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from xcai.test_utils import *\n",
    "from xcai.models.BT000X import *\n",
    "from xcai.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab5fa-1f75-44e8-b533-525d577f4209",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188e3a7-9458-456f-8a19-104fd139a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "block = XCBlock.from_cfg('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4d23b-4525-46bd-9a75-19ffca285235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch = block.train.one_batch(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47e1ed-ad70-475f-a01f-cd9320705796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf42c7-5471-4ff7-bc65-4020a5eb5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95cdc4-ff76-4cd7-aa20-a54a03f7c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "b = prepare_batch(m, batch, m_args='lbl2data_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef492419-4e5a-4f08-bb1b-3ba3283703d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7a04d-81d7-402d-9a2d-b449b3acd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = m.to('cuda')\n",
    "b = b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64be93-2e54-4fbd-841b-c5d2828f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5e531-8f3c-438e-ac10-b46c42e90093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8700, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d9190-682e-4017-954e-fad35c79ab13",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54c7fe-18b7-4502-a68d-5bbca3a75a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scatter(inputs, target_gpus, chunk_sizes=None, dim=0):\n",
    "    def scatter_map(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n",
    "        if _is_namedtuple(obj):\n",
    "            return [type(obj)(*args) for args in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, tuple) and len(obj) > 0:\n",
    "            return list(zip(*map(scatter_map, obj)))\n",
    "        if isinstance(obj, list) and len(obj) > 0:\n",
    "            return [list(i) for i in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, dict) and len(obj) > 0:\n",
    "            return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
    "        return [obj for _ in target_gpus] \n",
    "    try:\n",
    "        res = scatter_map(inputs)\n",
    "    finally:\n",
    "        scatter_map = None\n",
    "    return res\n",
    "    \n",
    "def scatter_kwargs(\n",
    "    inputs: Tuple[Any, ...],\n",
    "    kwargs: Optional[Dict[str, Any]],\n",
    "    target_gpus: Sequence[Union[int, torch.device]],\n",
    "    chunk_sizes: Optional[Sequence[int]]=None,\n",
    "    dim: int = 0,\n",
    ") -> Tuple[Tuple[Any, ...], Tuple[Dict[str, Any], ...]]:\n",
    "    scattered_inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n",
    "    scattered_kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n",
    "    if len(scattered_inputs) < len(scattered_kwargs):\n",
    "        scattered_inputs.extend(() for _ in range(len(scattered_kwargs) - len(scattered_inputs)))\n",
    "    elif len(scattered_kwargs) < len(inputs):\n",
    "        scattered_kwargs.extend({} for _ in range(len(scattered_inputs) - len(scattered_kwargs)))\n",
    "    return scattered_inputs, scattered_kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420b5aa-6d09-47c1-809d-af19e02f800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCDataParallel(DataParallel):\n",
    "\n",
    "    @delegates(DataParallel.__init__)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _get_meta_name(self, x:Optional[Dict[str, Any]]):\n",
    "        return list(set([k.split('_')[0] for k in x]).difference(['data', 'lbl2data'])) if len(x) else set()\n",
    "\n",
    "    def _extract_feat(self, x:Optional[Dict[str, Any]], prefix:str):\n",
    "        return {k:v for k,v in x.items() if re.match(f'^{prefix}', k) and not re.match(r'.*2ptr$', k)}\n",
    "\n",
    "    def scatter(\n",
    "        self,\n",
    "        inputs: Tuple[Any, ...],\n",
    "        kwargs: Optional[Dict[str, Any]],\n",
    "        device_ids: Sequence[Union[int, torch.device]],\n",
    "    ) -> Any:\n",
    "        if len(inputs): raise ValueError('`inputs` should be empty.')\n",
    "        meta_name = self._get_meta_name(kwargs)+['lbl2data']\n",
    "        \n",
    "        data_feat = self._extract_feat(kwargs, 'data')\n",
    "        scattered_inputs, scattered_kwargs = scatter_kwargs(inputs, data_feat, device_ids, None, dim=self.dim)\n",
    "        \n",
    "        for o in meta_name:\n",
    "            pn, chunk_sizes = f'{o}_data2ptr', None\n",
    "            if pn in kwargs:\n",
    "                ptr = kwargs[pn]\n",
    "                psz, csz = ptr.shape[0], math.ceil(ptr.shape[0]/len(device_ids))\n",
    "                psz = (psz//csz+1)*csz if psz%csz else psz # Use torch.chunk\n",
    "                chunk_sizes = [ptr[p:q].sum() for p,q in zip(range(0, psz+1, csz), range(csz, psz+1, csz))]\n",
    "                _, sc_ptr = scatter_kwargs(inputs, {pn:ptr}, device_ids, None, dim=self.dim)\n",
    "                for p,q in zip(scattered_kwargs, sc_ptr): p.update(q)\n",
    "                    \n",
    "            feat = self._extract_feat(kwargs, o)    \n",
    "            _, sc_kwargs = scatter_kwargs(inputs, feat, device_ids, chunk_sizes, dim=self.dim)\n",
    "            for p,q in zip(scattered_kwargs, sc_kwargs): p.update(q)\n",
    "            \n",
    "        return tuple(scattered_inputs), tuple(scattered_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553913e-875d-44c7-b58e-3b77ab8c4ae7",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484462a0-4806-461a-957c-6ae5d1bd1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        for k,v in kwargs.items(): print(k, ': ', v, ', ', v.device)\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9a39d-bff3-4588-9fd4-2899208a6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = XCDataParallel(module=MyModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bff3f-e395-4486-9865-19d0f5bff763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_input_ids :  data_input_ids :  tensor([[  101,   153,  4490,  7170, 22918,  2105,  1818,   102,     0,     0],\n",
      "        [  101, 11341,  8032,  1548,   102,     0,     0,     0,     0,     0],\n",
      "        [  101, 19166,  2779,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1652,  1535,   112,   188,  1569,  4896,  3779,  1264,   102],\n",
      "        [  101, 12556,  4616,  2328,   102,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:1') ,  cuda:1\n",
      "data_token_type_ids :  tensor([[  101, 18958, 11752,  1186, 11907,  2354,  4559,   102,     0,     0],\n",
      "        [  101, 12886,  2161,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3132,   161,  4538,  2162,   102,     0,     0,     0,     0],\n",
      "        [  101,  5755,   174, 26623,  4724,   102,     0,     0,     0,     0],\n",
      "        [  101,  5479,  1948,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   153, 17384,  1161,  1595,   113, 15019,   114,   102,     0]],\n",
      "       device='cuda:0') ,  cuda:0\n",
      "data_token_type_ids :  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n",
      "data_attention_mask :  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') ,  cuda:0\n",
      "data_attention_mask :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n",
      "lbl2data_data2ptr :  tensor([1, 3, 1, 4, 4], device='cuda:1') ,  cuda:1\n",
      "lbl2data_idx :  tensor([ 51524, 180191, 187988, 267293, 206451, 134916, 134917, 134918, 134973,\n",
      "         11661,  14798,  17605,  19869], device='cuda:1') ,  cuda:1\n",
      "lbl2data_input_ids :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:0') ,  cuda:0\n",
      "lbl2data_data2ptr :  tensor([ 1,  1,  2, 12,  5,  1], device='cuda:0') ,  cuda:0\n",
      "lbl2data_idx :  tensor([268888,  69066,  51848, 127494,  14400,  14402,  21360,  37008,  37172,\n",
      "         37259,  66128, 117048, 134455, 184458, 201618, 223919,   1160,  14855,\n",
      "        125870, 141101, 184009, 200719], device='cuda:0') ,  cuda:0\n",
      "lbl2data_input_ids :  tensor([[  101,  5619,  1104, 20764,  7709,  1107,  1103,   102,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  3479,  1555,  2635,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  8410, 10614,  4048,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  8410, 13274,  2635,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,   144, 15818,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1652,  1569,  1223,   118,  1406,  4896,  3779,  1264,   102,\n",
      "             0,     0,     0],\n",
      "        [  101,  1652,  1569,  4896,  1978,  1116,  1264,   102,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1652,  9843,  1116,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1652,  1569,  4896,  3779,  1264,   102,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 25738,   113,  6170,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 22781,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 12556,  4616,  2328,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1124, 27078,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]], device='cuda:1') ,  cuda:1\n",
      "lbl2data_token_type_ids :  tensor([[  101,  9018,  1116,  6430,  1104,  1709,  1104,   102,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5619,  1104,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5619,  1104,  1764,  2163,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,   144,  5697,  1399,   147,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  3180,  9672,  1116,  7107,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 16890, 12562,  2818,  1104,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 12118,  8767,  4571,  1769,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5755,  1769, 24768,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 14271,  2749,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5755,   174, 26623,  4724,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 13525,  1306,  1105,  1886,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5713,  4616,  2007, 10793,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 16890, 12789,  9022,  5709,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 16890, 12562, 22183,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,   151,  2149,   100, 12054,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1528,  5657, 14972,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1760,  7723,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  9040,  2263,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  5479,  1948,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101, 12125,  4817,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  8125,  3954,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4089,  1104,  1103,  4317,  5700,   102,     0,     0,     0,\n",
      "             0,     0,     0]], device='cuda:0') ,  cuda:0\n",
      "lbl2data_token_type_ids :  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n",
      "lbl2data_attention_mask :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') ,  cuda:0\n",
      "lbl2data_attention_mask :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0') ,  cuda:0\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a2e80-1b1f-4244-981b-4547cd7e3a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([268888,  69066,  51848, 127494,  14400,  14402,  21360,  37008,  37172,\n",
       "         37259,  66128, 117048, 134455, 184458, 201618, 223919,   1160,  14855,\n",
       "        125870, 141101, 184009, 200719,  51524, 180191, 187988, 267293, 206451,\n",
       "        134916, 134917, 134918, 134973,  11661,  14798,  17605,  19869])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch['lbl2data_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a130ea-6f06-4e02-902c-bfb366e03a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 18958, 11752,  1186, 11907,  2354,  4559,   102,     0,     0],\n",
       "        [  101, 12886,  2161,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  3132,   161,  4538,  2162,   102,     0,     0,     0,     0],\n",
       "        [  101,  5755,   174, 26623,  4724,   102,     0,     0,     0,     0],\n",
       "        [  101,  5479,  1948,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,   153, 17384,  1161,  1595,   113, 15019,   114,   102,     0],\n",
       "        [  101,   153,  4490,  7170, 22918,  2105,  1818,   102,     0,     0],\n",
       "        [  101, 11341,  8032,  1548,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 19166,  2779,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1652,  1535,   112,   188,  1569,  4896,  3779,  1264,   102],\n",
       "        [  101, 12556,  4616,  2328,   102,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch['data_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895da9d-a810-4ac1-953e-2200009481c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  2, 12,  5,  1,  1,  3,  1,  4,  4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch['lbl2data_data2ptr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc71ef-5e63-4330-8a9d-bf6cf342410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  9018,  1116,  6430,  1104,  1709,  1104,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5619,  1104,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5619,  1104,  1764,  2163,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   144,  5697,  1399,   147,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  3180,  9672,  1116,  7107,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 16890, 12562,  2818,  1104,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 12118,  8767,  4571,  1769,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5755,  1769, 24768,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 14271,  2749,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5755,   174, 26623,  4724,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 13525,  1306,  1105,  1886,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5713,  4616,  2007, 10793,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 16890, 12789,  9022,  5709,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 16890, 12562, 22183,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   151,  2149,   100, 12054,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1528,  5657, 14972,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1760,  7723,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  9040,  2263,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5479,  1948,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 12125,  4817,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  8125,  3954,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  4089,  1104,  1103,  4317,  5700,   102,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  5619,  1104, 20764,  7709,  1107,  1103,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  3479,  1555,  2635,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  8410, 10614,  4048,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  8410, 13274,  2635,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   144, 15818,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1652,  1569,  1223,   118,  1406,  4896,  3779,  1264,   102,\n",
       "             0,     0,     0],\n",
       "        [  101,  1652,  1569,  4896,  1978,  1116,  1264,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1652,  9843,  1116,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1652,  1569,  4896,  3779,  1264,   102,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 25738,   113,  6170,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 22781,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 12556,  4616,  2328,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1124, 27078,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch['lbl2data_input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60df7f-8004-49ec-8cce-eb36a285c8e2",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7b634-3ec5-4b5c-9d1f-75f32fa47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCEvalLoopOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    targ_idx: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    targ_ptr: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "\n",
    "class XCPredictionOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fb71-7eb7-43c6-96ba-71759aa2b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearningArguments(Seq2SeqTrainingArguments):\n",
    "\n",
    "    @delegates(Seq2SeqTrainingArguments.__init__)\n",
    "    def __init__(self, \n",
    "                 generation_length_penalty:Optional[float]=1.0, \n",
    "                 generation_num_beams:Optional[int]=5,\n",
    "                 generation_max_beams:Optional[int]=10,\n",
    "                 generation_max_info:Optional[int]=None,\n",
    "                 representation_accumulation_steps:Optional[int]=None,\n",
    "                 representation_attribute:Optional[str]='data_repr',\n",
    "                 representation_num_beams:Optional[int]=5,\n",
    "                 index_space:Optional[str]='cosine', \n",
    "                 index_efc:Optional[int]=200, \n",
    "                 index_m:Optional[int]=16, \n",
    "                 index_efs:Optional[int]=50,\n",
    "                 index_num_threads:Optional[int]=84,\n",
    "                 predict_with_generation:Optional[bool]=False,\n",
    "                 predict_with_representation:Optional[bool]=False,\n",
    "                 output_concatenation_weight:Optional[float]=1.0,\n",
    "                 group_by_cluster:Optional[bool]=False,\n",
    "                 minimum_clusters:Optional[int]=3,\n",
    "                 maximum_clusters:Optional[int]=None,\n",
    "                 num_cluster_update_epochs:Optional[int]=1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('generation_num_beams,generation_max_beams,generation_length_penalty,generation_max_info')\n",
    "        store_attr('representation_accumulation_steps,representation_attribute,representation_num_beams')\n",
    "        store_attr('index_space,index_efc,index_m,index_efs,index_num_threads')\n",
    "        store_attr('predict_with_generation,predict_with_representation,output_concatenation_weight')\n",
    "        store_attr('group_by_cluster,num_cluster_update_epochs')\n",
    "        self.minimum_clusters = max(1, minimum_clusters)\n",
    "        self.maximum_clusters = max(minimum_clusters, maximum_clusters) if maximum_clusters is not None else minimum_clusters\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b036146-ad70-4c47-802c-763f1f2aaf45",
   "metadata": {},
   "source": [
    "### `XCLearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b4bd-cd24-4b96-af56-474dfcbdcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearner(Seq2SeqTrainer):\n",
    "\n",
    "    @delegates(Seq2SeqTrainer.__init__)\n",
    "    def __init__(self, \n",
    "                 trie:Optional[Trie]=None, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tbs = TrieBeamSearch(trie, n_bm=self.args.generation_num_beams, max_bm=self.args.generation_max_beams,\n",
    "                                  len_penalty=self.args.generation_length_penalty, max_info=self.args.generation_max_info)\n",
    "        self.idxs = IndexSearch(space=self.args.index_space, efc=self.args.index_efc, m=self.args.index_m, efs=self.args.index_efs, \n",
    "                                n_bm=self.args.representation_num_beams, n_threads=self.args.index_num_threads)\n",
    "\n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        if unwrap_model(model) is not model:\n",
    "            return model\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            model = XCDataParallel(module=model)\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, eval_dataset:Optional[Dataset]=None, ignore_keys:Optional[List[str]]=None, metric_key_prefix:str=\"eval\",\n",
    "                 **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "        self.gather_function, self._gen_kwargs  = self.accelerator.gather, gen_kwargs\n",
    "\n",
    "        if self._perform_representation(unwrap_model(self.model)): self._build_lbl_index(eval_dataset)\n",
    "            \n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "    \n",
    "    def predict(self, test_dataset: Dataset, ignore_keys:Optional[List[str]]=None, metric_key_prefix: str = \"test\",**gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "\n",
    "        self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
    "        self._memory_tracker.start()\n",
    "        \n",
    "        if self._perform_representation(unwrap_model(self.model)): self._build_lbl_index(test_dataset)\n",
    "            \n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        output = self.evaluation_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(metric_key_prefix,start_time,num_samples=output.num_samples,num_steps=math.ceil(output.num_samples / total_batch_size),)\n",
    "        )\n",
    "        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "        return XCPredictionOutput(pred_idx=output.pred_idx, pred_ptr=output.pred_ptr, pred_score=output.pred_score, metrics=output.metrics, num_samples=output.num_samples)\n",
    "    \n",
    "    def _gather_host_output(self, output, host_output):\n",
    "        if output is not None:\n",
    "            output = self.accelerator.pad_across_processes(output, dim=1, pad_index=-100)\n",
    "            output = self.gather_function((output))\n",
    "            return output if host_output is None else nested_concat(host_output, output, padding_index=-100)\n",
    "        else: return host_output\n",
    "\n",
    "    def _gather_all_output(self, host_output, all_output):\n",
    "        if host_output is not None:\n",
    "            if isinstance(host_output, torch.Tensor): host_output = host_output.cpu()\n",
    "            return host_output if all_output is None else nested_concat(all_output, host_output, padding_index=-100)\n",
    "        else: return all_output\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f8f9-c2c1-4391-8ec9-d6ea44346107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def generation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"gen_num_beams\") if \"gen_num_beams\" in kwargs and kwargs[\"gen_num_beams\"] is not None else self.args.generation_num_beams\n",
    "    len_penalty = kwargs.pop(\"length_penalty\") if \"length_penalty\" in kwargs and kwargs[\"length_penalty\"] is not None else self.args.generation_length_penalty\n",
    "    \n",
    "    with torch.no_grad(): o = self.tbs.proc(self.model, inputs.copy(), n_bm=n_bm, len_penalty=len_penalty)\n",
    "        \n",
    "    return {'pred_idx':o['info2seq2data_idx'], 'pred_score':o['info2seq2data_score'], 'pred_ptr':o['info2seq2data_data2ptr']}\n",
    "\n",
    "@patch\n",
    "def representation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
    "    \n",
    "    with torch.no_grad(): o = getattr(model(**inputs), self.args.representation_attribute)\n",
    "    o = self.idxs.proc(o.cpu(), n_bm=n_bm)\n",
    "        \n",
    "    return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351dde7-b678-41d5-b710-005cb7db6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _perform_generation(self:XCLearner, model:nn.Module, predict_with_generation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_generation = self.args.predict_with_generation if predict_with_generation is None else predict_with_generation\n",
    "    return getattr(model,'use_generation') if hasattr(model,'use_generation') else predict_with_generation\n",
    "\n",
    "@patch\n",
    "def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
    "    return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
    "\n",
    "@patch\n",
    "def resize_pred(cls:XCLearner, t, n_t):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    r_t = torch.ones((len(t),), dtype=xn_t.dtype).scatter(0, t_ptr, xn_t)\n",
    "    xt = t.repeat_interleave(r_t).view(len(n_t), -1)\n",
    "    return xt\n",
    "\n",
    "@patch\n",
    "def output_mask(cls:XCLearner, n_t, l):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    mask_ptr = t_ptr+torch.arange(len(t_ptr))+1\n",
    "    mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype).scatter(0, mask_ptr, 0)\n",
    "    r_mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype).scatter(0, mask_ptr, xn_t-1)\n",
    "    mask = mask.repeat_interleave(r_mask).view(len(n_t), -1)\n",
    "    return mask\n",
    "\n",
    "@patch\n",
    "def resize_output(cls:XCLearner, pred_idx, pred_score, pred_ptr):\n",
    "    return cls.resize_pred(pred_idx, pred_ptr), cls.resize_pred(pred_score, pred_ptr), cls.output_mask(pred_ptr, len(pred_idx)), pred_ptr\n",
    "\n",
    "@patch\n",
    "def concatenate_output(cls:XCLearner, gen_o:Dict, repr_o:Dict):\n",
    "    gen_o['pred_score'] = torch.exp(gen_o['pred_score'])*cls.args.output_concatenation_weight\n",
    "    gen_o, repr_o = cls.resize_output(**gen_o), cls.resize_output(**repr_o)\n",
    "    pred_idx, pred_score, mask = [torch.hstack([gen_o[i], repr_o[i]]).flatten() for i in range(3)]\n",
    "    idx = torch.where(mask)[0]\n",
    "    return {\n",
    "        'pred_idx': pred_idx[idx],\n",
    "        'pred_score': pred_score[idx],\n",
    "        'pred_ptr': gen_o[3]+repr_o[3],\n",
    "    }\n",
    "    \n",
    "@patch\n",
    "def prediction_step(\n",
    "    self:XCLearner,\n",
    "    model: nn.Module,\n",
    "    inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "    prediction_loss_only: bool,\n",
    "    predict_with_generation: bool,\n",
    "    predict_with_representation: bool,\n",
    "    ignore_keys: Optional[List[str]] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "    with torch.no_grad():\n",
    "        with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
    "        loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "    prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
    "    if prediction_loss_only: return loss, None\n",
    "\n",
    "    output, repr_o = None, None\n",
    "    if self._perform_generation(model, predict_with_generation): output = self.generation_output(model, inputs, **kwargs)\n",
    "    if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
    "    if output is None: output = repr_o\n",
    "    elif repr_o is not None: output = self.concatenate_output(output, repr_o)\n",
    "        \n",
    "    labels = {'targ_idx':inputs['lbl2data_idx'], 'targ_ptr':inputs['lbl2data_data2ptr']} if 'lbl2data_idx' in inputs else None\n",
    "    if labels is not None: output.update(labels)\n",
    "    \n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189c5d7-a677-43e6-a78d-7485c07d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
    "    dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
    "    dataset = dataset if self.train_dataset is None else self.train_dataset\n",
    "    if dataset is not None:\n",
    "        lbl_dset = dataset.lbl_dset\n",
    "        lbl_dl = self.get_test_dataloader(lbl_dset)\n",
    "        lbl_repr = self.get_representation(lbl_dl)\n",
    "        self.idxs.build(lbl_repr)\n",
    "    else: raise ValueError('Failed to build `self.idxs`')\n",
    "    \n",
    "@patch\n",
    "def evaluation_loop(\n",
    "    self:XCLearner,\n",
    "    dataloader:DataLoader,\n",
    "    description:str,\n",
    "    prediction_loss_only:Optional[bool] = None,\n",
    "    predict_with_generation:Optional[bool]=None,\n",
    "    predict_with_representation:Optional[bool]=None,\n",
    "    ignore_keys:Optional[List[str]] = None,\n",
    "    metric_key_prefix:str=\"eval\",\n",
    ") -> XCEvalLoopOutput:\n",
    "\n",
    "    args = self.args\n",
    "    prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "    model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "    if len(self.accelerator._models) == 0 and model is self.model:\n",
    "        model = self.accelerator.prepare(model) if self.is_deepspeed_enabled else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "        if self.is_fsdp_enabled: self.model = model\n",
    "        if model is not self.model: self.model_wrapped = model\n",
    "        if self.is_deepspeed_enabled: self.deepspeed = self.model_wrapped\n",
    "\n",
    "    batch_size = self.args.eval_batch_size\n",
    "    model.eval()\n",
    "    self.callback_handler.eval_dataloader = dataloader\n",
    "    eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "    \n",
    "    if args.past_index >= 0: self._past = None\n",
    "\n",
    "    losses_host, all_losses = None, None\n",
    "    host_output, all_output = {}, {}\n",
    "    \n",
    "    observed_num_examples = 0\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        observed_batch_size = find_batch_size(inputs)\n",
    "        if observed_batch_size is not None:\n",
    "            observed_num_examples += observed_batch_size\n",
    "            if batch_size is None: batch_size = observed_batch_size\n",
    "                \n",
    "        loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
    "        \n",
    "        if loss is not None:\n",
    "            losses = self.gather_function((loss.repeat(batch_size)))\n",
    "            losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
    "        for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
    "            \n",
    "        self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "        \n",
    "        if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "            if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "            for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "    \n",
    "    self.gather_function = self.accelerator.gather_for_metrics\n",
    "    if args.past_index and hasattr(self, \"_past\"): delattr(self, \"_past\")\n",
    "\n",
    "    if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "    for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "        \n",
    "    if has_length(eval_dataset): num_samples = len(eval_dataset)\n",
    "    elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "        num_samples = eval_dataset.num_examples\n",
    "    else:\n",
    "        if has_length(dataloader): num_samples = self.num_examples(dataloader)\n",
    "        else: num_samples = observed_num_examples\n",
    "    if num_samples == 0 and observed_num_examples > 0: num_samples = observed_num_examples\n",
    "\n",
    "    if (self.compute_metrics is not None and \n",
    "        'targ_idx' in all_output and all_output['targ_idx'] is not None and \n",
    "        'pred_idx' in all_output and all_output['pred_idx'] is not None): \n",
    "        metrics = self.compute_metrics(**all_output)\n",
    "    else: metrics = {}\n",
    "        \n",
    "    metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "    if all_losses is not None: metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "    if hasattr(self, \"jit_compilation_time\"): metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
    "        \n",
    "    for key in list(metrics.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"): metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "    \n",
    "    return XCEvalLoopOutput(pred_idx=all_output['pred_idx'], pred_ptr=all_output['pred_ptr'], pred_score=all_output['pred_score'],\n",
    "                            targ_idx=all_output['targ_idx'], targ_ptr=all_output['targ_ptr'], metrics=metrics, num_samples=num_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bec117-9f89-445a-b82a-5def923d6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_representation(self:XCLearner, dataloader: DataLoader):\n",
    "    data_host, all_data = None, None\n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad(): data = getattr(self.model(**inputs), self.args.representation_attribute)\n",
    "        data_host = self._gather_host_output(data, data_host)\n",
    "        if self.args.representation_accumulation_steps is not None and (step + 1) % self.args.representation_accumulation_steps == 0:\n",
    "            all_data, data_host = self._gather_all_output(data_host, all_data), None\n",
    "    return self._gather_all_output(data_host, all_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ec06d-459a-4ba0-8b1d-74ff1d84a4ff",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b71a7-4ec2-47c5-a72d-77001b9c9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_train_sampler(self:XCLearner):\n",
    "    if self.train_dataset is None or not has_length(self.train_dataset):\n",
    "        return None\n",
    "        \n",
    "    if self.args.group_by_length:\n",
    "        if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\n",
    "            lengths = (\n",
    "                self.train_dataset[self.args.length_column_name]\n",
    "                if self.args.length_column_name in self.train_dataset.column_names\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            lengths = None\n",
    "        model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n",
    "        return LengthGroupedSampler(\n",
    "            self.args.train_batch_size * self.args.gradient_accumulation_steps,\n",
    "            dataset=self.train_dataset,\n",
    "            lengths=lengths,\n",
    "            model_input_name=model_input_name,\n",
    "        )\n",
    "\n",
    "    elif self.args.group_by_cluster:\n",
    "        return ClusterGroupedSampler(n=len(self.train_dataset))\n",
    "    else:\n",
    "        return RandomSampler(self.train_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cb389-f267-44a3-a65e-39b517b86660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_train_dataloader(self:XCLearner):\n",
    "    if self.train_dataset is None:\n",
    "        raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "    train_dataset = self.train_dataset\n",
    "    data_collator = self.data_collator\n",
    "    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "    else:\n",
    "        data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "    dataloader_params = {\n",
    "        \"batch_size\": self._train_batch_size,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"num_workers\": self.args.dataloader_num_workers,\n",
    "        \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "    }\n",
    "\n",
    "    if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "        dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "        dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "        dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "        dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "    \n",
    "    return DataLoader(train_dataset, **dataloader_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447567d1-8fc1-4582-9673-34ff1c971048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_n_cluster(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    if self.args.maximum_clusters is None: return self.args.minimum_clusters\n",
    "    else:\n",
    "        n_cluster = (self.args.maximum_clusters-self.args.minimum_clusters)/num_train_epochs*epochs_trained\n",
    "        n_cluster = int(self.args.minimum_clusters+n_cluster)\n",
    "        return n_cluster\n",
    "\n",
    "@patch\n",
    "def _get_train_data_cluster(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    dataset = self.train_dataset.data_dset\n",
    "    dataloader = self.get_test_dataloader(dataset)\n",
    "    data_repr = learn.get_representation(dataloader)\n",
    "    cluster, _ = BalancedClusters.proc(data_repr, n_cluster=self._get_n_cluster(epochs_trained, num_train_epochs))\n",
    "    return cluster\n",
    "\n",
    "@patch\n",
    "def update_dataloader_sampler(self:XCLearner, dataloader:DataLoader, epochs_trained:int, num_train_epochs:int):\n",
    "    if isinstance(dataloader.sampler, ClusterGroupedSampler):\n",
    "        cluster = self._get_train_data_cluster(epochs_trained, num_train_epochs)\n",
    "        dataloader.sampler.set_cluster(cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33907726-5248-4c73-84f1-ce900f119ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _validate_group_by_cluster(self:XCLearner):\n",
    "    if self.args.group_by_cluster and (not hasattr(model,'use_representation') or  not getattr(unwrap_model(model),'use_representation')):\n",
    "        raise ValueError('Cannot use `group_by_cluster` for models without `use_representation`.')\n",
    "        self.args.group_by_cluster = False\n",
    "\n",
    "@patch\n",
    "def _inner_training_loop(\n",
    "    self:XCLearner, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "):\n",
    "    self.accelerator.free_memory()\n",
    "    self._train_batch_size = batch_size\n",
    "    if self.args.auto_find_batch_size:\n",
    "        if self.state.train_batch_size != self._train_batch_size:\n",
    "            from accelerate.utils import release_memory\n",
    "\n",
    "            (self.model_wrapped,) = release_memory(self.model_wrapped)\n",
    "            self.model_wrapped = self.model\n",
    "\n",
    "            # Check for DeepSpeed *after* the intial pass and modify the config\n",
    "            if self.is_deepspeed_enabled:\n",
    "                # Temporarily unset `self.args.train_batch_size`\n",
    "                original_bs = self.args.per_device_train_batch_size\n",
    "                self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\n",
    "                self.propagate_args_to_deepspeed(True)\n",
    "                self.args.per_device_train_batch_size = original_bs\n",
    "        self.state.train_batch_size = self._train_batch_size\n",
    "    logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n",
    "    \n",
    "    # Data loader and number of training steps\n",
    "    self._validate_group_by_cluster()\n",
    "    train_dataloader = self.get_train_dataloader()\n",
    "    \n",
    "    if self.is_fsdp_xla_v2_enabled:\n",
    "        train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
    "\n",
    "    # Setting up training control variables:\n",
    "    # number of training epochs: num_train_epochs\n",
    "    # number of training steps per epoch: num_update_steps_per_epoch\n",
    "    # total number of training steps to execute: max_steps\n",
    "    total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "\n",
    "    len_dataloader = None\n",
    "    num_train_tokens = None\n",
    "    if has_length(train_dataloader):\n",
    "        len_dataloader = len(train_dataloader)\n",
    "        num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
    "        num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "        num_examples = self.num_examples(train_dataloader)\n",
    "        if args.max_steps > 0:\n",
    "            max_steps = args.max_steps\n",
    "            num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "                args.max_steps % num_update_steps_per_epoch > 0\n",
    "            )\n",
    "            # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "            # the best we can do.\n",
    "            num_train_samples = args.max_steps * total_train_batch_size\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = (\n",
    "                    self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "                )\n",
    "        else:\n",
    "            max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "            num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "            num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
    "    elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
    "        max_steps = args.max_steps\n",
    "        # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "        num_train_epochs = sys.maxsize\n",
    "        num_update_steps_per_epoch = max_steps\n",
    "        num_examples = total_train_batch_size * args.max_steps\n",
    "        num_train_samples = args.max_steps * total_train_batch_size\n",
    "        if args.include_tokens_per_second:\n",
    "            num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n",
    "            f\" {args.max_steps}\"\n",
    "        )\n",
    "\n",
    "    if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
    "        if self.args.n_gpu > 1:\n",
    "            # nn.DataParallel(model) replicates the model, creating new variables and module\n",
    "            # references registered here no longer work on other gpus, breaking the module\n",
    "            raise ValueError(\n",
    "                \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n",
    "                \" (torchrun or torch.distributed.launch (deprecated)).\"\n",
    "            )\n",
    "        else:\n",
    "            debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
    "\n",
    "    delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n",
    "\n",
    "    # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
    "    if self._created_lr_scheduler:\n",
    "        self.lr_scheduler = None\n",
    "        self._created_lr_scheduler = False\n",
    "\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
    "\n",
    "    if not delay_optimizer_creation:\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    self.state = TrainerState()\n",
    "    self.state.is_hyper_param_search = trial is not None\n",
    "    self.state.train_batch_size = self._train_batch_size\n",
    "\n",
    "    # Compute absolute values for logging, eval, and save if given as ratio\n",
    "    if args.logging_steps is not None:\n",
    "        if args.logging_steps < 1:\n",
    "            self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
    "        else:\n",
    "            self.state.logging_steps = args.logging_steps\n",
    "    if args.eval_steps is not None:\n",
    "        if args.eval_steps < 1:\n",
    "            self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
    "        else:\n",
    "            self.state.eval_steps = args.eval_steps\n",
    "    if args.save_steps is not None:\n",
    "        if args.save_steps < 1:\n",
    "            self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
    "        else:\n",
    "            self.state.save_steps = args.save_steps\n",
    "\n",
    "    # Activate gradient checkpointing if needed\n",
    "    if args.gradient_checkpointing:\n",
    "        if args.gradient_checkpointing_kwargs is None:\n",
    "            gradient_checkpointing_kwargs = {}\n",
    "        else:\n",
    "            gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs\n",
    "\n",
    "        self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "\n",
    "    model = self._wrap_model(self.model_wrapped)\n",
    "\n",
    "    # as the model is wrapped, don't use `accelerator.prepare`\n",
    "    # this is for unhandled cases such as\n",
    "    # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n",
    "    use_accelerator_prepare = True if model is self.model else False\n",
    "\n",
    "    if delay_optimizer_creation:\n",
    "        if use_accelerator_prepare:\n",
    "            self.model = self.accelerator.prepare(self.model)\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    # prepare using `accelerator` prepare\n",
    "    if use_accelerator_prepare:\n",
    "        self.model.train()\n",
    "        if hasattr(self.lr_scheduler, \"step\"):\n",
    "            if self.use_apex:\n",
    "                model = self.accelerator.prepare(self.model)\n",
    "            else:\n",
    "                model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
    "        else:\n",
    "            # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n",
    "            model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "                self.model, self.optimizer, self.lr_scheduler\n",
    "            )\n",
    "\n",
    "    if self.is_fsdp_enabled:\n",
    "        self.model = self.model_wrapped = model\n",
    "\n",
    "    # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "    if model is not self.model:\n",
    "        self.model_wrapped = model\n",
    "\n",
    "    # backward compatibility\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.deepspeed = self.model_wrapped\n",
    "\n",
    "    # ckpt loading\n",
    "    if resume_from_checkpoint is not None:\n",
    "        if self.is_deepspeed_enabled:\n",
    "            deepspeed_load_checkpoint(\n",
    "                self.model_wrapped, resume_from_checkpoint, load_module_strict=not _is_peft_model(self.model)\n",
    "            )\n",
    "        elif is_sagemaker_mp_enabled() or self.is_fsdp_enabled:\n",
    "            self._load_from_checkpoint(resume_from_checkpoint, self.model_wrapped)\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
    "\n",
    "    # important: at this point:\n",
    "    # self.model         is the Transformers Model\n",
    "    # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),\n",
    "    # FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {num_examples:,}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
    "    if self.args.per_device_train_batch_size != self._train_batch_size:\n",
    "        logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
    "    logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
    "\n",
    "    self.state.epoch = 0\n",
    "    start_time = time.time()\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    steps_trained_progress_bar = None\n",
    "\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if resume_from_checkpoint is not None and os.path.isfile(\n",
    "        os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
    "    ):\n",
    "        self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
    "        epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "        if not args.ignore_data_skip:\n",
    "            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "            steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "        else:\n",
    "            steps_trained_in_current_epoch = 0\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "        logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "        if not args.ignore_data_skip:\n",
    "            logger.info(\n",
    "                f\"  Will skip the first {epochs_trained} epochs then the first\"\n",
    "                f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n",
    "            )\n",
    "\n",
    "    # Update the references\n",
    "    self.callback_handler.model = self.model\n",
    "    self.callback_handler.optimizer = self.optimizer\n",
    "    self.callback_handler.lr_scheduler = self.lr_scheduler\n",
    "    self.callback_handler.train_dataloader = train_dataloader\n",
    "    if self.hp_name is not None and self._trial is not None:\n",
    "        # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n",
    "        # parameter to Train when using DDP.\n",
    "        self.state.trial_name = self.hp_name(self._trial)\n",
    "    if trial is not None:\n",
    "        assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
    "        self.state.trial_params = hp_params(assignments)\n",
    "    else:\n",
    "        self.state.trial_params = None\n",
    "    # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "    # to set this after the load.\n",
    "    self.state.max_steps = max_steps\n",
    "    self.state.num_train_epochs = num_train_epochs\n",
    "    self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "    self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "    # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n",
    "    tr_loss = torch.tensor(0.0).to(args.device)\n",
    "    # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n",
    "    self._total_loss_scalar = 0.0\n",
    "    self._globalstep_last_logged = self.state.global_step\n",
    "    model.zero_grad()\n",
    "    grad_norm: Optional[float] = None\n",
    "\n",
    "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "\n",
    "    # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n",
    "    if not args.ignore_data_skip:\n",
    "        for epoch in range(epochs_trained):\n",
    "            sampler = get_dataloader_sampler(train_dataloader)\n",
    "            sampler_kinds = [RandomSampler]\n",
    "            if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "                sampler_kinds.append(SeedableRandomSampler)\n",
    "            is_random_sampler = isinstance(sampler, tuple(sampler_kinds))\n",
    "            if not is_random_sampler:\n",
    "                # We just need to begin an iteration to create the randomization of the sampler.\n",
    "                for _ in train_dataloader:\n",
    "                    break\n",
    "            else:\n",
    "                # Otherwise we need to call the whooooole sampler cause there is some random operation added\n",
    "                # AT THE VERY END!\n",
    "                sampler = sampler if sampler is not None else []\n",
    "                _ = list(sampler)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(epochs_trained, num_train_epochs):\n",
    "        if self.args.group_by_cluster and epochs_trained % self.args.num_cluster_update_epochs == 0:\n",
    "            self.update_dataloader_sampler(train_dataloader, epochs_trained, num_train_epochs)\n",
    "            \n",
    "        epoch_iterator = train_dataloader\n",
    "        if hasattr(epoch_iterator, \"set_epoch\"):\n",
    "            epoch_iterator.set_epoch(epoch)\n",
    "\n",
    "        # Reset the past mems state at the beginning of each epoch if necessary.\n",
    "        if args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        steps_in_epoch = (\n",
    "            len(epoch_iterator)\n",
    "            if len_dataloader is not None\n",
    "            else args.max_steps * args.gradient_accumulation_steps\n",
    "        )\n",
    "        self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n",
    "\n",
    "        if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "            self._load_rng_state(resume_from_checkpoint)\n",
    "\n",
    "        rng_to_sync = False\n",
    "        steps_skipped = 0\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n",
    "            steps_skipped = steps_trained_in_current_epoch\n",
    "            steps_trained_in_current_epoch = 0\n",
    "            rng_to_sync = True\n",
    "\n",
    "        step = -1\n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            if self.args.include_num_input_tokens_seen:\n",
    "                main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n",
    "                if main_input_name not in inputs:\n",
    "                    logger.warning(\n",
    "                        \"Tried to track the number of tokens seen, however the current model is \"\n",
    "                        \"not configured properly to know what item is the input. To fix this, add \"\n",
    "                        \"a `main_input_name` attribute to the model class you are using.\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.state.num_input_tokens_seen += self.accelerator.gather(inputs[main_input_name]).numel()\n",
    "            if rng_to_sync:\n",
    "                self._load_rng_state(resume_from_checkpoint)\n",
    "                rng_to_sync = False\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                if steps_trained_progress_bar is not None:\n",
    "                    steps_trained_progress_bar.update(1)\n",
    "                if steps_trained_in_current_epoch == 0:\n",
    "                    self._load_rng_state(resume_from_checkpoint)\n",
    "                continue\n",
    "            elif steps_trained_progress_bar is not None:\n",
    "                steps_trained_progress_bar.close()\n",
    "                steps_trained_progress_bar = None\n",
    "\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
    "\n",
    "            with self.accelerator.accumulate(model):\n",
    "                tr_loss_step = self.training_step(model, inputs)\n",
    "\n",
    "            if (\n",
    "                args.logging_nan_inf_filter\n",
    "                and not is_torch_tpu_available()\n",
    "                and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "            ):\n",
    "                # if loss is nan or inf simply add the average of previous logged losses\n",
    "                tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "            else:\n",
    "                tr_loss += tr_loss_step\n",
    "\n",
    "            self.current_flos += float(self.floating_point_ops(inputs))\n",
    "\n",
    "            is_last_step_and_steps_less_than_grad_acc = (\n",
    "                steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                total_batched_samples % args.gradient_accumulation_steps == 0\n",
    "                or\n",
    "                # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                is_last_step_and_steps_less_than_grad_acc\n",
    "            ):\n",
    "                # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\n",
    "                # in accelerate. So, explicitly enable sync gradients to True in that case.\n",
    "                if is_last_step_and_steps_less_than_grad_acc:\n",
    "                    self.accelerator.gradient_state._set_sync_gradients(True)\n",
    "\n",
    "                # Gradient clipping\n",
    "                if args.max_grad_norm is not None and args.max_grad_norm > 0:\n",
    "                    # deepspeed does its own clipping\n",
    "\n",
    "                    if is_sagemaker_mp_enabled() and args.fp16:\n",
    "                        _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n",
    "                    elif self.use_apex:\n",
    "                        # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "                        _grad_norm = nn.utils.clip_grad_norm_(\n",
    "                            amp.master_params(self.optimizer),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "                    else:\n",
    "                        _grad_norm = self.accelerator.clip_grad_norm_(\n",
    "                            model.parameters(),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "\n",
    "                    if (\n",
    "                        is_accelerate_available()\n",
    "                        and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n",
    "                    ):\n",
    "                        grad_norm = model.get_global_grad_norm()\n",
    "                    else:\n",
    "                        grad_norm = _grad_norm.item() if _grad_norm is not None else None\n",
    "\n",
    "                # Optimizer step\n",
    "                self.optimizer.step()\n",
    "                optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n",
    "                if optimizer_was_run:\n",
    "                    # Delay optimizer scheduling until metrics are generated\n",
    "                    if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                self.state.global_step += 1\n",
    "                self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n",
    "                self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
    "\n",
    "                self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "            else:\n",
    "                self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
    "\n",
    "            if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "                # PyTorch/XLA relies on the data loader to insert the mark_step for\n",
    "                # each step. Since we are breaking the loop early, we need to manually\n",
    "                # insert the mark_step here.\n",
    "                if is_torch_tpu_available():\n",
    "                    xm.mark_step()\n",
    "                break\n",
    "        if step < 0:\n",
    "            logger.warning(\n",
    "                \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "                f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n",
    "                f\" num_steps ({max_steps}) higher than the number of available samples.\"\n",
    "            )\n",
    "            self.control.should_training_stop = True\n",
    "\n",
    "        self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
    "        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            if is_torch_tpu_available():\n",
    "                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                xm.master_print(met.metrics_report())\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n",
    "                    \"configured. Check your training configuration if this is unexpected.\"\n",
    "                )\n",
    "        if self.control.should_training_stop:\n",
    "            break\n",
    "\n",
    "    if args.past_index and hasattr(self, \"_past\"):\n",
    "        # Clean the state at the end of training\n",
    "        delattr(self, \"_past\")\n",
    "\n",
    "    logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "    if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n",
    "        # Wait for everyone to get here so we are sure the model has been saved by process 0.\n",
    "        if is_torch_tpu_available():\n",
    "            xm.rendezvous(\"load_best_model_at_end\")\n",
    "        elif args.parallel_mode == ParallelMode.DISTRIBUTED:\n",
    "            dist.barrier()\n",
    "        elif is_sagemaker_mp_enabled():\n",
    "            smp.barrier()\n",
    "\n",
    "        self._load_best_model()\n",
    "\n",
    "    # add remaining tr_loss\n",
    "    self._total_loss_scalar += tr_loss.item()\n",
    "    train_loss = self._total_loss_scalar / self.state.global_step\n",
    "\n",
    "    metrics = speed_metrics(\n",
    "        \"train\",\n",
    "        start_time,\n",
    "        num_samples=num_train_samples,\n",
    "        num_steps=self.state.max_steps,\n",
    "        num_tokens=num_train_tokens,\n",
    "    )\n",
    "    self.store_flos()\n",
    "    metrics[\"total_flos\"] = self.state.total_flos\n",
    "    metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "    self.is_in_train = False\n",
    "\n",
    "    self._memory_tracker.stop_and_update_metrics(metrics)\n",
    "\n",
    "    self.log(metrics)\n",
    "\n",
    "    run_dir = self._get_output_dir(trial)\n",
    "    checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)\n",
    "\n",
    "    # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.\n",
    "    if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:\n",
    "        for checkpoint in checkpoints_sorted:\n",
    "            if not os.path.samefile(checkpoint, self.state.best_model_checkpoint):\n",
    "                logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n",
    "                shutil.rmtree(checkpoint)\n",
    "\n",
    "    self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "\n",
    "    # Wait for the checkpoint to be uploaded.\n",
    "    self._finish_current_push()\n",
    "\n",
    "    # After training we make sure to retrieve back the original forward pass method\n",
    "    # for the embedding layer by removing the forward post hook.\n",
    "    if self.neftune_noise_alpha is not None:\n",
    "        self._deactivate_neftune(self.model)\n",
    "\n",
    "    return TrainOutput(self.state.global_step, train_loss, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc27f85-b8d7-4147-9e19-2ea8f95f7d6d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90781b5d-e073-4e77-b102-cdf72a55c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5926050-d0c2-401f-9131-9676fbf0831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "block = XCBlock.from_cfg('data', valid_pct=0.001, tokz='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b7ca-4600-4066-bfa4-a347724e9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/garbage/T1/',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_steps=10,\n",
    "    representation_accumulation_steps=10,\n",
    "    representation_attribute='data_repr',\n",
    "    evaluation_strategy='steps',\n",
    "    label_names=['lbl2data_idx'],\n",
    "    group_by_cluster=True,\n",
    "    minimum_clusters=5,\n",
    "    maximum_clusters=15,\n",
    "    num_cluster_update_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258a9bf-9a0a-4fbe-9ca9-55e375780297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0004 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dr_loss_fn.t', 'lm_loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "model = BT0004.from_pretrained('bert-base-uncased', lw=0.5, bsz=bsz, tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9385d-a46a-4b29-8424-a6c174db797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8faa0bdd917435aac249d192f3cd9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "trie = XCTrie.from_block(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5bd9b-3f60-4eaa-8d6d-ddc3e2e6fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "metric = PrecRecl(block.n_lbl, block.valid.data_lbl_filterer, prop=block.train.dset.data.data_lbl, pk=5, rk=5, rep_pk=[1, 3, 5], rep_rk=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc998894-1a9f-4c12-8c75-2f4ff5758302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    trie=trie,\n",
    "    data_collator=block.collator, \n",
    "    train_dataset=block.train.dset, \n",
    "    eval_dataset=block.valid.dset,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b369f0f-f0aa-4ba5-a311-e79ca5ff84b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f1d617baba4d0a9102de6bde393053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='103860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    21/103860 03:05 < 281:53:51, 0.10 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>Psp@1</th>\n",
       "      <th>Psp@3</th>\n",
       "      <th>Psp@5</th>\n",
       "      <th>Psn@1</th>\n",
       "      <th>Psn@3</th>\n",
       "      <th>Psn@5</th>\n",
       "      <th>R@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.706070</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.019889</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0.033410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/6 00:44 < 02:57, 0.02 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f1fbcd4a69404bb8042b6378f2a3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab372bf34c9450f9a1bf67f39cd8a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 413\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2412\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2412\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[104], line 34\u001b[0m, in \u001b[0;36mXCLearner.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather, gen_kwargs\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_representation(unwrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_lbl_index(eval_dataset)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "Cell \u001b[0;32mIn[107], line 52\u001b[0m, in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     49\u001b[0m     observed_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m---> 52\u001b[0m loss, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_with_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_with_representation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((loss\u001b[38;5;241m.\u001b[39mrepeat(batch_size)))\n",
      "Cell \u001b[0;32mIn[106], line 68\u001b[0m, in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_loss_only: \u001b[38;5;28;01mreturn\u001b[39;00m loss, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m output, repr_o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_generation(model, predict_with_generation): output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_representation(model, predict_with_representation): repr_o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_output(model, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: output \u001b[38;5;241m=\u001b[39m repr_o\n",
      "Cell \u001b[0;32mIn[105], line 13\u001b[0m, in \u001b[0;36mgeneration_output\u001b[0;34m(self, model, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m n_bm \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_num_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_num_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_num_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[1;32m     11\u001b[0m len_penalty \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_length_penalty\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_penalty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_idx\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_score\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_data2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:167\u001b[0m, in \u001b[0;36mTrieBeamSearch.proc\u001b[0;34m(self, model, inputs, n_bm, len_penalty)\u001b[0m\n\u001b[1;32m    165\u001b[0m logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    166\u001b[0m hyps \u001b[38;5;241m=\u001b[39m [TrieBeam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrie, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_penalty) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m--> 167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [h\u001b[38;5;241m.\u001b[39mproc(l) \u001b[38;5;28;01mfor\u001b[39;00m h,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hyps, logits)]\n\u001b[1;32m    168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfm({k:\u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m[o[k] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]})\n\u001b[1;32m    169\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_seq2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:167\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    166\u001b[0m hyps \u001b[38;5;241m=\u001b[39m [TrieBeam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrie, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_penalty) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m--> 167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hyps, logits)]\n\u001b[1;32m    168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfm({k:\u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m[o[k] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]})\n\u001b[1;32m    169\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_seq2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:144\u001b[0m, in \u001b[0;36mTrieBeam.proc\u001b[0;34m(self, logits, n_bm, len_penalty)\u001b[0m\n\u001b[1;32m    142\u001b[0m sc \u001b[38;5;241m=\u001b[39m logits[cur_len:cur_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(sc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m sc\n\u001b[1;32m    143\u001b[0m v_tok, v_sc, v_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid(ptr, sc)\n\u001b[0;32m--> 144\u001b[0m top_ptr, top_sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_tok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ptr, sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(top_ptr, top_sc)\n\u001b[1;32m    146\u001b[0m cur_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:102\u001b[0m, in \u001b[0;36mTrieBeam.topk\u001b[0;34m(self, ptr, tok, sc, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtopk\u001b[39m(\u001b[38;5;28mself\u001b[39m, ptr:List, tok:List, sc:List, idx:List):\n\u001b[1;32m    101\u001b[0m     top_sc, top_i \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 102\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtopk(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msort(torch\u001b[38;5;241m.\u001b[39mtensor(sc), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     top_sc \u001b[38;5;241m=\u001b[39m top_sc\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    106\u001b[0m     top_idx, top_tok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[(idx[i],tok[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_i]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccdb0b-e563-4ace-9aab-5cd634dfd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "o = learn.predict(learn.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b7444-332d-42b7-8766-e3192a1f2f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c40ed3-6a16-47ab-b6ad-4dc832cfeca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
