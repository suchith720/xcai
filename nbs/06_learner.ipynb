{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b937d98-9beb-445c-bc68-c8d71a8c1ae0",
   "metadata": {},
   "source": [
    "# learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714cf2-0372-49f6-a39c-24752a28145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b498e-a0a3-4bdd-8741-ca516ddc64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ac07c-66a4-40d0-a39b-e5220c20fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from packaging import version\n",
    "import torch, re, math, numpy as np, os, time, datasets, pickle\n",
    "from typing import Any, Tuple, Optional, Sequence, Union, Dict, List, NamedTuple\n",
    "from transformers import AutoTokenizer, BatchEncoding, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.parallel._functions import Scatter\n",
    "from torch.nn.parallel.scatter_gather import _is_namedtuple\n",
    "\n",
    "from xcai.core import *\n",
    "from xcai.data import *\n",
    "from xcai.representation.search import *\n",
    "from xcai.generation.trie import *\n",
    "from xcai.generation.generate import *\n",
    "from xcai.clustering.cluster import *\n",
    "from xcai.transform import PadFeatTfm\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from fastcore.dispatch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821000f3-31ca-44ed-a2a5-e060f900ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers.trainer_pt_utils import (\n",
    "    find_batch_size, \n",
    "    nested_concat, nested_numpify, \n",
    "    IterableDatasetShard, \n",
    "    get_dataloader_sampler, \n",
    "    get_model_param_count,\n",
    "    LengthGroupedSampler\n",
    ")\n",
    "from transformers.trainer_utils import has_length, denumpify_detensorize, speed_metrics, TrainOutput, HPSearchBackend, seed_worker\n",
    "from transformers.trainer_callback import TrainerState\n",
    "from transformers.trainer import _is_peft_model\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.utils import is_sagemaker_mp_enabled, is_accelerate_available, is_torch_tpu_available, logging, is_datasets_available\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "\n",
    "from transformers.integrations import hp_params\n",
    "from transformers.integrations.tpu import tpu_spmd_dataloader\n",
    "from transformers.integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n",
    "\n",
    "if is_accelerate_available():\n",
    "    from accelerate import Accelerator, skip_first_batches\n",
    "    from accelerate import __version__ as accelerate_version\n",
    "    from accelerate.utils import (\n",
    "        DistributedDataParallelKwargs,\n",
    "        DistributedType,\n",
    "        GradientAccumulationPlugin,\n",
    "        load_fsdp_model,\n",
    "        load_fsdp_optimizer,\n",
    "        save_fsdp_model,\n",
    "        save_fsdp_optimizer,\n",
    "    )\n",
    "\n",
    "    DATA_SAMPLERS = [RandomSampler]\n",
    "    if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "        from accelerate.data_loader import SeedableRandomSampler\n",
    "\n",
    "        DATA_SAMPLERS += [SeedableRandomSampler]\n",
    "\n",
    "    if is_deepspeed_available():\n",
    "        from accelerate.utils import DeepSpeedSchedulerWrapper\n",
    "\n",
    "if is_accelerate_available(\"0.28.0\"):\n",
    "    from accelerate.utils import DataLoaderConfiguration\n",
    "\n",
    "TRAINING_ARGS_NAME = \"training_args.bin\"\n",
    "TRAINER_STATE_NAME = \"trainer_state.json\"\n",
    "OPTIMIZER_NAME = \"optimizer.pt\"\n",
    "OPTIMIZER_NAME_BIN = \"optimizer.bin\"\n",
    "SCHEDULER_NAME = \"scheduler.pt\"\n",
    "SCALER_NAME = \"scaler.pt\"\n",
    "FSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547561-f805-40c1-a6b2-d89e772ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1b571-0043-4b86-acce-cb6cb0ef598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from xcai.block import *\n",
    "from xcai.models.PPP0XX import *\n",
    "from xcai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c73cc-d066-4c7f-a2b5-31003f38ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from xcai.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9985262",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab5fa-1f75-44e8-b533-525d577f4209",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188e3a7-9458-456f-8a19-104fd139a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "block = XCBlock.from_cfg('/home/aiscuser/scratch/datasets', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4d23b-4525-46bd-9a75-19ffca285235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch = block.train.one_batch(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47e1ed-ad70-475f-a01f-cd9320705796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee43b109060489393f1b4531512f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c7e7b7f64465ea007044a1668c98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf42c7-5471-4ff7-bc65-4020a5eb5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95cdc4-ff76-4cd7-aa20-a54a03f7c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "b = prepare_batch(m, batch, m_args='lbl2data_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef492419-4e5a-4f08-bb1b-3ba3283703d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7a04d-81d7-402d-9a2d-b449b3acd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = m.to('cuda')\n",
    "b = b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64be93-2e54-4fbd-841b-c5d2828f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5e531-8f3c-438e-ac10-b46c42e90093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.9452, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133a0cc-6ce3-4310-a2f4-5e936ac5ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abf6df-1ee2-4105-9b48-7e66829e8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final-aug-hlk.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad44158-2094-4c74-8313-611d125a49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-meta_distilbert-base-uncased_rm_ramen-cat.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbee9ca-5a2c-4be0-882d-f7fffec78cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiseealso_data_distilbert-base-uncased_rm_ngame.pkl\n",
      "wikiseealso_data_distilbert-base-uncased_xcnlg_ngame.pkl\n",
      "wikiseealso_data-meta_distilbert-base-uncased_rm_ramen-cat.pkl\n",
      "wikiseealso_data-metas_distilbert-base-uncased_rm_radga-aug-cat-block-032.pkl\n",
      "wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final-aug-cat.pkl\n",
      "wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final-aug-hlk.pkl\n",
      "wikiseealso_data-metas_distilbert-base-uncased_rm_radga-final.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls {pkl_dir}/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3638794-1429-44d7-9707-1d0e4bf54788",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d9190-682e-4017-954e-fad35c79ab13",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54c7fe-18b7-4502-a68d-5bbca3a75a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scatter(inputs, target_gpus, chunk_sizes=None, dim=0):\n",
    "    def scatter_map(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n",
    "        if _is_namedtuple(obj):\n",
    "            return [type(obj)(*args) for args in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, tuple) and len(obj) > 0:\n",
    "            return list(zip(*map(scatter_map, obj)))\n",
    "        if isinstance(obj, list) and len(obj) > 0:\n",
    "            return [list(i) for i in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, dict) and len(obj) > 0:\n",
    "            return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
    "        return [obj for _ in target_gpus] \n",
    "    try:\n",
    "        res = scatter_map(inputs)\n",
    "    finally:\n",
    "        scatter_map = None\n",
    "    return res\n",
    "    \n",
    "def scatter_kwargs(\n",
    "    inputs: Tuple[Any, ...],\n",
    "    kwargs: Optional[Dict[str, Any]],\n",
    "    target_gpus: Sequence[Union[int, torch.device]],\n",
    "    chunk_sizes: Optional[Sequence[int]]=None,\n",
    "    dim: int = 0,\n",
    ") -> Tuple[Tuple[Any, ...], Tuple[Dict[str, Any], ...]]:\n",
    "    scattered_inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n",
    "    scattered_kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n",
    "    if len(scattered_inputs) < len(scattered_kwargs):\n",
    "        scattered_inputs.extend(() for _ in range(len(scattered_kwargs) - len(scattered_inputs)))\n",
    "    elif len(scattered_kwargs) < len(inputs):\n",
    "        scattered_kwargs.extend({} for _ in range(len(scattered_inputs) - len(scattered_kwargs)))\n",
    "    return scattered_inputs, scattered_kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420b5aa-6d09-47c1-809d-af19e02f800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCDataParallel(DataParallel):\n",
    "\n",
    "    @delegates(DataParallel.__init__)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def _get_feat_name(self, x:Optional[Dict[str, Any]]):\n",
    "        return list(set([k.split('_', maxsplit=1)[0] for k in x]))\n",
    "    \n",
    "    def _extract_feat(self, x:Optional[Dict[str, Any]], prefix:str):\n",
    "        return {k:v for k,v in x.items() if re.match(f'^{prefix}_(?!.*2ptr)', k) or re.match(f'^.*_{prefix}2ptr$', k)}\n",
    "\n",
    "    def scatter(\n",
    "        self,\n",
    "        inputs: Tuple[Any, ...],\n",
    "        kwargs: Optional[Dict[str, Any]],\n",
    "        device_ids: Sequence[Union[int, torch.device]],\n",
    "    ) ->Any:\n",
    "        if len(inputs): raise ValueError('`inputs` should be empty.')    \n",
    "        feat_name = self._get_feat_name(kwargs)\n",
    "        \n",
    "        data_feat = self._extract_feat(kwargs, 'data')\n",
    "        scattered_inputs, scattered_kwargs = scatter_kwargs(inputs, data_feat, device_ids, None, dim=self.dim)\n",
    "        feat_name.remove('data')\n",
    "        \n",
    "        for k in feat_name:\n",
    "            ptr_name = f'{k}_data2ptr'\n",
    "            if ptr_name in scattered_kwargs[0] and scattered_kwargs[0][ptr_name] is not None:\n",
    "                chunk_sz = [o[ptr_name].sum().item() for o in scattered_kwargs]\n",
    "                if len(chunk_sz) < len(device_ids): \n",
    "                    chunk_sz.extend([0 for _ in range(len(device_ids) - len(chunk_sz))])\n",
    "                \n",
    "                feat = self._extract_feat(kwargs, k)\n",
    "                _, o = scatter_kwargs(inputs, feat, device_ids, chunk_sz, dim=self.dim)\n",
    "                for p,q in zip(scattered_kwargs, o): p.update(q)\n",
    "                    \n",
    "        return tuple(scattered_inputs), tuple(scattered_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553913e-875d-44c7-b58e-3b77ab8c4ae7",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, BatchEncoding\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae19912",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/aiscuser/scratch/datasets'\n",
    "pkl_dir = f'{data_dir}/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_rm_radga.pkl', 'rb') as file: \n",
    "    block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{pkl_dir}/wikiseealso_data-metas_distilbert-base-uncased_xcnlg_radga.pkl', 'rb') as file: \n",
    "    block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = block.train.one_batch(4)\n",
    "bb = BatchEncoding({k:v for k,v in b.items() if isinstance(v, torch.Tensor)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a78059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbl2data_data2ptr :  torch.Size([4])\n",
      "lbl2data_data2ptr :  torch.Size([4])\n",
      "pcat2data_data2ptr :  torch.Size([4])\n",
      "cat2data_data2ptr :  torch.Size([4])\n",
      "pcat2lbl2data_data2ptr :  torch.Size([4])\n",
      "cat2lbl2data_data2ptr :  torch.Size([4])\n",
      "phlk2data_data2ptr :  torch.Size([4])\n",
      "hlk2data_data2ptr :  torch.Size([4])\n",
      "hlk2lbl2data_data2ptr :  torch.Size([4])\n",
      "hlk2lbl2data_plbl2data2ptr :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for k,v in bb.items():\n",
    "    if 'ptr' in k: print(k, ': ', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484462a0-4806-461a-957c-6ae5d1bd1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        for k,v in kwargs.items(): \n",
    "            if isinstance(v, torch.Tensor): print(k, ': ', v, ', ', v.device)\n",
    "        return kwargs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9a39d-bff3-4588-9fd4-2899208a6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "m = XCDataParallel(module=MyModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bff3f-e395-4486-9865-19d0f5bff763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plbl2data_data2ptrplbl2data_data2ptr :   :  tensor([2, 1], device='cuda:0')tensor([1, 2], device='cuda:1')  , ,   cuda:0cuda:1\n",
      "\n",
      "lbl2data_data2ptrlbl2data_data2ptr  : :   tensor([1, 2], device='cuda:1')tensor([2, 1], device='cuda:0') ,   , cuda:1 \n",
      "cuda:0pcat2data_data2ptr\n",
      " pcat2data_data2ptr:   : tensor([14,  6], device='cuda:1')  tensor([13,  6], device='cuda:0'),   cuda:1,  \n",
      "cat2data_data2ptrcuda:0\n",
      " : cat2data_data2ptr  :  tensor([1, 1], device='cuda:1') tensor([1, 1], device='cuda:0') , ,   cuda:0cuda:1\n",
      "\n",
      "pcat2lbl2data_data2ptrpcat2lbl2data_data2ptr :  :   tensor([4, 7], device='cuda:1')tensor([0, 4], device='cuda:0') ,   , cuda:0 \n",
      "cat2lbl2data_data2ptrcuda:1\n",
      " cat2lbl2data_data2ptr:   :  tensor([0, 1], device='cuda:0') tensor([1, 1], device='cuda:1'),   , cuda:0 \n",
      "cuda:1phlk2data_data2ptr\n",
      " phlk2data_data2ptr:   : tensor([16, 18], device='cuda:0')  , tensor([15, 40], device='cuda:1') cuda:0 \n",
      ", hlk2data_data2ptr  cuda:1: \n",
      "hlk2data_data2ptr  tensor([3, 3], device='cuda:0'):   , tensor([3, 3], device='cuda:1')  , cuda:0 \n",
      "cuda:1data_input_ids\n",
      " data_input_ids:   :  tensor([[ 101, 9808, 4270, 2314,  102,    0,    0],\n",
      "        [ 101, 4748, 3909, 4049,  102,    0,    0]], device='cuda:0') ,  cuda:0tensor([[  101,  2957, 14135,  1006,  2236,  1007,   102],\n",
      "        [  101,  9805,  3676,  2221,  1010,  2662,   102]], device='cuda:1')\n",
      " data_attention_mask,   : cuda:1\n",
      " data_attention_mask : tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0]], device='cuda:0')  ,  cuda:0tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:1')\n",
      "hlk2lbl2data_data2ptr  , :   cuda:1\n",
      "tensor([0, 5], device='cuda:0')hlk2lbl2data_data2ptr  : ,   cuda:0tensor([5, 9], device='cuda:1')\n",
      "cat2data_idx ,  :   cuda:1\n",
      "tensor([152298,  55068], device='cuda:0')cat2data_idx  , :   cuda:0\n",
      "cat2data_input_idstensor([ 54425, 104014], device='cuda:1')  : ,   cuda:1\n",
      "tensor([[  101,  5485,  1997, 18685,  2221,  1010,  5284,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2309,  1011, 21235,  5245,  2121,  2948,   102,     0,     0,\n",
      "             0,     0]], device='cuda:0')cat2data_input_ids  , :   cuda:0\n",
      "cat2data_attention_mask : tensor([[  101,  8055,  2163,  2510,  5073,  2730,  1999,  1996,  2137,  2942,\n",
      "          2162,   102],\n",
      "        [  101,  7973, 17228,  1999,  2662,   102,     0,     0,     0,     0,\n",
      "             0,     0]], device='cuda:1')  ,  cuda:1tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0') \n",
      ", cat2data_attention_mask  cuda:0: \n",
      " cat2lbl2data_idx tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:1'):   ,  tensor([273427], device='cuda:0')cuda:1 , \n",
      "cat2lbl2data_idx  cuda:0\n",
      ": cat2lbl2data_input_ids  tensor([490081, 104014], device='cuda:1'):   ,  tensor([[ 101, 7201, 1997, 2510, 2948,  102]], device='cuda:0') cuda:1\n",
      ", cat2lbl2data_input_ids cuda:0 : \n",
      "cat2lbl2data_attention_mask  : tensor([[  101,  7201,  1997, 11593,   102,     0],\n",
      "        [  101,  7973, 17228,  1999,  2662,   102]], device='cuda:1')  , tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')  cuda:1, \n",
      " cat2lbl2data_attention_maskcuda:0 \n",
      ":  pcat2lbl2data_idx : tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1]], device='cuda:1')  , tensor([498200, 496349, 273427,  92979], device='cuda:0')  , cuda:1 cuda:0\n",
      "\n",
      "pcat2lbl2data_idxlbl2data_idx  :  :  tensor([395355, 107556, 395354, 490081, 547792, 545725, 355196, 153765,  98839,\n",
      "        104014, 119469], device='cuda:1')tensor([101316,  71037,  99923], device='cuda:0') ,   cuda:0, \n",
      " lbl2data_input_idscuda:1\n",
      " lbl2data_idx:   : tensor([[ 101, 2862, 1997, 5284, 5485,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2862, 1997, 5111, 5485,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2862, 1997, 2948, 1997, 1996, 2548, 3987, 2250, 2326,  102,    0,\n",
      "            0,    0]], device='cuda:0')  , tensor([  269, 55151, 55150], device='cuda:1') cuda:0 , \n",
      "lbl2data_attention_mask  cuda:1\n",
      ":  lbl2data_input_ids :  tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], device='cuda:0') ,  tensor([[  101,  2862,  1997,  2137,  2942,  2162, 11593,  1006,  8055,  1007,\n",
      "           102,     0,     0,     0],\n",
      "        [  101,  9805,  3676,  2221,  1010,  2662,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182, 26213,  1999,  9805,  3676,\n",
      "          2221,  1010,  2662,   102]], device='cuda:1')cuda:0\n",
      " pcat2data_idx,   : cuda:1 \n",
      "lbl2data_attention_mask : tensor([ 64717, 159252, 152298, 202322, 242932, 225043, 183785, 191189, 183855,\n",
      "        218989, 242907, 228216, 188424,  54866,  56102,  55068,  55069,  56101,\n",
      "         56100], device='cuda:0')  ,  cuda:0tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')\n",
      " , plbl2data_idx  : cuda:1\n",
      " pcat2data_idxtensor([ 71037, 101316,  99923], device='cuda:0')  :  ,  tensor([ 68276, 106638, 102346, 102550, 367802, 207931,  66312, 133998, 171930,\n",
      "         62824,  54423,  54425, 108939, 281131, 153767, 104014,  98839, 355196,\n",
      "        119469, 153765], device='cuda:1')cuda:0 \n",
      "hlk2lbl2data_plbl2data2ptr,   : cuda:1 \n",
      "plbl2data_idx : tensor([0, 0, 5], device='cuda:0')  ,  tensor([  269, 55150, 55151], device='cuda:1')cuda:0\n",
      " , phlk2data_idx  cuda:1:  \n",
      "hlk2lbl2data_plbl2data2ptr :  tensor([ 113865,    8637,   48624, 1397758,  794132,   13068,     254,   24213,\n",
      "         808587,  310460,  846531,  997655, 1221759,  916835, 1210975,  345267,\n",
      "          77253,   25418,  467468,    2667,  145886,  179309,  467472,   52621,\n",
      "         467465,  467466,    3004,  467473,    2736,  467471,  467464,  467470,\n",
      "         433167,  467469], device='cuda:0') , tensor([5, 4, 5], device='cuda:1')  cuda:0\n",
      ",  hlk2data_idxcuda:1 : \n",
      " phlk2data_idx tensor([ 48624, 794132,    254, 179309, 467472, 433167], device='cuda:0'):   ,  cuda:0\n",
      "tensor([   1571,  241461,    8805,    2582,   89844,  460143,    9824,     297,\n",
      "        1283293,   19288,  567212,   40308,    1464, 1282940,   14059,  948811,\n",
      "          61257,  517280,    4927,   15729,  304807,  243045,  815271,  716531,\n",
      "           7839,    8646,  441558,   20720,  147120,    2932, 2337137,  817356,\n",
      "          46779,   15335,   15675,  484300,  815267,     254,  834837, 1914233,\n",
      "          38369,  948613,  847391, 2455908,  438723,     147,  511595,  561003,\n",
      "          14452,    2894,     317,  836235,   39074,  511775,  403957],\n",
      "       device='cuda:1')hlk2data_input_ids ,  :   cuda:1\n",
      "hlk2data_idx :  tensor([[  101,  9808,  4270,  3842,   102,     0,     0,     0,     0],\n",
      "        [  101, 15237,  8071,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2142,  2163,   102,     0,     0,     0,     0,     0],\n",
      "        [  101, 28667, 11514,  3217, 18252,  3194,   102,     0,     0],\n",
      "        [  101, 11409,  2669,  3246,   102,     0,     0,     0,     0],\n",
      "        [  101, 24185,  4877, 22352, 17947,   102,     0,     0,     0]],\n",
      "       device='cuda:0') tensor([  19288, 1283293,    2582,    7839,    2894,  511595], device='cuda:1'),   cuda:0\n",
      "hlk2data_attention_mask,   : cuda:1 \n",
      "hlk2data_input_ids :  tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0]], device='cuda:0') ,  cuda:0\n",
      "tensor([[  101,  2236,  3738,  1999,  1996,  8055,  2163,  2390,   102],\n",
      "        [  101,  5900,  2110, 10211,   102,     0,     0,     0,     0],\n",
      "        [  101,  9991, 15049,  2808,   102,     0,     0,     0,     0],\n",
      "        [  101,  3534,  2051,  4224,   102,     0,     0,     0,     0],\n",
      "        [  101,  3009,  2653,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  4956,  7778,  2181,   102,     0,     0,     0,     0]],\n",
      "       device='cuda:1')hlk2lbl2data_idx  , :   cuda:1tensor([  40156,  527144, 1560778,  552110,  757054], device='cuda:0') \n",
      ", hlk2data_attention_mask  : cuda:0 \n",
      "hlk2lbl2data_input_ids : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:1')  ,  cuda:1\n",
      "tensor([[  101,  2061, 28400,  8939, 20720, 13714,  2358, 22134,  3334,   102],\n",
      "        [  101,  2460,  2828, 19681,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  2632, 13959,  7464,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1054, 22394,  1011,  2465, 27636,   102,     0,     0,     0],\n",
      "        [  101, 20704,  3217, 23475,   102,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')hlk2lbl2data_idx ,  :   cuda:0\n",
      "tensor([  13617, 1644592,   12853,  656719,   42218,  948160,     254,    7140,\n",
      "           2932,   46779,  441558, 1914233, 2337137, 2455908], device='cuda:1')hlk2lbl2data_attention_mask  ,  :  cuda:1\n",
      "hlk2lbl2data_input_ids : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:0')  ,  cuda:0\n",
      "tensor([[  101,  2343,  1997,  1996,  8055,  2163,  1997,  2637,   102,     0],\n",
      "        [  101,  4557, 21863,  5420,  3044,   102,     0,     0,     0,     0],\n",
      "        [  101,  2390,  1997,  5900,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  5340, 20082,  3483,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  3448,  6627,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182, 26213,  1999,  2662,   102],\n",
      "        [  101,  2142,  2163,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2120,  4236,  1997,  3181,  3182,   102,     0,     0,     0],\n",
      "        [  101,  2662,   102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 11932,  3028,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 20287,  5063,  2653,   102,     0,     0,     0,     0,     0],\n",
      "        [  101, 25540,  2571, 26614,  2697,   102,     0,     0,     0,     0],\n",
      "        [  101,  9805,  3676,  1011, 10514, 12079,  6671,   102,     0,     0],\n",
      "        [  101,  9805,  3676,  2221,  3075,   102,     0,     0,     0,     0]],\n",
      "       device='cuda:1') ,  cuda:1\n",
      "hlk2lbl2data_attention_mask :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:1') ,  cuda:1\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "o = m(**bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49800f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all([torch.all(bb[k] == o[k].to('cpu')) for k in o.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60df7f-8004-49ec-8cce-eb36a285c8e2",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7b634-3ec5-4b5c-9d1f-75f32fa47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCEvalLoopOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    targ_idx: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    targ_ptr: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    gen_output: Optional[Dict]\n",
    "    repr_output: Optional[Dict]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "\n",
    "class XCPredictionOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    gen_output: Optional[Dict]\n",
    "    repr_output: Optional[Dict]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fb71-7eb7-43c6-96ba-71759aa2b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearningArguments(Seq2SeqTrainingArguments):\n",
    "\n",
    "    @delegates(Seq2SeqTrainingArguments.__init__)\n",
    "    def __init__(self, \n",
    "                 use_encoder_parallel:Optional[bool]=False,\n",
    "                 generation_length_penalty:Optional[float]=1.0,\n",
    "                 generation_eos_token:Optional[int]=102,\n",
    "                 generation_num_beams:Optional[int]=5,\n",
    "                 generation_max_info:Optional[int]=None,\n",
    "                 representation_accumulation_steps:Optional[int]=None,\n",
    "                 representation_attribute:Optional[str]='data_repr',\n",
    "                 representation_num_beams:Optional[int]=5,\n",
    "                 representation_search_type:Optional[str]='INDEX',\n",
    "                 index_space:Optional[str]='cosine', \n",
    "                 index_efc:Optional[int]=300, \n",
    "                 index_m:Optional[int]=100, \n",
    "                 index_efs:Optional[int]=300,\n",
    "                 index_num_threads:Optional[int]=84,\n",
    "                 predict_with_generation:Optional[bool]=False,\n",
    "                 predict_with_representation:Optional[bool]=False,\n",
    "                 output_concatenation_weight:Optional[float]=1.0,\n",
    "                 group_by_cluster:Optional[bool]=False,\n",
    "                 num_clustering_warmup_epochs:Optional[int]=None,\n",
    "                 num_cluster_update_epochs:Optional[int]=1,\n",
    "                 num_cluster_size_update_epochs:Optional[int]=1,\n",
    "                 clustering_type:Optional[str]='EXPO',\n",
    "                 minimum_clusters:Optional[int]=3,\n",
    "                 maximum_clusters:Optional[int]=None,\n",
    "                 minimum_cluster_size:Optional[int]=1,\n",
    "                 maximum_cluster_size:Optional[int]=None,\n",
    "                 clustering_devices:Optional[List]=None,\n",
    "                 target_indices_key:Optional[str]='lbl2data_idx',\n",
    "                 target_pointer_key:Optional[str]='lbl2data_data2ptr',\n",
    "                 data_aug_meta_name:Optional[str]=None,\n",
    "                 augmentation_num_beams:Optional[int]=3,\n",
    "                 predict_with_augmentation:Optional[bool]=False,\n",
    "                 use_augmentation_index_representation:Optional[bool]=False,\n",
    "                 metadata_representation_attribute:Optional[str]='data_repr',\n",
    "                 data_augmentation_attribute:Optional[str]='data_repr',\n",
    "                 use_distributional_representation:Optional[bool]=False,\n",
    "                 use_label_metadata:Optional[bool]=True,\n",
    "                 prune_metadata:Optional[bool]=False,\n",
    "                 num_metadata_prune_epochs:Optional[int]=1,\n",
    "                 metadata_prune_batch_size:Optional[int]=64,\n",
    "                 num_metadata_prune_warmup_epochs:Optional[int]=10,\n",
    "                 prune_metadata_names:Optional[List]=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('generation_num_beams,generation_length_penalty,generation_max_info,generation_eos_token')\n",
    "        store_attr('representation_accumulation_steps,representation_attribute,representation_num_beams,representation_search_type')\n",
    "        store_attr('index_space,index_efc,index_m,index_efs,index_num_threads')\n",
    "        store_attr('predict_with_generation,predict_with_representation,output_concatenation_weight')\n",
    "        store_attr('group_by_cluster,num_cluster_update_epochs,num_cluster_size_update_epochs,num_clustering_warmup_epochs')\n",
    "        store_attr('clustering_devices,clustering_type,maximum_cluster_size')\n",
    "        store_attr('target_indices_key,target_pointer_key')\n",
    "        store_attr('use_encoder_parallel')\n",
    "        store_attr('data_aug_meta_name,augmentation_num_beams,predict_with_augmentation')\n",
    "        store_attr('use_augmentation_index_representation,metadata_representation_attribute,data_augmentation_attribute')\n",
    "        store_attr('use_distributional_representation')\n",
    "        store_attr('use_label_metadata')\n",
    "        store_attr('prune_metadata,num_metadata_prune_epochs,num_metadata_prune_warmup_epochs,metadata_prune_batch_size,prune_metadata_names')\n",
    "        self.minimum_clusters = max(1, minimum_clusters)\n",
    "        self.maximum_clusters = max(minimum_clusters, maximum_clusters) if maximum_clusters is not None else minimum_clusters\n",
    "        self.minimum_cluster_size = max(1, minimum_cluster_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b036146-ad70-4c47-802c-763f1f2aaf45",
   "metadata": {},
   "source": [
    "### `XCLearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b4bd-cd24-4b96-af56-474dfcbdcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearner(Seq2SeqTrainer):\n",
    "\n",
    "    @delegates(Seq2SeqTrainer.__init__)\n",
    "    def __init__(self, \n",
    "                 trie:Optional[Trie]=None, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tbs = TrieBeamSearch(trie, self.args.generation_eos_token, n_bm=self.args.generation_num_beams, \n",
    "                                  len_penalty=self.args.generation_length_penalty, max_info=self.args.generation_max_info, **kwargs)\n",
    "        self.idxs = (\n",
    "            BruteForceSearch(n_bm=self.args.representation_num_beams)\n",
    "            if self.args.representation_search_type == 'BRUTEFORCE' else\n",
    "            IndexSearch(space=self.args.index_space, efc=self.args.index_efc, m=self.args.index_m, \n",
    "                        efs=self.args.index_efs, n_bm=self.args.representation_num_beams, \n",
    "                        n_threads=self.args.index_num_threads) \n",
    "        )\n",
    "        self.aug_idxs, self.aug_info = None, None \n",
    "        self.aug_pad = PadFeatTfm(pad_tok=self.model.config.pad_token_id, prefix=\"meta\")\n",
    "\n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        if unwrap_model(model) is not model:\n",
    "            return model\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            if (hasattr(model, 'encoder') and isinstance(model.encoder, nn.DataParallel)) or self.args.use_encoder_parallel: return model\n",
    "            else: return XCDataParallel(module=model)\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, eval_dataset:Optional[Dataset]=None, ignore_keys:Optional[List[str]]=None, \n",
    "             metric_key_prefix:str=\"eval\", **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "        if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
    "            gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
    "            \n",
    "        self.gather_function, self._gen_kwargs  = self.accelerator.gather, gen_kwargs\n",
    "        \n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "\n",
    "    def predict(self, test_dataset: Dataset, ignore_keys:Optional[List[str]]=None, \n",
    "            metric_key_prefix:str=\"test\", **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        if gen_kwargs.get(\"gen_num_beams\") is None and self.args.generation_num_beams is not None:\n",
    "            gen_kwargs[\"gen_num_beams\"] = self.args.generation_num_beams\n",
    "        if gen_kwargs.get(\"repr_num_beams\") is None and self.args.representation_num_beams is not None:\n",
    "            gen_kwargs[\"repr_num_beams\"] = self.args.representation_num_beams\n",
    "        if gen_kwargs.get(\"aug_num_beams\") is None and self.args.augmentation_num_beams is not None:\n",
    "            gen_kwargs[\"aug_num_beams\"] = self.args.augmentation_num_beams\n",
    "    \n",
    "        self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
    "        self._memory_tracker.start()\n",
    "    \n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "        start_time = time.time()\n",
    "    \n",
    "        output = self.evaluation_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(metric_key_prefix,start_time,num_samples=output.num_samples,num_steps=math.ceil(output.num_samples / total_batch_size),)\n",
    "        )\n",
    "        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "        return XCPredictionOutput(pred_idx=output.pred_idx, pred_ptr=output.pred_ptr, pred_score=output.pred_score, \n",
    "                              gen_output=output.gen_output, repr_output=output.repr_output, metrics=output.metrics, \n",
    "                              num_samples=output.num_samples)\n",
    "    \n",
    "    def _gather_host_output(self, output, host_output):\n",
    "        if output is not None:\n",
    "            output = self.accelerator.pad_across_processes(output, dim=1, pad_index=-100)\n",
    "            output = self.gather_function((output))\n",
    "            return output if host_output is None else nested_concat(host_output, output, padding_index=-100)\n",
    "        else: return host_output\n",
    "\n",
    "    def _gather_all_output(self, host_output, all_output, to_cpu=True):\n",
    "        if host_output is not None:\n",
    "            if isinstance(host_output, torch.Tensor) and to_cpu: host_output = host_output.cpu()\n",
    "            return host_output if all_output is None else nested_concat(all_output, host_output, padding_index=-100)\n",
    "        else: return all_output\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc228b-c71e-4f8f-a51b-3e495fc5337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_aug_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
    "    dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
    "    dataset = dataset if self.train_dataset is None else self.train_dataset\n",
    "    \n",
    "    aug_meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
    "    if (\n",
    "        dataset is not None and dataset.meta is not None and aug_meta_name is not None and \n",
    "        aug_meta_name in dataset.meta\n",
    "    ):\n",
    "        self.aug_idxs = IndexSearch(space=self.args.index_space, efc=self.args.index_efc, m=self.args.index_m, \n",
    "                                    efs=self.args.index_efs, n_bm=self.args.representation_num_beams, \n",
    "                                    n_threads=self.args.index_num_threads)\n",
    "        \n",
    "        self.aug_info = getattr(dataset.meta[aug_meta_name], 'meta_info')\n",
    "        \n",
    "        aug_dset = MainXCDataset(self.aug_info)\n",
    "        aug_dl = self.get_test_dataloader(aug_dset)\n",
    "        aug_repr = self.get_meta_representation(aug_dl, to_cpu=isinstance(self.aug_idxs, IndexSearch))\n",
    "        self.aug_idxs.build(aug_repr)\n",
    "\n",
    "@patch\n",
    "def _build_lbl_index(self:XCLearner, dataset:Optional[Dataset]=None):\n",
    "    dataset = dataset if self.eval_dataset is None else self.eval_dataset\n",
    "    dataset = dataset if self.train_dataset is None else self.train_dataset\n",
    "    \n",
    "    if dataset is not None:\n",
    "        lbl_dset = dataset.lbl_dset\n",
    "        \n",
    "        meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
    "        if meta_name is not None and dataset.meta is not None and meta_name in dataset.meta and self.args.use_label_metadata:\n",
    "            prefix,lbl_meta,meta_info  = dataset.meta[meta_name].prefix,dataset.meta[meta_name].lbl_meta,dataset.meta[meta_name].meta_info\n",
    "            meta_kwargs = {meta_name: MetaXCDataset(prefix, lbl_meta, lbl_meta, meta_info, n_data_meta_samples=self.args.augmentation_num_beams)}\n",
    "            lbl_dset = XCDataset(lbl_dset, **meta_kwargs)\n",
    "        \n",
    "        lbl_dl = self.get_test_dataloader(lbl_dset)\n",
    "        lbl_repr = self.get_representation(lbl_dl, to_cpu=isinstance(self.idxs, IndexSearch))\n",
    "        \n",
    "        self.idxs.build(lbl_repr)\n",
    "    else: raise ValueError('Failed to build `self.idxs`')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f8f9-c2c1-4391-8ec9-d6ea44346107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def generation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"gen_num_beams\") if \"gen_num_beams\" in kwargs and kwargs[\"gen_num_beams\"] is not None else self.args.generation_num_beams\n",
    "    len_penalty = kwargs.pop(\"length_penalty\") if \"length_penalty\" in kwargs and kwargs[\"length_penalty\"] is not None else self.args.generation_length_penalty\n",
    "    \n",
    "    with torch.no_grad(): o = self.tbs.proc(self.model, inputs.copy(), n_bm=n_bm, len_penalty=len_penalty)\n",
    "        \n",
    "    return {'pred_idx':o['info2seq2data_idx'], 'pred_score':o['info2seq2data_score'], 'pred_ptr':o['info2seq2data_data2ptr']}\n",
    "\n",
    "@patch\n",
    "def representation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"repr_num_beams\") if \"repr_num_beams\" in kwargs and kwargs[\"repr_num_beams\"] is not None else self.args.representation_num_beams\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        o = getattr(model(**inputs), self.args.representation_attribute)\n",
    "        if self.args.use_distributional_representation: o = o.exp()\n",
    "            \n",
    "    o = self.idxs.proc(o, n_bm=n_bm)\n",
    "        \n",
    "    return {'pred_idx':o['info2data_idx'], 'pred_score':o['info2data_score'], 'pred_ptr':o['info2data_data2ptr']}\n",
    "\n",
    "@patch\n",
    "def augmentation_output(\n",
    "    self:XCLearner,\n",
    "    model:nn.Module,\n",
    "    inputs:Dict[str, Union[torch.Tensor, Any]],\n",
    "    **kwargs\n",
    "):\n",
    "    if self.aug_idxs is None: raise ValueError('Augmentation `aug_idx` is not initialized.')\n",
    "        \n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = kwargs.pop(\"aug_num_beams\") if \"aug_num_beams\" in kwargs and kwargs[\"aug_num_beams\"] is not None else self.args.augmentation_num_beams\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        o = getattr(model(**{'data_input_ids':inputs['data_input_ids'], 'data_attention_mask':inputs['data_attention_mask']}), self.args.data_augmentation_attribute)\n",
    "        if self.args.use_distributional_representation: o = o.exp()\n",
    "            \n",
    "    o = self.aug_idxs.proc(o, n_bm=n_bm)\n",
    "    \n",
    "    aug_info = self.aug_pad({\n",
    "        'meta_input_ids':[self.aug_info['input_ids'][i] for i in o['info2data_idx']], \n",
    "        'meta_attention_mask':[self.aug_info['input_ids'][i] for i in o['info2data_idx']]\n",
    "    })\n",
    "    \n",
    "    if self.args.use_augmentation_index_representation:\n",
    "        meta_repr = torch.tensor(self.aug_idxs.index.get_items(o['info2data_idx']))\n",
    "        return {\n",
    "            f'{self.args.data_aug_meta_name}2data_idx':o['info2data_idx'], \n",
    "            f'{self.args.data_aug_meta_name}2data_meta_repr': meta_repr,\n",
    "            f'{self.args.data_aug_meta_name}2data_attention_mask': aug_info['meta_attention_mask'],\n",
    "            f'{self.args.data_aug_meta_name}2data_data2ptr': o['info2data_data2ptr'],\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            f'{self.args.data_aug_meta_name}2data_idx':o['info2data_idx'], \n",
    "            f'{self.args.data_aug_meta_name}2data_input_ids': aug_info['meta_input_ids'], \n",
    "            f'{self.args.data_aug_meta_name}2data_attention_mask': aug_info['meta_attention_mask'],\n",
    "            f'{self.args.data_aug_meta_name}2data_data2ptr': o['info2data_data2ptr']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967d99a-b531-4ff0-83a7-6808c0a8ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _perform_generation(self:XCLearner, model:nn.Module, predict_with_generation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_generation = self.args.predict_with_generation if predict_with_generation is None else predict_with_generation\n",
    "    return getattr(model,'use_generation') if hasattr(model,'use_generation') else predict_with_generation\n",
    "\n",
    "@patch\n",
    "def _perform_representation(self:XCLearner, model:nn.Module, predict_with_representation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_representation = self.args.predict_with_representation if predict_with_representation is None else predict_with_representation\n",
    "    return getattr(model,'use_representation') if hasattr(model,'use_representation') else predict_with_representation\n",
    "\n",
    "@patch\n",
    "def _perform_augmentation(self:XCLearner, model:nn.Module, predict_with_augmentation:Optional[bool]=None):\n",
    "    model = unwrap_model(model)\n",
    "    predict_with_augmentation = self.args.predict_with_augmentation if predict_with_augmentation is None else predict_with_augmentation\n",
    "    return getattr(model,'use_augmentation') if hasattr(model,'use_augmentation') else predict_with_augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351dde7-b678-41d5-b710-005cb7db6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def resize_pred(cls:XCLearner, t, n_t):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    r_t = torch.ones((len(t),), dtype=xn_t.dtype, device=xn_t.device).scatter(0, t_ptr, xn_t)\n",
    "    xt = t.repeat_interleave(r_t).view(len(n_t), -1)\n",
    "    return xt\n",
    "\n",
    "@patch\n",
    "def output_mask(cls:XCLearner, n_t, l):\n",
    "    max_n_t = n_t.max()\n",
    "    xn_t = max_n_t.max()-n_t+1\n",
    "    t_ptr = n_t.cumsum(dim=0)-1\n",
    "    mask_ptr = t_ptr+torch.arange(len(t_ptr), device=t_ptr.device)+1\n",
    "    mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype, device=mask_ptr.device).scatter(0, mask_ptr, 0)\n",
    "    r_mask = torch.ones((l+len(n_t),), dtype=mask_ptr.dtype, device=mask_ptr.device).scatter(0, mask_ptr, xn_t-1)\n",
    "    mask = mask.repeat_interleave(r_mask).view(len(n_t), -1)\n",
    "    return mask\n",
    "\n",
    "@patch\n",
    "def resize_output(cls:XCLearner, pred_idx, pred_score, pred_ptr):\n",
    "    return cls.resize_pred(pred_idx, pred_ptr), cls.resize_pred(pred_score, pred_ptr), cls.output_mask(pred_ptr, len(pred_idx)), pred_ptr\n",
    "\n",
    "@patch\n",
    "def concatenate_output(cls:XCLearner, gen_o:Dict, repr_o:Dict):\n",
    "    gen_o['pred_score'] = torch.exp(gen_o['pred_score'])*cls.args.output_concatenation_weight\n",
    "    gen_o, repr_o = cls.resize_output(**gen_o), cls.resize_output(**repr_o)\n",
    "    pred_idx, pred_score, mask = [torch.hstack([gen_o[i], repr_o[i].cpu()]).flatten() for i in range(3)]\n",
    "    idx = torch.where(mask)[0]\n",
    "    return {\n",
    "        'pred_idx': pred_idx[idx],\n",
    "        'pred_score': pred_score[idx],\n",
    "        'pred_ptr': gen_o[3]+repr_o[3].cpu(),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e5ae1-6971-416b-9b11-1e6234632c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prediction_step(\n",
    "    self:XCLearner,\n",
    "    model: nn.Module,\n",
    "    inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "    prediction_loss_only: bool,\n",
    "    predict_with_generation: bool,\n",
    "    predict_with_representation: bool,\n",
    "    predict_with_augmentation:Optional[bool]=None,\n",
    "    ignore_keys: Optional[List[str]] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "    with torch.no_grad():\n",
    "        with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
    "        loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "    prediction_loss_only = self.args.prediction_loss_only if prediction_loss_only is None else prediction_loss_only\n",
    "    if prediction_loss_only: return loss, {}\n",
    "    \n",
    "    if self._perform_augmentation(model, predict_with_augmentation): \n",
    "        aug_inputs = self.augmentation_output(model, inputs, **kwargs)\n",
    "        inputs.update(aug_inputs)\n",
    "        \n",
    "    output, gen_o, repr_o = None, None, None\n",
    "    if self._perform_generation(model, predict_with_generation): gen_o = self.generation_output(model, inputs, **kwargs)\n",
    "    if self._perform_representation(model, predict_with_representation): repr_o = self.representation_output(model, inputs, **kwargs)\n",
    "    \n",
    "    if gen_o is not None and repr_o is not None:\n",
    "        output = {f'{k}_gen':v for k,v in gen_o.items()}\n",
    "        output.update({f'{k}_repr':v for k,v in repr_o.items()})\n",
    "        output.update(self.concatenate_output(gen_o, repr_o))\n",
    "    else:\n",
    "        output = gen_o if repr_o is None else repr_o\n",
    "        \n",
    "    labels = {'targ_idx':inputs[self.args.target_indices_key], 'targ_ptr':inputs[self.args.target_pointer_key]} if self.args.target_indices_key in inputs else None\n",
    "    if labels is not None: output.update(labels)\n",
    "    \n",
    "    return loss, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189c5d7-a677-43e6-a78d-7485c07d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluation_loop(\n",
    "    self:XCLearner,\n",
    "    dataloader:DataLoader,\n",
    "    description:str,\n",
    "    prediction_loss_only:Optional[bool] = None,\n",
    "    predict_with_generation:Optional[bool]=None,\n",
    "    predict_with_representation:Optional[bool]=None,\n",
    "    ignore_keys:Optional[List[str]] = None,\n",
    "    metric_key_prefix:str=\"eval\",\n",
    ") -> XCEvalLoopOutput:\n",
    "    args = self.args\n",
    "    prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "    \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        use_noise = self.model.disable_noise()\n",
    "\n",
    "    model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "    if len(self.accelerator._models) == 0 and model is self.model:\n",
    "        model = self.accelerator.prepare(model) if self.is_deepspeed_enabled else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "        if self.is_fsdp_enabled: self.model = model\n",
    "        if model is not self.model: self.model_wrapped = model\n",
    "        if self.is_deepspeed_enabled: self.deepspeed = self.model_wrapped\n",
    "\n",
    "    batch_size = self.args.eval_batch_size\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    self.callback_handler.eval_dataloader = dataloader\n",
    "    eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "    \n",
    "    if self._perform_representation(unwrap_model(model)) and not prediction_loss_only: \n",
    "        self._build_lbl_index(eval_dataset)\n",
    "            \n",
    "    if self._perform_augmentation(unwrap_model(model)) and not prediction_loss_only: \n",
    "        self._build_aug_index(eval_dataset)\n",
    "    \n",
    "    if args.past_index >= 0: self._past = None\n",
    "\n",
    "    losses_host, all_losses = None, None\n",
    "    host_output, all_output = {}, {}\n",
    "    \n",
    "    observed_num_examples = 0\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        observed_batch_size = find_batch_size(inputs)\n",
    "        if observed_batch_size is not None:\n",
    "            observed_num_examples += observed_batch_size\n",
    "            if batch_size is None: batch_size = observed_batch_size\n",
    "                \n",
    "        loss, output = self.prediction_step(model, inputs, prediction_loss_only, predict_with_generation, predict_with_representation, ignore_keys=ignore_keys)\n",
    "        \n",
    "        if loss is not None:\n",
    "            losses = self.gather_function((loss.repeat(batch_size)))\n",
    "            losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
    "        for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
    "            \n",
    "        self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "        \n",
    "        if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "            if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "            for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "    \n",
    "    self.gather_function = self.accelerator.gather_for_metrics\n",
    "    if args.past_index and hasattr(self, \"_past\"): delattr(self, \"_past\")\n",
    "\n",
    "    if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "    for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "        \n",
    "    if has_length(eval_dataset): num_samples = len(eval_dataset)\n",
    "    elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "        num_samples = eval_dataset.num_examples\n",
    "    else:\n",
    "        if has_length(dataloader): num_samples = self.num_examples(dataloader)\n",
    "        else: num_samples = observed_num_examples\n",
    "    if num_samples == 0 and observed_num_examples > 0: num_samples = observed_num_examples\n",
    "        \n",
    "    gen_output, repr_output = None, None\n",
    "    metric_input_keys = ['targ_idx', 'targ_ptr', 'pred_idx', 'pred_ptr', 'pred_score']\n",
    "    if 'pred_idx_gen' in all_output and all_output['pred_idx_gen'] is not None:\n",
    "        gen_output = {o:all_output[f'{o}_gen' if o.startswith('pred_') else o] for o in metric_input_keys}\n",
    "    if 'pred_idx_repr' in all_output and all_output['pred_idx_repr'] is not None:\n",
    "        repr_output = {o:all_output[f'{o}_repr' if o.startswith('pred_') else o] for o in metric_input_keys}\n",
    "    \n",
    "\n",
    "    if (self.compute_metrics is not None and \n",
    "        'targ_idx' in all_output and all_output['targ_idx'] is not None and \n",
    "        'pred_idx' in all_output and all_output['pred_idx'] is not None):\n",
    "        \n",
    "        metrics = self.compute_metrics(**{o:all_output[o] for o in metric_input_keys})\n",
    "        if gen_output is not None:\n",
    "            m = self.compute_metrics(**gen_output)\n",
    "            metrics.update({f'{k}_GEN':v for k,v in m.items()})\n",
    "        if repr_output is not None:\n",
    "            m = self.compute_metrics(**repr_output)\n",
    "            metrics.update({f'{k}_REPR':v for k,v in m.items()})      \n",
    "    else: metrics = {}\n",
    "        \n",
    "    metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "    if all_losses is not None: metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "    if hasattr(self, \"jit_compilation_time\"): metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
    "        \n",
    "    for key in list(metrics.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"): metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "        \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        self.model.set_noise(use_noise)\n",
    "    \n",
    "    return XCEvalLoopOutput(pred_idx=all_output.get('pred_idx'), pred_ptr=all_output.get('pred_ptr'), \n",
    "                            pred_score=all_output.get('pred_score'),targ_idx=all_output.get('targ_idx'), \n",
    "                            targ_ptr=all_output.get('targ_ptr'), gen_output=gen_output, repr_output=repr_output,\n",
    "                            metrics=metrics, num_samples=num_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d626f2e-22fa-4b44-82d9-af7d082cb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_meta_representation(self:XCLearner, dataloader: DataLoader, to_cpu:Optional[bool]=True):\n",
    "    data_host, all_data = None, None\n",
    "    \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        use_noise = self.model.disable_noise()\n",
    "    \n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad(): data = getattr(self.model.get_meta_representation(**inputs), self.args.metadata_representation_attribute)\n",
    "        data_host = self._gather_host_output(data, data_host)\n",
    "        if self.args.representation_accumulation_steps is not None and (step + 1) % self.args.representation_accumulation_steps == 0:\n",
    "            all_data, data_host = self._gather_all_output(data_host, all_data, to_cpu=to_cpu), None\n",
    "            \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        self.model.set_noise(use_noise)\n",
    "            \n",
    "    return self._gather_all_output(data_host, all_data, to_cpu=to_cpu)\n",
    "\n",
    "@patch\n",
    "def get_representation(self:XCLearner, dataloader: DataLoader, to_cpu:Optional[bool]=True):\n",
    "    data_host, all_data = None, None\n",
    "    \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        use_noise = self.model.disable_noise()\n",
    "    \n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        with torch.no_grad(): data = getattr(self.model(**inputs), self.args.representation_attribute)\n",
    "        data_host = self._gather_host_output(data, data_host)\n",
    "        if self.args.representation_accumulation_steps is not None and (step + 1) % self.args.representation_accumulation_steps == 0:\n",
    "            all_data, data_host = self._gather_all_output(data_host, all_data, to_cpu=to_cpu), None\n",
    "            \n",
    "    if hasattr(self.model, 'disable_noise') and callable(getattr(self.model, 'disable_noise')):\n",
    "        self.model.set_noise(use_noise)\n",
    "            \n",
    "    return self._gather_all_output(data_host, all_data, to_cpu=to_cpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ec06d-459a-4ba0-8b1d-74ff1d84a4ff",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b71a7-4ec2-47c5-a72d-77001b9c9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_train_sampler(self:XCLearner):\n",
    "    if self.train_dataset is None or not has_length(self.train_dataset):\n",
    "        return None\n",
    "        \n",
    "    if self.args.group_by_length:\n",
    "        if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\n",
    "            lengths = (\n",
    "                self.train_dataset[self.args.length_column_name]\n",
    "                if self.args.length_column_name in self.train_dataset.column_names\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            lengths = None\n",
    "        model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n",
    "        return LengthGroupedSampler(\n",
    "            self.args.train_batch_size * self.args.gradient_accumulation_steps,\n",
    "            dataset=self.train_dataset,\n",
    "            lengths=lengths,\n",
    "            model_input_name=model_input_name,\n",
    "        )\n",
    "\n",
    "    elif self.args.group_by_cluster:\n",
    "        return ClusterGroupedSampler(n=len(self.train_dataset))\n",
    "    else:\n",
    "        return RandomSampler(self.train_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cb389-f267-44a3-a65e-39b517b86660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_train_dataloader(self:XCLearner):\n",
    "    if self.train_dataset is None:\n",
    "        raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "    train_dataset = self.train_dataset\n",
    "    data_collator = self.data_collator\n",
    "    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "    else:\n",
    "        data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "    dataloader_params = {\n",
    "        \"batch_size\": self._train_batch_size,\n",
    "        \"collate_fn\": data_collator,\n",
    "        \"num_workers\": self.args.dataloader_num_workers,\n",
    "        \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "    }\n",
    "\n",
    "    if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "        dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "        dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "        dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "        dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "    \n",
    "    return DataLoader(train_dataset, **dataloader_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_min_cluster_sz(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    \n",
    "    if self.args.num_clustering_warmup_epochs is not None:\n",
    "        if epochs_trained < self.args.num_clustering_warmup_epochs: return None\n",
    "        else: epochs_trained -= self.args.num_clustering_warmup_epochs\n",
    "    \n",
    "    if self.args.clustering_type == 'LINEAR':\n",
    "        if self.args.maximum_clusters is None: return self.train_dataset.n_data//self.args.minimum_clusters\n",
    "        else:\n",
    "            n_cluster = (self.args.maximum_clusters-self.args.minimum_clusters)/num_train_epochs*epochs_trained\n",
    "            return self.train_dataset.n_data//int(self.args.minimum_clusters+n_cluster)\n",
    "        \n",
    "    elif self.args.clustering_type == 'EXPO':\n",
    "        mult = 2**(epochs_trained//self.args.num_cluster_size_update_epochs)\n",
    "        cluster_sz = self.args.minimum_cluster_size*mult\n",
    "        cluster_sz = (\n",
    "            self.args.maximum_cluster_size \n",
    "            if self.args.maximum_cluster_size is not None and cluster_sz > self.args.maximum_cluster_size \n",
    "            else cluster_sz\n",
    "        )\n",
    "        return cluster_sz\n",
    "    \n",
    "    else: raise ValueError(f'Invalid `clustering_type`({self.args.clustering_type}).')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447567d1-8fc1-4582-9673-34ff1c971048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_train_data_cluster(self:XCLearner, epochs_trained:int, num_train_epochs:int):\n",
    "    dataset = self.train_dataset\n",
    "    data_dset = dataset.data_dset\n",
    "    \n",
    "    meta_name = f'{self.args.data_aug_meta_name}_meta' if self.args.data_aug_meta_name is not None else None\n",
    "    if meta_name is not None and dataset.meta is not None and meta_name in dataset.meta:\n",
    "        prefix,data_meta,meta_info  = dataset.meta[meta_name].prefix,dataset.meta[meta_name].data_meta,dataset.meta[meta_name].meta_info\n",
    "        meta_kwargs = {meta_name: MetaXCDataset(prefix, data_meta, data_meta, meta_info, n_data_meta_samples=self.args.augmentation_num_beams)}\n",
    "        data_dset = XCDataset(data_dset, **meta_kwargs)\n",
    "            \n",
    "    dataloader = self.get_test_dataloader(data_dset)\n",
    "    data_repr = self.get_representation(dataloader)\n",
    "    \n",
    "    if self.args.use_distributional_representation: data_repr = data_repr.exp()\n",
    "        \n",
    "    cluster = BalancedClusters.proc(data_repr, self._get_min_cluster_sz(epochs_trained, num_train_epochs), clustering_devices=self.args.clustering_devices)\n",
    "    return cluster\n",
    "\n",
    "@patch\n",
    "def update_dataloader_sampler(self:XCLearner, dataloader:DataLoader, epochs_trained:int, num_train_epochs:int):\n",
    "    if isinstance(dataloader.sampler, ClusterGroupedSampler):\n",
    "        cluster = self._get_train_data_cluster(epochs_trained, num_train_epochs)\n",
    "        dataloader.sampler.set_cluster(cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca40d6-b489-4225-ac4e-57369e4599be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prune_metadata(self:XCLearner):\n",
    "    if self.train_dataset.meta is None: return\n",
    "        \n",
    "    data_dset = self.train_dataset.data_dset\n",
    "    dataloader = self.get_test_dataloader(data_dset)\n",
    "    data_repr = self.get_representation(dataloader)\n",
    "\n",
    "    lbl_dset = self.train_dataset.lbl_dset\n",
    "    dataloader = self.get_test_dataloader(lbl_dset)\n",
    "    lbl_repr = self.get_representation(dataloader)\n",
    "\n",
    "    prune_metadata_names = list(self.train_dataset.meta.keys()) if self.args.prune_metadata_names is None else self.args.prune_metadata_names\n",
    "    for m in self.args.prune_metadata_names:\n",
    "        if m not in self.train_dataset.meta: raise ValueError(f'Invalid metadata name: {m}')\n",
    "            \n",
    "        meta_dset = self.train_dataset.meta[m]\n",
    "        dataloader = self.get_test_dataloader(MainXCDataset(meta_dset.meta_info))\n",
    "        meta_repr = self.get_representation(dataloader)\n",
    "\n",
    "        meta_dset.prune_data_meta(data_repr, meta_repr, batch_size=self.args.metadata_prune_batch_size)\n",
    "        meta_dset.prune_lbl_meta(lbl_repr, meta_repr, batch_size=self.args.metadata_prune_batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33907726-5248-4c73-84f1-ce900f119ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _validate_group_by_cluster(self:XCLearner):\n",
    "    if self.args.group_by_cluster and (not hasattr(self.model,'use_representation') or  not getattr(unwrap_model(self.model),'use_representation')):\n",
    "        raise ValueError('Cannot use `group_by_cluster` for models without `use_representation`.')\n",
    "        self.args.group_by_cluster = False\n",
    "\n",
    "@patch\n",
    "def _inner_training_loop(\n",
    "    self:XCLearner, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "):\n",
    "    self.accelerator.free_memory()\n",
    "    self._train_batch_size = batch_size\n",
    "    if self.args.auto_find_batch_size:\n",
    "        if self.state.train_batch_size != self._train_batch_size:\n",
    "            from accelerate.utils import release_memory\n",
    "\n",
    "            (self.model_wrapped,) = release_memory(self.model_wrapped)\n",
    "            self.model_wrapped = self.model\n",
    "\n",
    "            # Check for DeepSpeed *after* the intial pass and modify the config\n",
    "            if self.is_deepspeed_enabled:\n",
    "                # Temporarily unset `self.args.train_batch_size`\n",
    "                original_bs = self.args.per_device_train_batch_size\n",
    "                self.args.per_device_train_batch_size = self._train_batch_size // max(1, self.args.n_gpu)\n",
    "                self.propagate_args_to_deepspeed(True)\n",
    "                self.args.per_device_train_batch_size = original_bs\n",
    "        self.state.train_batch_size = self._train_batch_size\n",
    "    logger.debug(f\"Currently training with a batch size of: {self._train_batch_size}\")\n",
    "    \n",
    "    # Data loader and number of training steps\n",
    "    self._validate_group_by_cluster()\n",
    "    train_dataloader = self.get_train_dataloader()\n",
    "    \n",
    "    if self.is_fsdp_xla_v2_enabled:\n",
    "        train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
    "\n",
    "    # Setting up training control variables:\n",
    "    # number of training epochs: num_train_epochs\n",
    "    # number of training steps per epoch: num_update_steps_per_epoch\n",
    "    # total number of training steps to execute: max_steps\n",
    "    total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "\n",
    "    len_dataloader = None\n",
    "    num_train_tokens = None\n",
    "    if has_length(train_dataloader):\n",
    "        len_dataloader = len(train_dataloader)\n",
    "        num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
    "        num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "        num_examples = self.num_examples(train_dataloader)\n",
    "        if args.max_steps > 0:\n",
    "            max_steps = args.max_steps\n",
    "            num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "                args.max_steps % num_update_steps_per_epoch > 0\n",
    "            )\n",
    "            # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "            # the best we can do.\n",
    "            num_train_samples = args.max_steps * total_train_batch_size\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = (\n",
    "                    self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "                )\n",
    "        else:\n",
    "            max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "            num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "            num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "            if args.include_tokens_per_second:\n",
    "                num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n",
    "    elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
    "        max_steps = args.max_steps\n",
    "        # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "        num_train_epochs = sys.maxsize\n",
    "        num_update_steps_per_epoch = max_steps\n",
    "        num_examples = total_train_batch_size * args.max_steps\n",
    "        num_train_samples = args.max_steps * total_train_batch_size\n",
    "        if args.include_tokens_per_second:\n",
    "            num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n",
    "            f\" {args.max_steps}\"\n",
    "        )\n",
    "\n",
    "    if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
    "        if self.args.n_gpu > 1:\n",
    "            # nn.DataParallel(model) replicates the model, creating new variables and module\n",
    "            # references registered here no longer work on other gpus, breaking the module\n",
    "            raise ValueError(\n",
    "                \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n",
    "                \" (torchrun or torch.distributed.launch (deprecated)).\"\n",
    "            )\n",
    "        else:\n",
    "            debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
    "\n",
    "    delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n",
    "\n",
    "    # We need to reset the scheduler, as its parameters may be different on subsequent calls\n",
    "    if self._created_lr_scheduler:\n",
    "        self.lr_scheduler = None\n",
    "        self._created_lr_scheduler = False\n",
    "\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
    "\n",
    "    if not delay_optimizer_creation:\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    self.state = TrainerState()\n",
    "    self.state.is_hyper_param_search = trial is not None\n",
    "    self.state.train_batch_size = self._train_batch_size\n",
    "\n",
    "    # Compute absolute values for logging, eval, and save if given as ratio\n",
    "    if args.logging_steps is not None:\n",
    "        if args.logging_steps < 1:\n",
    "            self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n",
    "        else:\n",
    "            self.state.logging_steps = args.logging_steps\n",
    "    if args.eval_steps is not None:\n",
    "        if args.eval_steps < 1:\n",
    "            self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n",
    "        else:\n",
    "            self.state.eval_steps = args.eval_steps\n",
    "    if args.save_steps is not None:\n",
    "        if args.save_steps < 1:\n",
    "            self.state.save_steps = math.ceil(max_steps * args.save_steps)\n",
    "        else:\n",
    "            self.state.save_steps = args.save_steps\n",
    "\n",
    "    # Activate gradient checkpointing if needed\n",
    "    if args.gradient_checkpointing:\n",
    "        if args.gradient_checkpointing_kwargs is None:\n",
    "            gradient_checkpointing_kwargs = {}\n",
    "        else:\n",
    "            gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs\n",
    "\n",
    "        self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "\n",
    "    model = self._wrap_model(self.model_wrapped)\n",
    "\n",
    "    # as the model is wrapped, don't use `accelerator.prepare`\n",
    "    # this is for unhandled cases such as\n",
    "    # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n",
    "    use_accelerator_prepare = True if model is self.model else False\n",
    "\n",
    "    if delay_optimizer_creation:\n",
    "        if use_accelerator_prepare:\n",
    "            self.model = self.accelerator.prepare(self.model)\n",
    "        self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "    # prepare using `accelerator` prepare\n",
    "    if use_accelerator_prepare:\n",
    "        self.model.train()\n",
    "        if hasattr(self.lr_scheduler, \"step\"):\n",
    "            if self.use_apex:\n",
    "                model = self.accelerator.prepare(self.model)\n",
    "            else:\n",
    "                model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
    "        else:\n",
    "            # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n",
    "            model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n",
    "                self.model, self.optimizer, self.lr_scheduler\n",
    "            )\n",
    "\n",
    "    if self.is_fsdp_enabled:\n",
    "        self.model = self.model_wrapped = model\n",
    "\n",
    "    # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "    if model is not self.model:\n",
    "        self.model_wrapped = model\n",
    "\n",
    "    # backward compatibility\n",
    "    if self.is_deepspeed_enabled:\n",
    "        self.deepspeed = self.model_wrapped\n",
    "\n",
    "    # ckpt loading\n",
    "    if resume_from_checkpoint is not None:\n",
    "        if self.is_deepspeed_enabled:\n",
    "            deepspeed_load_checkpoint(\n",
    "                self.model_wrapped, resume_from_checkpoint, load_module_strict=not _is_peft_model(self.model)\n",
    "            )\n",
    "        elif is_sagemaker_mp_enabled() or self.is_fsdp_enabled:\n",
    "            self._load_from_checkpoint(resume_from_checkpoint, self.model_wrapped)\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
    "\n",
    "    # important: at this point:\n",
    "    # self.model         is the Transformers Model\n",
    "    # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),\n",
    "    # FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {num_examples:,}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
    "    if self.args.per_device_train_batch_size != self._train_batch_size:\n",
    "        logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {self._train_batch_size:,}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
    "    logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
    "\n",
    "    self.state.epoch = 0\n",
    "    start_time = time.time()\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    steps_trained_progress_bar = None\n",
    "\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if resume_from_checkpoint is not None and os.path.isfile(\n",
    "        os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
    "    ):\n",
    "        self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
    "        epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "        if not args.ignore_data_skip:\n",
    "            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "            steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "        else:\n",
    "            steps_trained_in_current_epoch = 0\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "        logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "        if not args.ignore_data_skip:\n",
    "            logger.info(\n",
    "                f\"  Will skip the first {epochs_trained} epochs then the first\"\n",
    "                f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n",
    "            )\n",
    "\n",
    "    # Update the references\n",
    "    self.callback_handler.model = self.model\n",
    "    self.callback_handler.optimizer = self.optimizer\n",
    "    self.callback_handler.lr_scheduler = self.lr_scheduler\n",
    "    self.callback_handler.train_dataloader = train_dataloader\n",
    "    if self.hp_name is not None and self._trial is not None:\n",
    "        # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n",
    "        # parameter to Train when using DDP.\n",
    "        self.state.trial_name = self.hp_name(self._trial)\n",
    "    if trial is not None:\n",
    "        assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
    "        self.state.trial_params = hp_params(assignments)\n",
    "    else:\n",
    "        self.state.trial_params = None\n",
    "    # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "    # to set this after the load.\n",
    "    self.state.max_steps = max_steps\n",
    "    self.state.num_train_epochs = num_train_epochs\n",
    "    self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "    self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "    # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n",
    "    tr_loss = torch.tensor(0.0).to(args.device)\n",
    "    # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n",
    "    self._total_loss_scalar = 0.0\n",
    "    self._globalstep_last_logged = self.state.global_step\n",
    "    model.zero_grad()\n",
    "    grad_norm: Optional[float] = None\n",
    "\n",
    "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "\n",
    "    # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n",
    "    if not args.ignore_data_skip:\n",
    "        for epoch in range(epochs_trained):\n",
    "            sampler = get_dataloader_sampler(train_dataloader)\n",
    "            sampler_kinds = [RandomSampler]\n",
    "            if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n",
    "                sampler_kinds.append(SeedableRandomSampler)\n",
    "            is_random_sampler = isinstance(sampler, tuple(sampler_kinds))\n",
    "            if not is_random_sampler:\n",
    "                # We just need to begin an iteration to create the randomization of the sampler.\n",
    "                for _ in train_dataloader:\n",
    "                    break\n",
    "            else:\n",
    "                # Otherwise we need to call the whooooole sampler cause there is some random operation added\n",
    "                # AT THE VERY END!\n",
    "                sampler = sampler if sampler is not None else []\n",
    "                _ = list(sampler)\n",
    "\n",
    "    total_batched_samples = 0\n",
    "    for epoch in range(epochs_trained, num_train_epochs):\n",
    "        if self.args.group_by_cluster and (epoch % self.args.num_cluster_update_epochs == 0 or epoch == self.args.num_clustering_warmup_epochs) and epoch >= self.args.num_clustering_warmup_epochs:\n",
    "            self.update_dataloader_sampler(train_dataloader, epoch, num_train_epochs)\n",
    "        \n",
    "        if self.args.prune_metadata and (epoch % self.args.num_metadata_prune_epochs == 0 or epoch == self.args.num_metadata_prune_warmup_epochs) and epoch >= self.args.num_metadata_prune_warmup_epochs: \n",
    "            self.prune_metadata()\n",
    "        \n",
    "        epoch_iterator = train_dataloader\n",
    "        if hasattr(epoch_iterator, \"set_epoch\"):\n",
    "            epoch_iterator.set_epoch(epoch)\n",
    "\n",
    "        # Reset the past mems state at the beginning of each epoch if necessary.\n",
    "        if args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        steps_in_epoch = (\n",
    "            len(epoch_iterator)\n",
    "            if len_dataloader is not None\n",
    "            else args.max_steps * args.gradient_accumulation_steps\n",
    "        )\n",
    "        self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n",
    "\n",
    "        if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "            self._load_rng_state(resume_from_checkpoint)\n",
    "\n",
    "        rng_to_sync = False\n",
    "        steps_skipped = 0\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n",
    "            steps_skipped = steps_trained_in_current_epoch\n",
    "            steps_trained_in_current_epoch = 0\n",
    "            rng_to_sync = True\n",
    "\n",
    "        step = -1\n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            total_batched_samples += 1\n",
    "\n",
    "            if self.args.include_num_input_tokens_seen:\n",
    "                main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n",
    "                if main_input_name not in inputs:\n",
    "                    logger.warning(\n",
    "                        \"Tried to track the number of tokens seen, however the current model is \"\n",
    "                        \"not configured properly to know what item is the input. To fix this, add \"\n",
    "                        \"a `main_input_name` attribute to the model class you are using.\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.state.num_input_tokens_seen += self.accelerator.gather(inputs[main_input_name]).numel()\n",
    "            if rng_to_sync:\n",
    "                self._load_rng_state(resume_from_checkpoint)\n",
    "                rng_to_sync = False\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                if steps_trained_progress_bar is not None:\n",
    "                    steps_trained_progress_bar.update(1)\n",
    "                if steps_trained_in_current_epoch == 0:\n",
    "                    self._load_rng_state(resume_from_checkpoint)\n",
    "                continue\n",
    "            elif steps_trained_progress_bar is not None:\n",
    "                steps_trained_progress_bar.close()\n",
    "                steps_trained_progress_bar = None\n",
    "\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
    "\n",
    "            with self.accelerator.accumulate(model):\n",
    "                tr_loss_step = self.training_step(model, inputs)\n",
    "\n",
    "            if (\n",
    "                args.logging_nan_inf_filter\n",
    "                and not is_torch_tpu_available()\n",
    "                and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "            ):\n",
    "                # if loss is nan or inf simply add the average of previous logged losses\n",
    "                tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "            else:\n",
    "                tr_loss += tr_loss_step\n",
    "\n",
    "            self.current_flos += float(self.floating_point_ops(inputs))\n",
    "\n",
    "            is_last_step_and_steps_less_than_grad_acc = (\n",
    "                steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                total_batched_samples % args.gradient_accumulation_steps == 0\n",
    "                or\n",
    "                # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                is_last_step_and_steps_less_than_grad_acc\n",
    "            ):\n",
    "                # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\n",
    "                # in accelerate. So, explicitly enable sync gradients to True in that case.\n",
    "                if is_last_step_and_steps_less_than_grad_acc:\n",
    "                    self.accelerator.gradient_state._set_sync_gradients(True)\n",
    "\n",
    "                # Gradient clipping\n",
    "                if args.max_grad_norm is not None and args.max_grad_norm > 0:\n",
    "                    # deepspeed does its own clipping\n",
    "\n",
    "                    if is_sagemaker_mp_enabled() and args.fp16:\n",
    "                        _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n",
    "                    elif self.use_apex:\n",
    "                        # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "                        _grad_norm = nn.utils.clip_grad_norm_(\n",
    "                            amp.master_params(self.optimizer),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "                    else:\n",
    "                        _grad_norm = self.accelerator.clip_grad_norm_(\n",
    "                            model.parameters(),\n",
    "                            args.max_grad_norm,\n",
    "                        )\n",
    "\n",
    "                    if (\n",
    "                        is_accelerate_available()\n",
    "                        and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n",
    "                    ):\n",
    "                        grad_norm = model.get_global_grad_norm()\n",
    "                    else:\n",
    "                        grad_norm = _grad_norm.item() if _grad_norm is not None else None\n",
    "\n",
    "                # Optimizer step\n",
    "                self.optimizer.step()\n",
    "                optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n",
    "                if optimizer_was_run:\n",
    "                    # Delay optimizer scheduling until metrics are generated\n",
    "                    if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                self.state.global_step += 1\n",
    "                self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n",
    "                self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
    "\n",
    "                self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "            else:\n",
    "                self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
    "\n",
    "            if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "                # PyTorch/XLA relies on the data loader to insert the mark_step for\n",
    "                # each step. Since we are breaking the loop early, we need to manually\n",
    "                # insert the mark_step here.\n",
    "                if is_torch_tpu_available():\n",
    "                    xm.mark_step()\n",
    "                break\n",
    "        if step < 0:\n",
    "            logger.warning(\n",
    "                \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "                f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n",
    "                f\" num_steps ({max_steps}) higher than the number of available samples.\"\n",
    "            )\n",
    "            self.control.should_training_stop = True\n",
    "\n",
    "        self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
    "        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            if is_torch_tpu_available():\n",
    "                # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                xm.master_print(met.metrics_report())\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n",
    "                    \"configured. Check your training configuration if this is unexpected.\"\n",
    "                )\n",
    "        if self.control.should_training_stop:\n",
    "            break\n",
    "\n",
    "    if args.past_index and hasattr(self, \"_past\"):\n",
    "        # Clean the state at the end of training\n",
    "        delattr(self, \"_past\")\n",
    "\n",
    "    logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "    if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n",
    "        # Wait for everyone to get here so we are sure the model has been saved by process 0.\n",
    "        if is_torch_tpu_available():\n",
    "            xm.rendezvous(\"load_best_model_at_end\")\n",
    "        elif args.parallel_mode == ParallelMode.DISTRIBUTED:\n",
    "            dist.barrier()\n",
    "        elif is_sagemaker_mp_enabled():\n",
    "            smp.barrier()\n",
    "\n",
    "        self._load_best_model()\n",
    "\n",
    "    # add remaining tr_loss\n",
    "    self._total_loss_scalar += tr_loss.item()\n",
    "    train_loss = self._total_loss_scalar / self.state.global_step\n",
    "\n",
    "    metrics = speed_metrics(\n",
    "        \"train\",\n",
    "        start_time,\n",
    "        num_samples=num_train_samples,\n",
    "        num_steps=self.state.max_steps,\n",
    "        num_tokens=num_train_tokens,\n",
    "    )\n",
    "    self.store_flos()\n",
    "    metrics[\"total_flos\"] = self.state.total_flos\n",
    "    metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "    self.is_in_train = False\n",
    "\n",
    "    self._memory_tracker.stop_and_update_metrics(metrics)\n",
    "\n",
    "    self.log(metrics)\n",
    "\n",
    "    run_dir = self._get_output_dir(trial)\n",
    "    checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)\n",
    "\n",
    "    # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.\n",
    "    if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:\n",
    "        for checkpoint in checkpoints_sorted:\n",
    "            if not os.path.samefile(checkpoint, self.state.best_model_checkpoint):\n",
    "                logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n",
    "                shutil.rmtree(checkpoint)\n",
    "\n",
    "    self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "\n",
    "    # Wait for the checkpoint to be uploaded.\n",
    "    self._finish_current_push()\n",
    "\n",
    "    # After training we make sure to retrieve back the original forward pass method\n",
    "    # for the embedding layer by removing the forward post hook.\n",
    "    if self.neftune_noise_alpha is not None:\n",
    "        self._deactivate_neftune(self.model)\n",
    "\n",
    "    return TrainOutput(self.state.global_step, train_loss, metrics)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc27f85-b8d7-4147-9e19-2ea8f95f7d6d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dad14e-12dd-4e8c-a16a-1da96ec96257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.models.radga import RAD002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90781b5d-e073-4e77-b102-cdf72a55c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b7ca-4600-4066-bfa4-a347724e9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/scratch/scai/phd/aiz218323/scratch/outputs/default/',\n",
    "    per_device_train_batch_size=100,\n",
    "    per_device_eval_batch_size=100,\n",
    "    num_train_epochs=50,\n",
    "    eval_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    representation_accumulation_steps=100,\n",
    "    representation_attribute='data_fused_repr',\n",
    "    representation_search_type='INDEX',\n",
    "    evaluation_strategy='steps',\n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=0,\n",
    "    num_cluster_update_epochs=2,\n",
    "    num_cluster_size_update_epochs=2,\n",
    "    use_distributional_representation=False,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=1,\n",
    "    maximum_cluster_size=4,\n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None, \n",
    "    fp16=True,\n",
    "    \n",
    "    data_aug_meta_name='hlk',\n",
    "    augmentation_num_beams=3,\n",
    "    use_label_metadata=False,\n",
    "\n",
    "    predict_with_augmentation=True,\n",
    "    use_augmentation_index_representation=True,\n",
    "    metadata_representation_attribute='data_repr',\n",
    "    data_augmentation_attribute='data_repr',\n",
    "\n",
    "    # label_names=['hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask',\n",
    "    #              'hlk2lbl2data_idx', 'hlk2lbl2data_input_ids', 'hlk2lbl2data_attention_mask'],\n",
    "    label_names=['hlk2data_idx', 'hlk2data_input_ids', 'hlk2data_attention_mask'],\n",
    "\n",
    "    prune_metadata=True,\n",
    "    num_metadata_prune_epochs=1,\n",
    "    metadata_prune_batch_size=64,\n",
    "    num_metadata_prune_warmup_epochs=0,\n",
    "    prune_metadata_names=['cat_meta'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e98fe-3f9e-42ee-9d14-404fe9555533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RAD002 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.cross_head.k.bias', 'encoder.cross_head.k.weight', 'encoder.cross_head.o.bias', 'encoder.cross_head.o.weight', 'encoder.cross_head.q.bias', 'encoder.cross_head.q.weight', 'encoder.cross_head.v.bias', 'encoder.cross_head.v.weight', 'encoder.dr_head.layer_norm.bias', 'encoder.dr_head.layer_norm.weight', 'encoder.dr_head.projector.bias', 'encoder.dr_head.projector.weight', 'encoder.dr_head.transform.bias', 'encoder.dr_head.transform.weight', 'encoder.meta_head.layer_norm.bias', 'encoder.meta_head.layer_norm.weight', 'encoder.meta_head.projector.bias', 'encoder.meta_head.projector.weight', 'encoder.meta_head.transform.bias', 'encoder.meta_head.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = RAD002.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', num_batch_labels=5000, batch_size=bsz,\n",
    "                               margin=0.3, num_negatives=10, tau=0.1, apply_softmax=True,\n",
    "                               \n",
    "                               data_aug_meta_prefix='hlk2data', lbl2data_aug_meta_prefix='hlk2lbl', \n",
    "                               resize_length=5000,\n",
    "                               \n",
    "                               meta_loss_weight=0.3, pred_meta_prefix='cat', \n",
    "                               \n",
    "                               fusion_loss_weight=0.05, use_fusion_loss=True, use_noise=True, use_encoder_parallel=False)\n",
    "model.init_retrieval_head()\n",
    "model.init_cross_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4b300-50b2-40b4-b29d-40b6d578d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb74349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, valid_dset = block.train.dset.sample(n=1000), block.test.dset.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5bd9b-3f60-4eaa-8d6d-ddc3e2e6fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, valid_dset.data.data_lbl_filterer, prop=block.train.dset.data.data_lbl, \n",
    "                  pk=5, rk=5, rep_pk=[1, 3, 5], rep_rk=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc998894-1a9f-4c12-8c75-2f4ff5758302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    data_collator=block.collator, \n",
    "    train_dataset=train_dset, \n",
    "    eval_dataset=valid_dset,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d5fab-2c43-4ff8-b609-916bd609ff6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b369f0f-f0aa-4ba5-a311-e79ca5ff84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bda65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccdb0b-e563-4ace-9aab-5cd634dfd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = learn.predict(learn.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c40ed3-6a16-47ab-b6ad-4dc832cfeca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
