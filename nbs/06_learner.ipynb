{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b937d98-9beb-445c-bc68-c8d71a8c1ae0",
   "metadata": {},
   "source": [
    "# learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714cf2-0372-49f6-a39c-24752a28145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b498e-a0a3-4bdd-8741-ca516ddc64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ac07c-66a4-40d0-a39b-e5220c20fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch, re, math, numpy as np, os\n",
    "import torch.nn as nn\n",
    "from typing import Any, Tuple, Optional, Sequence, Union, Dict, List, NamedTuple\n",
    "\n",
    "from transformers.trainer_pt_utils import find_batch_size, nested_concat, nested_numpify, IterableDatasetShard\n",
    "from transformers.trainer_utils import has_length, denumpify_detensorize, speed_metrics\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers import AutoTokenizer, BatchEncoding, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.nn.parallel._functions import Scatter\n",
    "from torch.nn.parallel.scatter_gather import _is_namedtuple\n",
    "\n",
    "from xcai.data import *\n",
    "from xcai.generation.trie import Trie\n",
    "from xcai.generation.generate import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from fastcore.dispatch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547561-f805-40c1-a6b2-d89e772ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1b571-0043-4b86-acce-cb6cb0ef598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.test_utils import *\n",
    "from xcai.models.BT000X import *\n",
    "from xcai.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efab5fa-1f75-44e8-b533-525d577f4209",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188e3a7-9458-456f-8a19-104fd139a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "block = Test.from_cfg('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4d23b-4525-46bd-9a75-19ffca285235",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "batch = block.train.one_batch(bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47e1ed-ad70-475f-a01f-cd9320705796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0002.from_pretrained('bert-base-cased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95cdc4-ff76-4cd7-aa20-a54a03f7c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7a04d-81d7-402d-9a2d-b449b3acd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.to('cuda')\n",
    "b = b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64be93-2e54-4fbd-841b-c5d2828f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5e531-8f3c-438e-ac10-b46c42e90093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.9060, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d9190-682e-4017-954e-fad35c79ab13",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54c7fe-18b7-4502-a68d-5bbca3a75a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scatter(inputs, target_gpus, chunk_sizes=None, dim=0):\n",
    "    def scatter_map(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n",
    "        if _is_namedtuple(obj):\n",
    "            return [type(obj)(*args) for args in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, tuple) and len(obj) > 0:\n",
    "            return list(zip(*map(scatter_map, obj)))\n",
    "        if isinstance(obj, list) and len(obj) > 0:\n",
    "            return [list(i) for i in zip(*map(scatter_map, obj))]\n",
    "        if isinstance(obj, dict) and len(obj) > 0:\n",
    "            return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
    "        return [obj for _ in target_gpus] \n",
    "    try:\n",
    "        res = scatter_map(inputs)\n",
    "    finally:\n",
    "        scatter_map = None\n",
    "    return res\n",
    "    \n",
    "def scatter_kwargs(\n",
    "    inputs: Tuple[Any, ...],\n",
    "    kwargs: Optional[Dict[str, Any]],\n",
    "    target_gpus: Sequence[Union[int, torch.device]],\n",
    "    chunk_sizes: Optional[Sequence[int]]=None,\n",
    "    dim: int = 0,\n",
    ") -> Tuple[Tuple[Any, ...], Tuple[Dict[str, Any], ...]]:\n",
    "    scattered_inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n",
    "    scattered_kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n",
    "    if len(scattered_inputs) < len(scattered_kwargs):\n",
    "        scattered_inputs.extend(() for _ in range(len(scattered_kwargs) - len(scattered_inputs)))\n",
    "    elif len(scattered_kwargs) < len(inputs):\n",
    "        scattered_kwargs.extend({} for _ in range(len(scattered_inputs) - len(scattered_kwargs)))\n",
    "    return scattered_inputs, scattered_kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420b5aa-6d09-47c1-809d-af19e02f800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCDataParallel(DataParallel):\n",
    "\n",
    "    @delegates(DataParallel.__init__)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _get_meta_name(self, x:Optional[Dict[str, Any]]):\n",
    "        return list(set([k.split('_')[0] for k in x]).difference(['data', 'lbl2data'])) if len(x) else set()\n",
    "\n",
    "    def _extract_feat(self, x:Optional[Dict[str, Any]], prefix:str):\n",
    "        return {k:v for k,v in x.items() if re.match(f'^{prefix}', k) and not re.match(r'.*2ptr$', k)}\n",
    "\n",
    "    def scatter(\n",
    "        self,\n",
    "        inputs: Tuple[Any, ...],\n",
    "        kwargs: Optional[Dict[str, Any]],\n",
    "        device_ids: Sequence[Union[int, torch.device]],\n",
    "    ) -> Any:\n",
    "        if len(inputs): raise ValueError('`inputs` should be empty.')\n",
    "        meta_name = self._get_meta_name(kwargs)+['lbl2data']\n",
    "        \n",
    "        data_feat = self._extract_feat(kwargs, 'data')\n",
    "        scattered_inputs, scattered_kwargs = scatter_kwargs(inputs, data_feat, device_ids, None, dim=self.dim)\n",
    "        \n",
    "        for o in meta_name:\n",
    "            pn, chunk_sizes = f'{o}_data2ptr', None\n",
    "            if pn in kwargs:\n",
    "                ptr = kwargs[pn]\n",
    "                psz, csz = ptr.shape[0], math.ceil(ptr.shape[0]/len(device_ids))\n",
    "                psz = (psz//csz+1)*csz if psz%csz else psz\n",
    "                chunk_sizes = [ptr[p:q].sum() for p,q in zip(range(0, psz+1, csz), range(csz, psz+1, csz))]\n",
    "                _, sc_ptr = scatter_kwargs(inputs, {pn:ptr}, device_ids, None, dim=self.dim)\n",
    "                for p,q in zip(scattered_kwargs, sc_ptr): p.update(q)\n",
    "                    \n",
    "            feat = self._extract_feat(kwargs, o)    \n",
    "            _, sc_kwargs = scatter_kwargs(inputs, feat, device_ids, chunk_sizes, dim=self.dim)\n",
    "            for p,q in zip(scattered_kwargs, sc_kwargs): p.update(q)\n",
    "            \n",
    "        return tuple(scattered_inputs), tuple(scattered_kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553913e-875d-44c7-b58e-3b77ab8c4ae7",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484462a0-4806-461a-957c-6ae5d1bd1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        for k,v in kwargs.items(): print(k, ': ', v.shape, ', ', v.device)\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9a39d-bff3-4588-9fd4-2899208a6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = XCDataParallel(module=MyModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bff3f-e395-4486-9865-19d0f5bff763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_input_ids :  torch.Size([5, 11]) ,  cuda:0\n",
      "data_token_type_ids :  torch.Size([5, 11]) ,  cuda:0\n",
      "data_attention_mask :  torch.Size([5, 11]) ,  cuda:0\n",
      "lbl2data_data2ptr :  torch.Size([5]) ,  cuda:0\n",
      "lbl2data_idx :  torch.Size([10]) ,  cuda:0\n",
      "lbl2data_input_ids :  torch.Size([10, 12]) ,  cuda:0\n",
      "lbl2data_token_type_ids :  torch.Size([10, 12]) ,  cuda:0\n",
      "lbl2data_attention_mask :  torch.Size([10, 12]) ,  cuda:0\n",
      "data_input_ids :  torch.Size([5, 11]) ,  cuda:1\n",
      "data_token_type_ids :  torch.Size([5, 11]) ,  cuda:1\n",
      "data_attention_mask :  torch.Size([5, 11]) ,  cuda:1\n",
      "lbl2data_data2ptr :  torch.Size([5]) ,  cuda:1\n",
      "lbl2data_idx :  torch.Size([26]) ,  cuda:1\n",
      "lbl2data_input_ids :  torch.Size([26, 12]) ,  cuda:1\n",
      "lbl2data_token_type_ids :  torch.Size([26, 12]) ,  cuda:1\n",
      "lbl2data_attention_mask :  torch.Size([26, 12]) ,  cuda:1\n"
     ]
    }
   ],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a130ea-6f06-4e02-902c-bfb366e03a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dbb81-c666-4b05-ab48-4b81887afcf3",
   "metadata": {},
   "source": [
    "## XCTrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc194c4-bc85-48e0-ac54-cfd452fb590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCTrie:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_block(cls, block:XCDataBlock, meta:Optional[List]=None):\n",
    "        lbl_toks = block.lbl_info['input_ids']\n",
    "        lbl_info = [[i] for i in range(len(lbl_toks))]\n",
    "        \n",
    "        trie = Trie.from_list(lbl_toks, lbl_info)\n",
    "\n",
    "        if meta is not None:\n",
    "            meta_dset = block.train.dset.meta\n",
    "            for o in meta:\n",
    "                if f'{o}_meta' not in meta_dset: raise ValueError(f'`{o}_meta` does not exist.')\n",
    "                meta_toks = meta_dset[f'{o}_meta'].meta_info['input_ids']\n",
    "                lbl_meta = meta_dset[f'{o}_meta'].lbl_meta.T.tocsr()\n",
    "                meta_info = [o.indices.tolist() for o in lbl_meta]\n",
    "                if len(meta_toks) != len(meta_info): raise ValueError(f'`meta_toks` and `meta_info` should have equal length.')\n",
    "                trie.update(meta_toks, meta_info)\n",
    "                \n",
    "        return trie\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60df7f-8004-49ec-8cce-eb36a285c8e2",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7b634-3ec5-4b5c-9d1f-75f32fa47fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCEvalLoopOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    targ_idx: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    targ_ptr: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "\n",
    "class XCPredictionOutput(NamedTuple):\n",
    "    pred_idx: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_ptr: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    pred_score: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    num_samples: Optional[int]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fb71-7eb7-43c6-96ba-71759aa2b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearningArguments(Seq2SeqTrainingArguments):\n",
    "\n",
    "    @delegates(Seq2SeqTrainingArguments.__init__)\n",
    "    def __init__(self, \n",
    "                 generation_length_penalty:Optional[float]=1.0, \n",
    "                 generation_num_beams:Optional[int]=5, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generation_num_beams = generation_num_beams\n",
    "        self.generation_length_penalty = generation_length_penalty\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77b4bd-cd24-4b96-af56-474dfcbdcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XCLearner(Seq2SeqTrainer):\n",
    "\n",
    "    @delegates(Seq2SeqTrainer.__init__)\n",
    "    def __init__(self, \n",
    "                 trie:Trie, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tbs = TrieBeamSearch(trie, n_bm=self.args.generation_num_beams, len_penalty=self.args.generation_length_penalty)\n",
    "\n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        if unwrap_model(model) is not model:\n",
    "            return model\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            model = XCDataParallel(module=model)\n",
    "        return model\n",
    "\n",
    "    def evaluate(self:XCLearner, eval_dataset:Optional[Dataset]=None, ignore_keys:Optional[List[str]]=None, metric_key_prefix:str=\"eval\",\n",
    "                 **gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        self.gather_function, self._gen_kwargs  = self.accelerator.gather, gen_kwargs\n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "    \n",
    "    def predict(self:XCLearner, test_dataset: Dataset, ignore_keys:Optional[List[str]]=None, metric_key_prefix: str = \"test\",**gen_kwargs):\n",
    "        gen_kwargs = gen_kwargs.copy()\n",
    "        if gen_kwargs.get(\"length_penalty\") is None and self.args.generation_length_penalty is not None:\n",
    "            gen_kwargs[\"length_penalty\"] = self.args.generation_length_penalty\n",
    "        self.gather_function, self._gen_kwargs = self.accelerator.gather, gen_kwargs\n",
    "        self._memory_tracker.start()\n",
    "        \n",
    "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "        start_time = time.time()\n",
    "\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        output = eval_loop(test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(metric_key_prefix,start_time,num_samples=output.num_samples,num_steps=math.ceil(output.num_samples / total_batch_size),)\n",
    "        )\n",
    "        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n",
    "        self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "        return XCPredictionOutput(pred_idx=output.pred_idx, pred_ptr=output.pred_ptr, pred_score=output.pred_score, metrics=output.metrics, num_samples=output.num_samples)\n",
    "    \n",
    "    def _gather_host_output(self, output, host_output):\n",
    "        if output is not None:\n",
    "            output = self.accelerator.pad_across_processes(output, dim=1, pad_index=-100)\n",
    "            output = self.gather_function((output))\n",
    "            return output if host_output is None else nested_concat(host_output, output, padding_index=-100)\n",
    "        else: return host_output\n",
    "\n",
    "    def _gather_all_output(self, host_output, all_output):\n",
    "        if host_output is not None:\n",
    "            if isinstance(host_output, torch.Tensor): host_output = host_output.cpu()\n",
    "            return host_output if all_output is None else nested_concat(all_output, host_output, padding_index=-100)\n",
    "        else: return all_output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351dde7-b678-41d5-b710-005cb7db6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prediction_step(\n",
    "    self:XCLearner,\n",
    "    model: nn.Module,\n",
    "    inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "    prediction_loss_only: bool,\n",
    "    ignore_keys: Optional[List[str]] = None,\n",
    "    **gen_kwargs,\n",
    ") -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with self.compute_loss_context_manager(): outputs = model(**inputs)\n",
    "        loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "    if self.args.prediction_loss_only or prediction_loss_only: return loss, None\n",
    "        \n",
    "    inputs = self._prepare_inputs(inputs)\n",
    "    n_bm = gen_kwargs.pop(\"num_beams\") if \"num_beams\" in gen_kwargs and gen_kwargs[\"num_beams\"] is not None else self.args.generation_num_beams\n",
    "    len_penalty = gen_kwargs.pop(\"length_penalty\") if \"length_penalty\" in gen_kwargs and gen_kwargs[\"length_penalty\"] is not None else self.args.generation_length_penalty\n",
    "\n",
    "    generation_inputs = inputs.copy()\n",
    "    with torch.no_grad(): o = self.tbs.proc(self.model, generation_inputs, n_bm=n_bm, len_penalty=len_penalty)\n",
    "        \n",
    "    output = {'pred_idx':o['info2seq2data_idx'], 'pred_score':o['info2seq2data_score'], 'pred_ptr':o['info2seq2data_data2ptr']}\n",
    "    labels = {'targ_idx':inputs['lbl2data_idx'], 'targ_ptr':inputs['lbl2data_data2ptr']} if 'lbl2data_idx' in inputs else None\n",
    "    if labels is not None: output.update(labels)\n",
    "    \n",
    "    return loss, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189c5d7-a677-43e6-a78d-7485c07d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluation_loop(\n",
    "        self:XCLearner,\n",
    "        dataloader: DataLoader,\n",
    "        description: str,\n",
    "        prediction_loss_only: Optional[bool] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> XCEvalLoopOutput:\n",
    "    \n",
    "    prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n",
    "\n",
    "    model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "\n",
    "    if len(self.accelerator._models) == 0 and model is self.model:\n",
    "        model = self.accelerator.prepare(model) if self.is_deepspeed_enabled else self.accelerator.prepare_model(model, evaluation_mode=True)\n",
    "        if self.is_fsdp_enabled: self.model = model\n",
    "        if model is not self.model: self.model_wrapped = model\n",
    "        if self.is_deepspeed_enabled: self.deepspeed = self.model_wrapped\n",
    "\n",
    "    batch_size = self.args.eval_batch_size\n",
    "    model.eval()\n",
    "    self.callback_handler.eval_dataloader = dataloader\n",
    "    eval_dataset = getattr(dataloader, \"dataset\", None)\n",
    "    \n",
    "    if args.past_index >= 0: self._past = None\n",
    "\n",
    "    losses_host, all_losses = None, None\n",
    "    host_output, all_output = {}, {}\n",
    "    \n",
    "    observed_num_examples = 0\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        observed_batch_size = find_batch_size(inputs)\n",
    "        if observed_batch_size is not None:\n",
    "            observed_num_examples += observed_batch_size\n",
    "            if batch_size is None: batch_size = observed_batch_size\n",
    "                \n",
    "        loss, output = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "        \n",
    "        if loss is not None:\n",
    "            losses = self.gather_function((loss.repeat(batch_size)))\n",
    "            losses_host = losses if losses_host is None else nested_concat(losses_host, losses, padding_index=-100)\n",
    "        for k in output: host_output[k] = self._gather_host_output(output[k], host_output.get(k, None))\n",
    "            \n",
    "        self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n",
    "        \n",
    "        if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n",
    "            if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "            for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "    \n",
    "    self.gather_function = self.accelerator.gather_for_metrics\n",
    "    if args.past_index and hasattr(self, \"_past\"): delattr(self, \"_past\")\n",
    "\n",
    "    if losses_host is not None: all_losses = losses_host if all_losses is None else nested_concat(all_losses, losses, padding_index=-100)\n",
    "    for k in host_output: all_output[k], host_output[k] = self._gather_all_output(host_output[k], all_output.get(k, None)), None\n",
    "        \n",
    "    if has_length(eval_dataset): num_samples = len(eval_dataset)\n",
    "    elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n",
    "        num_samples = eval_dataset.num_examples\n",
    "    else:\n",
    "        if has_length(dataloader): num_samples = self.num_examples(dataloader)\n",
    "        else: num_samples = observed_num_examples\n",
    "    if num_samples == 0 and observed_num_examples > 0: num_samples = observed_num_examples\n",
    "\n",
    "    if (self.compute_metrics is not None and \n",
    "        'targ_idx' in all_output and all_output['targ_idx'] is not None and \n",
    "        'pred_idx' in all_output and all_output['pred_idx'] is not None): \n",
    "        metrics = self.compute_metrics(**all_output)\n",
    "    else: metrics = {}\n",
    "        \n",
    "    metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "    if all_losses is not None: metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "    if hasattr(self, \"jit_compilation_time\"): metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n",
    "        \n",
    "    for key in list(metrics.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"): metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "    \n",
    "    return XCEvalLoopOutput(pred_idx=all_output['pred_idx'], pred_ptr=all_output['pred_ptr'], pred_score=all_output['pred_score'],\n",
    "                            targ_idx=all_output['targ_idx'], targ_ptr=all_output['targ_ptr'], metrics=metrics, num_samples=num_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc27f85-b8d7-4147-9e19-2ea8f95f7d6d",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90781b5d-e073-4e77-b102-cdf72a55c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5926050-d0c2-401f-9131-9676fbf0831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = Test.from_cfg('data', valid_pct=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b7ca-4600-4066-bfa4-a347724e9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/garbage/T1/',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    eval_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    label_names=['lbl2data_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258a9bf-9a0a-4fbe-9ca9-55e375780297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BT0002.from_pretrained('bert-base-cased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9385d-a46a-4b29-8424-a6c174db797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adef3cbf507741a69edff02df0cb1a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trie = XCTrie.from_block(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5bd9b-3f60-4eaa-8d6d-ddc3e2e6fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.valid.data_lbl_filterer, prop=block.train.dset.data.data_lbl, pk=5, rk=5, rep_pk=[1, 3, 5], rep_rk=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc998894-1a9f-4c12-8c75-2f4ff5758302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    trie=trie,\n",
    "    data_collator=block.collator, \n",
    "    train_dataset=block.train.dset, \n",
    "    eval_dataset=block.valid.dset,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23702bc6-1d29-42da-a17d-be8c5def889f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91892689-f8b5-4e3a-991a-6dd1844ade05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='103860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    41/103860 01:19 < 59:08:30, 0.49 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>Psp@1</th>\n",
       "      <th>Psp@3</th>\n",
       "      <th>Psp@5</th>\n",
       "      <th>Psn@1</th>\n",
       "      <th>Psn@3</th>\n",
       "      <th>Psn@5</th>\n",
       "      <th>R@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.048509</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.036365</td>\n",
       "      <td>0.009234</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>0.009234</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.015473</td>\n",
       "      <td>0.052109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.152409</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.031189</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.049224</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.031262</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.047098</td>\n",
       "      <td>0.031262</td>\n",
       "      <td>0.034214</td>\n",
       "      <td>0.040095</td>\n",
       "      <td>0.072299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.835953</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.048425</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.032948</td>\n",
       "      <td>0.037704</td>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.028077</td>\n",
       "      <td>0.032004</td>\n",
       "      <td>0.064488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/35 00:11 < 00:06, 1.91 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2412\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2412\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[165], line 24\u001b[0m, in \u001b[0;36mXCLearner.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_length_penalty\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather, gen_kwargs\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "Cell \u001b[0;32mIn[168], line 38\u001b[0m, in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     35\u001b[0m     observed_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m---> 38\u001b[0m loss, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((loss\u001b[38;5;241m.\u001b[39mrepeat(batch_size)))\n",
      "Cell \u001b[0;32mIn[166], line 21\u001b[0m, in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m len_penalty \u001b[38;5;241m=\u001b[39m gen_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m gen_kwargs \u001b[38;5;129;01mand\u001b[39;00m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_length_penalty\n\u001b[1;32m     20\u001b[0m generation_inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_penalty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_idx\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_score\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m:o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_data2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarg_idx\u001b[39m\u001b[38;5;124m'\u001b[39m:inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbl2data_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarg_ptr\u001b[39m\u001b[38;5;124m'\u001b[39m:inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbl2data_data2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbl2data_idx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:163\u001b[0m, in \u001b[0;36mTrieBeamSearch.proc\u001b[0;34m(self, model, inputs, n_bm, len_penalty)\u001b[0m\n\u001b[1;32m    161\u001b[0m logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    162\u001b[0m hyps \u001b[38;5;241m=\u001b[39m [TrieBeam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrie, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_penalty) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m--> 163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [h\u001b[38;5;241m.\u001b[39mproc(l) \u001b[38;5;28;01mfor\u001b[39;00m h,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hyps, logits)]\n\u001b[1;32m    164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfm({k:\u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m[o[k] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]})\n\u001b[1;32m    165\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_seq2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    161\u001b[0m logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    162\u001b[0m hyps \u001b[38;5;241m=\u001b[39m [TrieBeam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrie, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_penalty) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m--> 163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h,l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hyps, logits)]\n\u001b[1;32m    164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfm({k:\u001b[38;5;28mlist\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m[o[k] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]})\n\u001b[1;32m    165\u001b[0m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2data_score\u001b[39m\u001b[38;5;124m'\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo2seq2data_seq2ptr\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:142\u001b[0m, in \u001b[0;36mTrieBeam.proc\u001b[0;34m(self, logits, n_bm, len_penalty)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     sc \u001b[38;5;241m=\u001b[39m logits[cur_len:cur_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(sc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m sc\n\u001b[0;32m--> 142\u001b[0m     v_tok, v_sc, v_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     top_ptr, top_sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopk(ptr, v_tok, v_sc, v_idx)\n\u001b[1;32m    144\u001b[0m     ptr, sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(top_ptr, top_sc)\n",
      "File \u001b[0;32m/scratch/scai/phd/aiz218323/Projects/xcai/xcai/generation/generate.py:95\u001b[0m, in \u001b[0;36mTrieBeam.valid\u001b[0;34m(self, ptr, sc)\u001b[0m\n\u001b[1;32m     93\u001b[0m     toks \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtokens\n\u001b[1;32m     94\u001b[0m     v_tok\u001b[38;5;241m.\u001b[39mextend(toks)\n\u001b[0;32m---> 95\u001b[0m     v_sc\u001b[38;5;241m.\u001b[39mextend(\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     96\u001b[0m     v_idx\u001b[38;5;241m.\u001b[39mextend([i \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(toks))])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_tok, v_sc, v_idx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a79d7-74af-4b90-b771-a0465699d990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f8844-dd8f-47e3-9106-c0b9d7d71607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
