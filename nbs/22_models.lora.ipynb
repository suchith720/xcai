{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e6eb9-1055-4ef2-8a69-f96f114873b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5df670-455c-420d-ac04-7c20e9e8a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae7bfc-8832-4f58-a40a-9933efb59ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np, os, pickle\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from xcai.core import store_attr\n",
    "from xcai.losses import MultiTriplet\n",
    "\n",
    "from xcai.models.modeling_utils import XCModelOutput, Parameters\n",
    "\n",
    "from transformers import DistilBertPreTrainedModel,DistilBertConfig\n",
    "from transformers.utils.generic import ModelOutput\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44e9dc-7609-4240-a94c-1218f1e4a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735fc35-0a48-4144-9257-b20aff212570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from xcai.block import *\n",
    "from xcai.models.PPP0XX import DBT010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d2398-a433-4abb-8b8d-1fd6e1bc54fa",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466831ec-4870-4a29-a499-bdd257d27fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd26c2-5267-4d80-9f0a-31e8b4d6c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealso_data-meta_distilbert-base-uncased_rm_ramen-cat.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dce31-4b9c-4c11-9b0c-932d585b34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3188d-9c0d-419e-8539-393766884aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(5)\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d534c2a-24dc-496f-b021-ad07dcbf813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'pcat2lbl_idx', 'pcat2lbl_lbl2data2ptr', 'pcat2lbl_data2ptr', 'cat2lbl_idx', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_lbl2data2ptr', 'cat2lbl_data2ptr', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'cat2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'data_idx'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225744ae-244f-42f1-98bd-752524ba6911",
   "metadata": {},
   "source": [
    "## `LOR001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22107ea9-3a56-4b06-8331-f6f94adace13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LOR001(DistilBertPreTrainedModel):\n",
    "    use_representation,use_generation = True,False\n",
    "    _tied_weights_keys = [\"peft_model.base_model.model.encoder.distilbert\"]\n",
    "\n",
    "    def __init__(\n",
    "        self, config, model, peft_config, \n",
    "        \n",
    "        pred_meta_prefix:Optional[str]=None, \n",
    "        \n",
    "        num_batch_labels:Optional[int]=None, \n",
    "        batch_size:Optional[int]=None,\n",
    "        margin:Optional[float]=0.3,\n",
    "        num_negatives:Optional[int]=5,\n",
    "        tau:Optional[float]=0.1,\n",
    "        apply_softmax:Optional[bool]=True,\n",
    "        \n",
    "        meta_loss_weight:Optional[float]=0.1,\n",
    "        \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "        store_attr('pred_meta_prefix,meta_loss_weight')\n",
    "        self.peft_model = get_peft_model(model, peft_config)\n",
    "        self.rep_loss_fn = MultiTriplet(bsz=batch_size, tn_targ=num_batch_labels, margin=margin, n_negatives=num_negatives, \n",
    "                                        tau=tau, apply_softmax=apply_softmax, reduce='mean')\n",
    "\n",
    "        self._mark_entire_model_as_trainable()\n",
    "\n",
    "    def _mark_entire_model_as_trainable(self):\n",
    "        for p in self.peft_model.parameters(): p.requires_grad_(True)\n",
    "\n",
    "    def _mark_only_adapters_as_trainable(self):\n",
    "        self.peft_model.base_model._mark_only_adapters_as_trainable(self.peft_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        **kwargs\n",
    "    ):  \n",
    "        data_o = self.peft_model(data_input_ids, data_attention_mask, **kwargs)\n",
    "\n",
    "        loss = data_o.loss\n",
    "        meta_inputs = Parameters.from_meta_pred_prefix(self.pred_meta_prefix, **kwargs)\n",
    "        if meta_inputs and loss is not None:\n",
    "            self._mark_only_adapters_as_trainable()\n",
    "            meta_inputs = next(iter(meta_inputs.values()))\n",
    "        \n",
    "            idx = torch.where(meta_inputs['data2ptr'])[0]\n",
    "            if len(idx) > 0:\n",
    "                meta_o = self.peft_model(data_input_ids=meta_inputs['input_ids'], data_attention_mask=meta_inputs['attention_mask'])\n",
    "                m_loss = self.rep_loss_fn(data_o.data_repr[idx], meta_o.data_repr, meta_inputs['data2ptr'][idx], meta_inputs['idx'], \n",
    "                                      meta_inputs['pdata2ptr'][idx], meta_inputs['pidx'])\n",
    "                loss += self.meta_loss_weight * m_loss\n",
    "                \n",
    "        self._mark_entire_model_as_trainable()\n",
    "        \n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_o.data_repr,\n",
    "            lbl2data_repr=data_o.lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f2400-412c-4f23-b7d2-b1d30c7df214",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbf773-3060-41d6-b888-e73e9aed1083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT010 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DBT010.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=1600, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                               n_negatives=10, apply_softmax=True, use_encoder_parallel=False)\n",
    "model.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8d675-46ea-459e-9ef9-6109e1c48e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"],\n",
    "    bias='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f025964-9a9b-4eda-9595-b59d6da9a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LOR001(DistilBertConfig(), model, lora_config, pred_meta_prefix='cat2data', batch_size=1600, num_batch_labels=5000, \n",
    "               margin=0.3, num_negatives=10, tau=0.1, apply_softmax=True, meta_loss_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc42c0b-a37a-4408-be8e-999dcc6fb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(model, batch, m_args=['pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_input_ids', \n",
    "                                        'cat2data_attention_mask', 'cat2data_data2ptr', \n",
    "                                        'pcat2lbl_idx', 'pcat2lbl_lbl2data2ptr', 'pcat2lbl_data2ptr', 'cat2lbl_idx', 'cat2lbl_input_ids', \n",
    "                                        'cat2lbl_attention_mask', 'cat2lbl_lbl2data2ptr', 'cat2lbl_data2ptr',\n",
    "                                        'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_input_ids', \n",
    "                                        'lbl2data_attention_mask', 'lbl2data_data2ptr', \n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce237917-3835-4bb3-b88a-eabd6c079180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /tmp/ipykernel_22440/1180721033.py(44)forward()\n",
      "     42         import pdb; pdb.set_trace()\n",
      "     43 \n",
      "---> 44         data_o = self.peft_model(data_input_ids, data_attention_mask, **kwargs)\n",
      "     45 \n",
      "     46         loss = data_o.loss\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    }
   ],
   "source": [
    "o = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70af2f9-ecc0-40d8-9044-08ccc3b2d68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0745, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec2d67-f204-474d-b934-704651b8d520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed327116-6ee7-46dc-9a38-13d18e54c833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
