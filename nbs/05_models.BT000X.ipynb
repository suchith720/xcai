{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8c456-5339-43aa-baed-b9eb2114b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.BT000X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f76551-93c3-40cf-915f-5a4d3c04cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc422-47c6-4c49-a719-e83d23acf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any\n",
    "from transformers import BertLMHeadModel, BatchEncoding, BertPreTrainedModel, BertModel\n",
    "from transformers.utils.generic import ModelOutput\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.test_utils import *\n",
    "from xcai.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae88304-b2a5-4988-b6e8-d30a2bd79f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3b726-0f16-46f1-ae48-b34febc784b0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f118491-1eb9-4e38-a63f-395612689d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "block = Test.from_cfg('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734cfb6-162b-42b7-8ddb-f5a85bc3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 20\n",
    "batch = block.train.one_batch(bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8142ce8-4ddd-4ab7-b791-7343d1ae64f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ace8df-a4bb-4f06-9f12-eb703f3cab3b",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d52e6c-596d-4b1e-9ffc-497fd58fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class XCModelOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: Optional[torch.FloatTensor] = None\n",
    "    lm_loss: Optional[torch.FloatTensor] = None\n",
    "    dr_loss: Optional[torch.FloatTensor] = None\n",
    "    data_repr: Optional[torch.FloatTensor] = None\n",
    "    lbl2data_repr: Optional[torch.FloatTensor] = None\n",
    "    data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    lbl2data_cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0dbd5-94f0-4da3-bff8-224f68db6330",
   "metadata": {},
   "source": [
    "## XCModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04965f07-b85e-4146-9bb7-b2f7e19d5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XCModel(BertLMHeadModel):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf63b9-b42e-48ef-afc0-5f7e4e56d8bb",
   "metadata": {},
   "source": [
    "## BT0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5948a-5fec-4663-b4a4-2d75387023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0001(BertLMHeadModel):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        data_logits = self.cls(data_o[0])\n",
    "        data_repr = data_o[0].mean(dim=1)\n",
    "        \n",
    "        if lbl2data_input_ids is not None and lbl2data_data2ptr is not None:\n",
    "            lbl2data_o = self.bert(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict\n",
    "            )\n",
    "            lbl2data_repr = lbl2data_o[0].mean(dim=1)\n",
    "            return data_logits, lbl2data_input_ids, lbl2data_data2ptr, data_repr, lbl2data_repr\n",
    "\n",
    "        return data_logits, lbl2data_input_ids, lbl2data_data2ptr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b57869-5bf0-4b58-b782-57c436150913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "m = BT0001.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43542a-c62b-4189-b4b4-3522983a43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083cc97-7978-4450-b9c2-e9dc868d99e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 18, 30522])\n",
      "torch.Size([37, 16])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 768])\n",
      "torch.Size([37, 768])\n"
     ]
    }
   ],
   "source": [
    "for o in out: print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f2018-78bd-4216-b25d-210994859696",
   "metadata": {},
   "source": [
    "## BT0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2252a1-c6b2-4a21-a6d7-83f6b295c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0002(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,False \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o = self.bert(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss = None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            loss = self.loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_logits,) + data_o[2:]\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            logits=data_logits,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e45035-b68c-4da3-bed5-dbfa547c0e55",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3c66a-017e-429b-931f-221329b04e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0002 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0002.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565efc8-ca0a-4ab5-ac81-815b4941d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc2bb0-c675-460c-8ff6-abe4e26dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70274c98-da35-4b9d-8f12-405117ecca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50b8fa-3ece-4f98-9556-c8b4c98c1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.2184, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f7667-a0be-47c3-abe2-b3d3eb7fd35a",
   "metadata": {},
   "source": [
    "## BT0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cfb86-7439-443b-a494-220c67f8a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0003(BertPreTrainedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.8,\n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, ig_tok=ig_tok, reduce='mean')\n",
    "        self.post_init()\n",
    "\n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, F.normalize(o[0].mean(dim=1), dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(data_input_ids, \n",
    "                                          data_attention_mask, \n",
    "                                          data_token_type_ids, \n",
    "                                          output_attentions=output_attentions, \n",
    "                                          output_hidden_states=output_hidden_states,\n",
    "                                          return_dict=return_dict)\n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(lbl2data_input_ids, \n",
    "                                                      lbl2data_attention_mask, \n",
    "                                                      lbl2data_token_type_ids, \n",
    "                                                      output_attentions=output_attentions, \n",
    "                                                      output_hidden_states=output_hidden_states,\n",
    "                                                      return_dict=return_dict)\n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb35ee-a55e-4567-8416-3b1b08d7a695",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb553f-30cd-430e-b71e-7c4d8a6b70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BT0003 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['loss_fn.v']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0003.from_pretrained('bert-base-uncased', tn_targ=10_000, ig_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa33df-0968-4c9b-88f8-588855ea91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f649083-ff41-4718-ad01-e1c5f7b5271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lbl2data_input_ids', 'lbl2data_token_type_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_input_ids', 'data_token_type_ids', 'data_attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b92693-31e7-4df3-83b9-9d80bc1bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = m.to('cuda'), b.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bab768-2fe2-4855-b517-87604c84a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659148e-9406-47c5-b744-bad69c721aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.data_repr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032054db-666b-4bcd-aaf5-7ecbdf54a7aa",
   "metadata": {},
   "source": [
    "## BT0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1056009-fe22-400b-a10d-1bf379a53ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BT0004(BertLMHeadModel):\n",
    "    use_generation,use_representation = True,True \n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None, \n",
    "                 ig_tok:Optional[int]=0,\n",
    "                 lw:Optional[int]=0.5,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.lw, self.dr_loss_fn = lw, SoupCon(bsz=bsz, reduce='mean')\n",
    "        self.lm_loss_fn = MultiCrossEntropy(tn_targ=tn_targ, ig_tok=ig_tok, reduce='mean')\n",
    "        \n",
    "    @delegates(BertModel.__call__)\n",
    "    def get_repr(self, \n",
    "                 input_ids:Optional[torch.Tensor]=None, \n",
    "                 attention_mask:Optional[torch.Tensor]=None,\n",
    "                 token_type_ids:Optional[torch.Tensor]=None,\n",
    "                 **kwargs):\n",
    "        o = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        return o, F.normalize(o[0].mean(dim=1), dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_token_type_ids:Optional[torch.Tensor]=None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        data_o, data_repr = self.get_repr(\n",
    "            data_input_ids,\n",
    "            data_attention_mask,\n",
    "            data_token_type_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        data_logits = self.cls(data_o[0])\n",
    "        \n",
    "        loss, lm_loss, dr_loss, lbl2data_repr = None, None, None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = self.get_repr(\n",
    "                lbl2data_input_ids,\n",
    "                lbl2data_attention_mask,\n",
    "                lbl2data_token_type_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "            lm_loss = self.lm_loss_fn(data_logits, lbl2data_input_ids, lbl2data_data2ptr)\n",
    "            dr_loss = self.dr_loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr)\n",
    "            loss = lm_loss + self.lw*dr_loss\n",
    "            \n",
    "        if not return_dict:\n",
    "            o = (data_logits,data_repr,lbl2data_repr) + data_o[2:]\n",
    "            return ((loss,lm_loss,dr_loss) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            lm_loss=lm_loss,\n",
    "            dr_loss=dr_loss,\n",
    "            logits=data_logits,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "            data_hidden_states=data_o.hidden_states,\n",
    "            data_attentions=data_o.attentions,\n",
    "            data_cross_attentions=data_o.cross_attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdffa25-f966-46d2-bb01-3cb7502dd180",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffd983-5470-41bd-9530-3f32464d6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = prepare_batch(m, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6a2e5-f5f8-4d60-85c9-65802898bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = b['data_input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28092e0b-b7df-4d2d-99b2-ab47fbe6c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BT0004 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dr_loss_fn.t', 'lm_loss_fn.o']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m = BT0004.from_pretrained('bert-base-uncased', lw=0.5, bsz=bsz, tn_targ=10_000, ig_tok=0)\n",
    "m, b = m.to('cuda'), b.to('cuda')\n",
    "o = m(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dae38-748b-4333-8e60-03eefa57b7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
