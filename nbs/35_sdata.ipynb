{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646426c7-93f9-4c40-adeb-7b04b74501a2",
   "metadata": {},
   "source": [
    "# SData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d09450-d08f-4b8c-988c-547430250692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d26b05-a557-4b6f-9500-f668a8d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a57087ab-f73b-47e5-bf51-494691acb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, inspect, numpy as np, scipy.sparse as sp, inspect\n",
    "from typing import Callable, Optional, Union, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BatchEncoding\n",
    "from itertools import chain\n",
    "\n",
    "from xcai.core import Filterer, Info\n",
    "from xcai.data import MainXCData, MetaXCData\n",
    "from xcai.data import BaseXCDataset, MainXCDataset, MetaXCDataset, XCDataset\n",
    "from xcai.data import BaseXCDataBlock, XCDataBlock\n",
    "from xcai.data import _read_sparse_file\n",
    "from xcai.graph.operations import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from plum import dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "752b92f9-98ac-4b2c-9c35-cc889d59b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7eb4-9022-4c05-abb9-3c056fb00cc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef80bda6-674a-4597-9dfd-fafa01f330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dir = '/Users/suchith720/Projects/data/(mapped)LF-WikiSeeAlsoTitles-320K'\n",
    "data_cfg = {\n",
    "    'info_column_names': ['identifier', 'input_text'],\n",
    "    'use_tokenizer': True,\n",
    "    'tokenizer': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
    "    'tokenization_column': 'input_text',\n",
    "    'main_max_data_sequence_length': 32,\n",
    "    'main_max_lbl_sequence_length': 32,\n",
    "    'meta_max_sequence_length': 32,\n",
    "    'padding': True,\n",
    "    'return_tensors': 'pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb62f1c-176a-4adf-8d0b-ce003a7d7a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065e4bbf-0c93-4c30-9d96-e27a10822624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa2ebe81d9d42feb92e4bdd484d209d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae034670574458fa4067773261487a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a854d2060da454ca06bff141fe4451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f2b37d3814decbb8201db5e5d65ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cfg = {\n",
    "    'data_lbl': f'{dset_dir}/trn_X_Y.txt',\n",
    "    'data_info': f'{dset_dir}/raw_data/train.raw.txt',\n",
    "    'lbl_info': f'{dset_dir}/raw_data/label.raw.txt',\n",
    "    'data_lbl_filterer': f'{dset_dir}/filter_labels_train.txt',\n",
    "}\n",
    "train_data = MainXCData.from_file(**train_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ec403-3c17-4102-8816-c3f8a4d77b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a79c584-be9a-48bc-a740-242d6168bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suchith720/Projects/pyxclib/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "meta_cfg = {\n",
    "    'prefix': 'hlk',\n",
    "    'data_meta': f'{dset_dir}/category_trn_X_Y.txt',\n",
    "    'lbl_meta': f'{dset_dir}/category_lbl_X_Y.txt',\n",
    "    'meta_info': f'{dset_dir}/raw_data/category.raw.txt',\n",
    "}\n",
    "meta_data = MetaXCData.from_file(**meta_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7bd28b7-d91c-4c41-bafa-9bcec00ff6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identity_collate_fn(batch): return BatchEncoding(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68132be5-746c-461c-93d6-8bfd71a0b546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ffa1dd6-c726-4fac-b1b9-f8dac0df01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(idxs:List, remove:Optional[float]=None, replace:Optional[float]=None):\n",
    "        remove_mask, replace_mask = list(), list()\n",
    "        for idx in idxs:\n",
    "            if remove is not None:\n",
    "                if np.random.rand() < remove:\n",
    "                    remove_mask.append([1]*len(idx))\n",
    "                    if replace is not None:\n",
    "                        replace_mask.append([0]*len(idx))\n",
    "                else:\n",
    "                    remove_mask.append([0]*len(idx))\n",
    "                    if replace is not None: \n",
    "                        replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))\n",
    "            elif replace is not None:\n",
    "                replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))\n",
    "        return remove_mask, replace_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def prune_indices_and_scores(output:Dict, prefix:str, data_lbl_indices:List, data_lbl_scores:List, \n",
    "                                 indices:List, num_samples:Optional[int]=None, use_distribution:Optional[bool]=False,\n",
    "                                 return_scores:Optional[bool]=False, dtype=torch.int64):\n",
    "        entity = prefix.split('2')[-1]\n",
    "        output[f'p{prefix}_idx'] = [data_lbl_indices[idx] for idx in indices]\n",
    "        scores = [data_lbl_scores[idx] for idx in indices] if use_distribution or return_scores else None\n",
    "        \n",
    "        if num_samples:\n",
    "            if scores is None:\n",
    "                output[f'p{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:num_samples]] for o in output[f'p{prefix}_idx']]\n",
    "            else:\n",
    "                idxs, sc = list(), list()\n",
    "                for p,q in zip(output[f'p{prefix}_idx'], scores):\n",
    "                    assert len(p) == len(q)\n",
    "                    rnd_idx = np.random.permutation(len(p))[:num_samples]\n",
    "                    idxs.append([p[i] for i in rnd_idx])\n",
    "                    sc.append([q[i] for i in rnd_idx])\n",
    "                output[f'p{prefix}_idx'], scores = idxs, sc\n",
    "                \n",
    "        output[f'p{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'p{prefix}_idx']], dtype=dtype)\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_indices_and_scores(indices:List, scores:Optional[List]=None, num_samples:Optional[int]=1, \n",
    "                                  oversample:Optional[bool]=False, use_distribution:Optional[bool]=False, \n",
    "                                  return_scores:Optional[bool]=False):\n",
    "        if use_distribution and scores is None:\n",
    "            raise ValueError(f'`scores` cannot be empty when `use_distribution` is set.')\n",
    "        \n",
    "        s_indices, s_scores = [], []\n",
    "        for k in range(len(indices)):\n",
    "            probs = scores[k] if use_distribution else None\n",
    "            size = num_samples if oversample else min(num_samples, len(indices[k]))\n",
    "            \n",
    "            rnd_idx = np.random.choice(len(indices[k]), size=size, p=probs, replace=oversample) if len(indices[k]) else []\n",
    "\n",
    "            s_indices.append([indices[k][i] for i in rnd_idx])\n",
    "            if return_scores:\n",
    "                assert len(indices[k]) == len(scores[k]), f'Length of indices({len(indices[k])}) and scores({(len(scores[k]))}) should be equal.'\n",
    "                s_scores.append([scores[k][i] for i in rnd_idx])\n",
    "\n",
    "        return s_indices, s_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def get_info(prefix:str, idxs:List, info:Dict, info_keys:List):\n",
    "        output = dict()\n",
    "        for k,v in info.items():\n",
    "            if k in info_keys:\n",
    "                if isinstance(v, np.ndarray) or isinstance(v, torch.Tensor):\n",
    "                    o = v[idxs]\n",
    "                    if isinstance(o, np.ndarray): o = torch.from_numpy(o)\n",
    "                    output[f'{prefix}_{k}'] = o\n",
    "                else:\n",
    "                    output[f'{prefix}_{k}'] = [v[idx] for idx in idxs]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_items(\n",
    "        prefix:str,\n",
    "        data_lbl_indices:List,\n",
    "        \n",
    "        indices:List, \n",
    "        num_samples:int, \n",
    "        num_sampler_samples:int, \n",
    "        oversample:bool, \n",
    "                      \n",
    "        info:Dict, \n",
    "        info_keys:List,\n",
    "        \n",
    "        use_distribution:Optional[bool]=False, \n",
    "        data_lbl_scores:Optional[List]=None, \n",
    "                      \n",
    "        dropout_remove:Optional[float]=None, \n",
    "        dropout_replace:Optional[float]=None, \n",
    "        return_scores:Optional[bool]=False,\n",
    "        dtype=torch.int64,\n",
    "    ):\n",
    "        output, entity = dict(), prefix.split('2')[-1]\n",
    "            \n",
    "        scores = Sampler.prune_indices_and_scores(output, prefix, data_lbl_indices, data_lbl_scores, indices, \n",
    "                                                  num_samples, use_distribution, return_scores, dtype=dtype)\n",
    "        \n",
    "        output[f'{prefix}_idx'], scores = Sampler.sample_indices_and_scores(output[f'p{prefix}_idx'], scores, \n",
    "                                                                            num_sampler_samples, oversample, \n",
    "                                                                            use_distribution, return_scores)\n",
    "        if return_scores:\n",
    "            output[f'{prefix}_scores'] = torch.tensor(list(chain(*scores)), dtype=torch.float32)\n",
    "\n",
    "        output[f'{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'{prefix}_idx']], dtype=dtype)\n",
    "        output[f'{prefix}_idx'] = torch.tensor(list(chain(*output[f'{prefix}_idx'])), dtype=dtype)\n",
    "        output[f'p{prefix}_idx'] = torch.tensor(list(chain(*output[f'p{prefix}_idx'])), dtype=dtype)\n",
    "        \n",
    "        if info is not None:\n",
    "            output.update(Sampler.get_info(prefix, output[f'{prefix}_idx'], info, info_keys))\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df2464-461a-4389-99d6-23abf6d6eafa",
   "metadata": {},
   "source": [
    "## SDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26266ee9-e981-41e3-b485-69ddd17d60af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SMainXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a0d1d23-3b05-4516-b48d-bd81ef6299b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMainXCDataset(MainXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_slbl_samples:Optional[int]=1,\n",
    "        main_oversample:Optional[bool]=False,\n",
    "        use_main_distribution:Optional[bool]=False,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_slbl_samples,main_oversample,use_main_distribution,return_scores')\n",
    "        \n",
    "        self.data_lbl_scores = None\n",
    "        if use_main_distribution or return_scores: self._store_scores()\n",
    "        \n",
    "    def _store_scores(self):\n",
    "        if self.data_lbl is not None:\n",
    "            if self.use_main_distribution:\n",
    "                data_lbl = self.data_lbl / (self.data_lbl.sum(axis=1) + 1e-9)\n",
    "                data_lbl = data_lbl.tocsr()\n",
    "            else:\n",
    "                data_lbl = self.data_lbl\n",
    "            self.data_lbl_scores = [o.data.tolist() for o in data_lbl]\n",
    "        \n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = {'data_idx': torch.tensor(idxs, dtype=torch.int64)}\n",
    "        x.update(self.get_info('data', idxs, self.data_info, self.data_info_keys))\n",
    "        if self.data_lbl is not None:\n",
    "            prefix = 'lbl2data'\n",
    "            o = Sampler.extract_items(prefix, self.curr_data_lbl, idxs, self.n_lbl_samples, self.n_slbl_samples, \n",
    "                                      self.main_oversample, self.lbl_info, self.lbl_info_keys, self.use_main_distribution, \n",
    "                                      self.data_lbl_scores, return_scores=self.return_scores)\n",
    "            x.update(o)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32d6c-ecb1-4af8-a070-ebb82b5c11d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d1d1a90-f3ae-44b2-bd20-1866331886ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main = SMainXCDataset(**train_data, n_slbl_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a3dba53-bb5d-4494-a123-3a3b38a71532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  6207,  2102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([23961,   927,  1471,  1470]),\n",
       " 'lbl2data_data2ptr': tensor([2, 2]),\n",
       " 'lbl2data_identifier': ['Applet',\n",
       "  'Application_posture',\n",
       "  'Outline_of_Africa',\n",
       "  'List_of_national_parks_in_Africa'],\n",
       " 'lbl2data_input_text': ['Applet',\n",
       "  'Application posture',\n",
       "  'Outline of Africa',\n",
       "  'List of national parks in Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,  6207,  2102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4646, 16819,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12685,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2120,  6328,  1999,  3088,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_main.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b7144-619b-454b-977c-e17f6b32a321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8ce8c-3933-4bdf-834c-cb38acfc615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main.oversample = True\n",
    "train_main.n_slbl_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63c977-2561-4b86-b0bb-8b88339fce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    train_main.__getitems__([1,2,3,4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7cbd3-cd4e-4f08-b4f6-4e26db6185ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_main, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3aac4-38ba-4853-927d-dfe26083b701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'data_identifier': ['Anarchism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Academy_Awards',\n",
       "  'International_Atomic_Time',\n",
       "  'Altruism',\n",
       "  'Allan_Dwan',\n",
       "  'Anthropology',\n",
       "  'Agricultural_science',\n",
       "  'Animation'],\n",
       " 'data_input_text': ['Anarchism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Academy Awards',\n",
       "  'International Atomic Time',\n",
       "  'Altruism',\n",
       "  'Allan Dwan',\n",
       "  'Anthropology',\n",
       "  'Agricultural science',\n",
       "  'Animation'],\n",
       " 'data_input_ids': tensor([[  101,  9617, 11140,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 19465,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 17484,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2914,  2982,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2248,  9593,  2051,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12456,  6820,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  8926,  1040,  7447,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12795,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4910,  2671,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  7284,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([    0,     1,     2,     3,     9, 26766,    12,    13,    14,    15,\n",
       "            16,    17,    18, 56258,    19,    20,    21,    22,    23,    24,\n",
       "            25,    26,    27,    28,    29,    30,    31,    32,    33,    34,\n",
       "            35,    36,    37,    38,    39,    40,    41,    42, 10243,    45,\n",
       "            48,    49,    50,    51,    52,    53,    54,    55,    56,    57,\n",
       "            58,    59,    60,    61,    62,    63,    64,    65,    66,    67,\n",
       "            68,    69,    70, 81953,   101,   102,   103,   104,   105]),\n",
       " 'plbl2data_data2ptr': tensor([ 3,  1,  2,  4,  4, 25,  1, 14, 10,  5]),\n",
       " 'lbl2data_idx': tensor([    1,     0,     3, 26766,     9,    12,    13,    16,    17, 10243,\n",
       "            21,    45,    56,    59,    64,    62,   101,   104]),\n",
       " 'lbl2data_data2ptr': tensor([2, 1, 2, 2, 2, 2, 1, 2, 2, 2]),\n",
       " 'lbl2data_identifier': ['Libertarian_socialism',\n",
       "  'Antinomianism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Conimbricenses',\n",
       "  'List_of_film_awards',\n",
       "  'List_of_Academy_Award_records',\n",
       "  'Clock_synchronization',\n",
       "  'Network_Time_Protocol',\n",
       "  'Altruism',\n",
       "  'Earning_to_give',\n",
       "  'Canadian_pioneers_in_early_Hollywood',\n",
       "  'Memetics',\n",
       "  'Prehistoric_medicine',\n",
       "  'Agroecology',\n",
       "  'Agricultural_sciences_basic_topics',\n",
       "  '12_basic_principles_of_animation',\n",
       "  'List_of_motion_picture_topics'],\n",
       " 'lbl2data_input_text': ['Libertarian socialism',\n",
       "  'Antinomianism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Conimbricenses',\n",
       "  'List of film awards',\n",
       "  'List of Academy Award records',\n",
       "  'Clock synchronization',\n",
       "  'Network Time Protocol',\n",
       "  'Altruism',\n",
       "  'Earning to give',\n",
       "  'Canadian pioneers in early Hollywood',\n",
       "  'Memetics',\n",
       "  'Prehistoric medicine',\n",
       "  'Agroecology',\n",
       "  'Agricultural sciences basic topics',\n",
       "  '12 basic principles of animation',\n",
       "  'List of motion picture topics'],\n",
       " 'lbl2data_input_ids': tensor([[  101, 19297, 14649,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3424,  3630, 20924,  2964,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 19465,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 17484,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  9530,  5714, 23736, 19023,  2229,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2143,  2982,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2914,  2400,  2636,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  5119, 26351,  8093, 10698,  9276,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2897,  2051,  8778,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12456,  6820,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  7414,  2000,  2507,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3010, 13200,  1999,  2220,  5365,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2033, 11368,  6558,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 14491,  4200,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12943,  3217,  8586,  6779,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4910,  4163,  3937,  7832,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2260,  3937,  6481,  1997,  7284,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  4367,  3861,  7832,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138603c1-ccec-4fc9-91d5-4470d4ff8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(data_cfg['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc2eef-e8f7-47ad-becc-30e910f9bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['plbl2data_data2ptr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57589218-fc9d-44c3-aea3-60992791b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.batch_decode(batch['data_input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ff3e2-a50b-41e3-a53f-be9cb818a402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SMetaXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f4215992-e99e-4fc3-b20c-4b639f547517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMetaXCDataset(MetaXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_sdata_meta_samples:Optional[int]=1,\n",
    "        n_slbl_meta_samples:Optional[int]=1,\n",
    "        meta_oversample:Optional[bool]=False,\n",
    "        use_meta_distribution:Optional[bool]=False,\n",
    "        meta_dropout_remove:Optional[float]=None,\n",
    "        meta_dropout_replace:Optional[float]=None,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_sdata_meta_samples,n_slbl_meta_samples,meta_oversample,use_meta_distribution')\n",
    "        store_attr('meta_dropout_remove,meta_dropout_replace,return_scores')\n",
    "\n",
    "        self.data_meta_scores, self.lbl_meta_scores = None, None\n",
    "        if use_meta_distribution or return_scores: self._store_scores()\n",
    "\n",
    "    def _store_scores(self):\n",
    "        def get_scores(matrix:sp.csr_matrix, use_meta_distribution:bool):\n",
    "            if matrix is not None:\n",
    "                if use_meta_distribution:\n",
    "                    matrix = matrix / (matrix.sum(axis=1) + 1e-9)\n",
    "                    matrix = matrix.tocsr()\n",
    "                return [o.data.tolist() for o in matrix]\n",
    "                \n",
    "        self.data_meta_scores = get_scores(self.data_meta, self.use_meta_distribution)\n",
    "        self.lbl_meta_scores = get_scores(self.lbl_meta, self.use_meta_distribution)\n",
    "        \n",
    "    def get_data_meta(self, idxs:List):\n",
    "        x, prefix = dict(), f'{self.prefix}2data'\n",
    "        o = Sampler.extract_items(prefix, self.curr_data_meta, idxs, self.n_data_meta_samples, self.n_sdata_meta_samples, \n",
    "                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, \n",
    "                                  self.data_meta_scores, dropout_remove=self.meta_dropout_remove, \n",
    "                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "        \n",
    "    def get_lbl_meta(self, idxs:List):\n",
    "        if self.curr_lbl_meta is None: return {}\n",
    "        x, prefix = dict(), f'{self.prefix}2lbl'\n",
    "        o = Sampler.extract_items(prefix, self.curr_lbl_meta, idxs, self.n_lbl_meta_samples, self.n_slbl_meta_samples, \n",
    "                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, \n",
    "                                  self.lbl_meta_scores, dropout_remove=self.meta_dropout_remove, \n",
    "                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b40080-e07e-4d00-929f-77247cf2f1d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f456f40c-89f4-445f-b3c2-1f74ed54b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = SMetaXCDataset(**meta_data, n_sdata_meta_samples=2, n_slbl_meta_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "182a9b58-8edd-4dd6-b78b-5d0df121bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.meta_oversample = True\n",
    "train_meta.n_sdata_meta_samples = 3\n",
    "train_meta.n_slbl_meta_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d418f48e-3278-4f39-9f6d-62c4f408c00f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phlk2data_idx': tensor([  1058, 147261, 149012,  85726]),\n",
       " 'phlk2data_data2ptr': tensor([3, 1]),\n",
       " 'hlk2data_idx': tensor([  1058, 149012,   1058,  85726,  85726,  85726]),\n",
       " 'hlk2data_data2ptr': tensor([3, 3]),\n",
       " 'hlk2data_identifier': ['Category:Technology_neologisms',\n",
       "  'Category:Component-based_software_engineering',\n",
       "  'Category:Technology_neologisms',\n",
       "  'Category:Geography_of_Africa',\n",
       "  'Category:Geography_of_Africa',\n",
       "  'Category:Geography_of_Africa'],\n",
       " 'hlk2data_input_text': ['Technology neologisms',\n",
       "  'Component-based software engineering',\n",
       "  'Technology neologisms',\n",
       "  'Geography of Africa',\n",
       "  'Geography of Africa',\n",
       "  'Geography of Africa'],\n",
       " 'hlk2data_input_ids': tensor([[  101,  2974,  9253, 21197, 22556,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  6922,  1011,  2241,  4007,  3330,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2974,  9253, 21197, 22556,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'hlk2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.get_data_meta([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73b58370-cc74-4605-9932-acc737beea93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phlk2lbl_idx': tensor([3384]),\n",
       " 'phlk2lbl_lbl2ptr': tensor([1, 0]),\n",
       " 'hlk2lbl_idx': tensor([3384, 3384, 3384]),\n",
       " 'hlk2lbl_lbl2ptr': tensor([3, 0]),\n",
       " 'hlk2lbl_identifier': ['Category:Animation_techniques',\n",
       "  'Category:Animation_techniques',\n",
       "  'Category:Animation_techniques'],\n",
       " 'hlk2lbl_input_text': ['Animation techniques',\n",
       "  'Animation techniques',\n",
       "  'Animation techniques'],\n",
       " 'hlk2lbl_input_ids': tensor([[ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0]]),\n",
       " 'hlk2lbl_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.get_lbl_meta([101, 201])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c371b7-6fd2-4561-be7a-790ce4695f59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0e4cf79-14dc-4228-b00b-b1d0f22be966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataset(XCDataset):\n",
    "\n",
    "    def __init__(self, data:SMainXCDataset, **kwargs):\n",
    "        super().__init__()\n",
    "        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], SMetaXCDataset)})\n",
    "        self._verify_inputs()\n",
    "        \n",
    "    @classmethod\n",
    "    @delegates(SMainXCDataset.from_file)\n",
    "    def from_file(cls, **kwargs):\n",
    "        data = SMainXCDataset.from_file(**kwargs)\n",
    "        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}\n",
    "\n",
    "        meta = dict()\n",
    "        for k,v in meta_kwargs.items():\n",
    "            input_kwargs = {p:q.get(k,None) if isinstance(q, dict) else q for p,q in kwargs.items()}\n",
    "            for o in v: input_kwargs.pop(o, None)\n",
    "            meta[k] = SMetaXCDataset.from_file(**v, **input_kwargs)  \n",
    "        # meta = {k:SMetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}\n",
    "        \n",
    "        return cls(data, **meta)\n",
    "        \n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = self.data.__getitems__(idxs)\n",
    "        if self.n_meta:\n",
    "            for meta in self.meta.values():\n",
    "                x.update(meta.get_data_meta(idxs))\n",
    "                if self.n_lbl:\n",
    "                    z = meta.get_lbl_meta(x['lbl2data_idx'])\n",
    "                    if len(z):\n",
    "                        z[f'{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                        z[f'p{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                    x.update(z)\n",
    "        return x\n",
    "\n",
    "    # =========== Operations ===========\n",
    "    \n",
    "    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, topk:Optional[int]=10, **kwargs):\n",
    "        data_lbl = Graph.threshold_on_degree(self.data.data_lbl, thresh=thresh)\n",
    "        data_meta, lbl_meta = Graph.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk, do_normalize=True)\n",
    "        self.meta['ohm_meta'] = SMetaXCDataset(prefix='ohm', data_meta=data_meta, lbl_meta=lbl_meta, \n",
    "                                               meta_info=self.data.lbl_info, **kwargs)\n",
    "\n",
    "    def get_random_walk_metadata(self, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, prob_reset:Optional[float]=0.8, \n",
    "                                 topk_thresh:Optional[int]=10, degree_thresh=20, **kwargs):\n",
    "        data_meta = perform_random_walk(data_lbl, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                        n_hops=1, thresh=degree_thresh, topk=topk_thresh, do_normalize=True)\n",
    "        lbl_meta = perform_random_walk(data_lbl.transpose().tocsr(), batch_size=batch_size, walk_to=walk_to, \n",
    "                                       prob_reset=prob_reset, n_hops=2, thresh=degree_thresh, topk=topk_thresh, do_normalize=True)\n",
    "        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_meta, lbl_meta=lbl_meta,\n",
    "                                               meta_info=self.data.lbl_info, **kwargs)\n",
    "\n",
    "    def get_random_walk_with_matrices_metadata(self, meta_name:str, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, \n",
    "                                               prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, data_degree_thresh=20, \n",
    "                                               lbl_degree_thresh=20, **kwargs):\n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "        data_meta, lbl_meta = self.meta[f'{meta_name}_meta'].data_meta, self.meta[f'{meta_name}_meta'].lbl_meta\n",
    "        data_rnw = perform_random_walk_with_matrices(data_meta, lbl_meta, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                                     n_hops=2, data_thresh=data_degree_thresh, lbl_thresh=lbl_degree_thresh, \n",
    "                                                     topk=topk_thresh, do_normalize=True)\n",
    "        lbl_rnw = perform_random_walk_with_matrices(lbl_meta, data_meta, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                                    n_hops=3, data_thresh=data_degree_thresh, lbl_thresh=lbl_degree_thresh, \n",
    "                                                    topk=topk_thresh, do_normalize=True)\n",
    "        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_rnw, lbl_meta=lbl_rnw, meta_info=self.data.lbl_info, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def combine_info(info_1:Dict, info_2:Dict, pad_token:int=0):\n",
    "        comb_info = dict()\n",
    "        for k,v in info_1.items():\n",
    "            if isinstance(v, tuple) or isinstance(v, list): comb_info[k] = v + info_2[k]\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                n_data = v.shape[0] + info_2[k].shape[0]\n",
    "                seq_len = max(v.shape[1], info_2[k].shape[1]) \n",
    "                \n",
    "                if k == 'input_ids': \n",
    "                    info = torch.full((n_data, seq_len), pad_token, dtype=v.dtype)\n",
    "                elif k == 'attention_mask': \n",
    "                    info = torch.full((n_data, seq_len), 0, dtype=v.dtype)\n",
    "                    \n",
    "                info[:v.shape[0], :v.shape[1]] = v\n",
    "                info[v.shape[0]:, :info_2[k].shape[1]] = info_2[k]\n",
    "                \n",
    "                comb_info[k] = info\n",
    "        return comb_info\n",
    "\n",
    "    def _get_main_dataset(\n",
    "        self,\n",
    "        data_info:Dict, \n",
    "        data_lbl:Optional[sp.csr_matrix]=None, \n",
    "        lbl_info:Optional[Dict]=None, \n",
    "        data_lbl_filterer:Optional[Union[sp.csr_matrix,np.array]]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        dset = self.data._get_dataset(data_info, data_lbl, lbl_info, data_lbl_filterer, **kwargs)\n",
    "        return SXCDataset(dset)\n",
    "        \n",
    "    def combine_lbl_and_meta(self, meta_name:str, pad_token:int=0, p_data=0.5, **kwargs): \n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "            \n",
    "        data_lbl = self.data.data_lbl\n",
    "        data_lbl = data_lbl.multiply(1/(data_lbl.getnnz(axis=1).reshape(-1, 1) + 1e-9))\n",
    "        data_lbl = data_lbl.tocsr() * p_data\n",
    "        \n",
    "        data_meta = self.meta[f'{meta_name}_meta'].data_meta\n",
    "        data_meta = data_meta.multiply(1/(data_meta.getnnz(axis=1).reshape(-1, 1) + 1e-9))\n",
    "        data_meta = data_meta.tocsr() * (1 - p_data)\n",
    "        \n",
    "        lbl_info = self.data.lbl_info\n",
    "        meta_info = self.meta[f'{meta_name}_meta'].meta_info\n",
    "\n",
    "        comb_info = self.combine_info(lbl_info, meta_info, pad_token)\n",
    "        \n",
    "        return self._get_main_dataset(self.data.data_info, sp.hstack([data_lbl, data_meta]), comb_info, \n",
    "                                      self.data.data_lbl_filterer, **kwargs)\n",
    "\n",
    "    def combine_data_and_meta(self, meta_name:str, pad_token:int=0, **kwargs):\n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "        \n",
    "        data_lbl, meta_lbl = self.data.data_lbl, self.meta[f'{meta_name}_meta'].lbl_meta.transpose().tocsr()\n",
    "        assert data_lbl.shape[1] == meta_lbl.shape[1], f\"Incompatible metadata shape: {meta_lbl.shape}\"\n",
    "\n",
    "        data_info = self.data.data_info\n",
    "        meta_info = self.meta[f'{meta_name}_meta'].meta_info\n",
    "        comb_info = self.combine_info(data_info, meta_info, pad_token)\n",
    "\n",
    "        dset = self._get_main_dataset(comb_info, sp.vstack([data_lbl, meta_lbl]), self.data.lbl_info, \n",
    "                                      self.data.data_lbl_filterer, **kwargs)\n",
    "        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]\n",
    "        return dset._getitems(valid_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_combined_data_and_meta(dset, meta_lbl:sp.csr_matrix, meta_info:Dict, pad_token:int=0, **kwargs):    \n",
    "        data_lbl = dset.data.data_lbl\n",
    "        assert data_lbl.shape[1] == meta_lbl.shape[1], f\"Incompatible metadata shape: {meta_lbl.shape}\"\n",
    "        \n",
    "        data_info = dset.data.data_info\n",
    "        comb_info = dset.combine_info(data_info, meta_info, pad_token)\n",
    "        \n",
    "        dset = dset._get_main_dataset(comb_info, sp.vstack([data_lbl, meta_lbl]), dset.data.lbl_info, \n",
    "                                      dset.data.data_lbl_filterer, **kwargs)\n",
    "        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]\n",
    "        return dset._getitems(valid_idx)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6bffa-f910-49d2-8768-aaf3cdf3307e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f4e65-ec24-4ccd-b549-b294710f355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = SXCDataset(train_main, hlk_meta=train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af6a17-4677-4432-b1e1-5b20ea85bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = train_dset.__getitems__([100, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfa06c-8951-4807-b596-274c7a7ca2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2698e0e-046a-495b-b33f-0537a16f8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_idx',\n",
       " 'data_identifier',\n",
       " 'data_input_text',\n",
       " 'data_input_ids',\n",
       " 'data_token_type_ids',\n",
       " 'data_attention_mask',\n",
       " 'plbl2data_idx',\n",
       " 'plbl2data_data2ptr',\n",
       " 'lbl2data_idx',\n",
       " 'lbl2data_data2ptr',\n",
       " 'lbl2data_identifier',\n",
       " 'lbl2data_input_text',\n",
       " 'lbl2data_input_ids',\n",
       " 'lbl2data_token_type_ids',\n",
       " 'lbl2data_attention_mask',\n",
       " 'phlk2data_idx',\n",
       " 'phlk2data_data2ptr',\n",
       " 'hlk2data_idx',\n",
       " 'hlk2data_data2ptr',\n",
       " 'hlk2data_identifier',\n",
       " 'hlk2data_input_text',\n",
       " 'hlk2data_input_ids',\n",
       " 'hlk2data_token_type_ids',\n",
       " 'hlk2data_attention_mask',\n",
       " 'phlk2lbl_idx',\n",
       " 'phlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_idx',\n",
       " 'hlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_identifier',\n",
       " 'hlk2lbl_input_text',\n",
       " 'hlk2lbl_input_ids',\n",
       " 'hlk2lbl_token_type_ids',\n",
       " 'hlk2lbl_attention_mask',\n",
       " 'hlk2lbl_data2ptr',\n",
       " 'phlk2lbl_data2ptr']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76066769-9ad8-4517-a8f9-bc3959551919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c19d34-89af-44ba-b37b-35f542c1017d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SBaseXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40685068-05c1-47f3-b185-c6384266800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SBaseXCDataBlock(BaseXCDataBlock):\n",
    "    \n",
    "    @classmethod\n",
    "    @delegates(SXCDataset.from_file)\n",
    "    def from_file(cls, collate_fn:Callable=identity_collate_fn, **kwargs):\n",
    "        return cls(SXCDataset.from_file(**kwargs), collate_fn, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f5540-8c21-425f-a4e9-15fa9df61d1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab7517-43b9-4f21-bdaa-a03c50f48aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_block = SBaseXCDataBlock(train_dset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cdac-4a66-4595-a633-a215b6669d93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09b24a62-8db4-4d46-88bf-3af0cc0ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataBlock(XCDataBlock):\n",
    "\n",
    "    @staticmethod\n",
    "    def inference_dset(data_info:Dict, data_lbl:sp.csr_matrix, lbl_info:Dict, data_lbl_filterer, \n",
    "                       **kwargs):\n",
    "        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)\n",
    "        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)\n",
    "        data_lbl[x_idx, y_idx] = 1\n",
    "        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])\n",
    "    \n",
    "        pred_dset = SXCDataset(SMainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,\n",
    "                                              data_lbl_filterer=data_lbl_filterer, **kwargs))\n",
    "        return pred_dset\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cfg(\n",
    "        cls, \n",
    "        cfg:Union[str,Dict],\n",
    "        collate_fn:Optional[Callable]=identity_collate_fn,\n",
    "        valid_pct:Optional[float]=0.2,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)\n",
    "\n",
    "        blocks = dict()\n",
    "        for o in ['train', 'valid', 'test']:\n",
    "            if o in cfg['path']:\n",
    "                params = cfg['parameters'].copy()\n",
    "                params.update(kwargs)\n",
    "                if o != 'train': \n",
    "                    params['meta_dropout_remove'], params['meta_dropout_replace'] = None, None\n",
    "                blocks[o] = SBaseXCDataBlock.from_file(**cfg['path'][o], **params, collate_fn=collate_fn)\n",
    "                \n",
    "        return cls(**blocks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed004a61-5ede-490a-85a5-5771fd33a53a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a21f-1de2-4b1f-ac65-eb401210d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.config import WIKISEEALSOTITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c260e-a02a-438a-a81c-c537523584cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock(train=train_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8194-6733-4938-a9cb-c278b4985a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_dset = block.linker_dset('hlk_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d91ba-9a68-4761-91d0-a5bdaf2ad782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194342b4-4515-4315-84e1-45124cd72afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f1099-f0ad-4df9-8ff5-2a5090685b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f3aaa-822f-48b6-9ed8-cf068d6cf539",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = WIKISEEALSOTITLES('/home/scai/phd/aiz218323/Projects/XC/data')['data_lnk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10505c9f-603a-440b-9f21-be93c4614210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transform_type': 'xc', 'smp_features': [('lbl2data', 1, 2), ('hlk2data', 1, 1), ('hlk2lbl2data', 2, 1)], 'pad_token': 0, 'oversample': False, 'sampling_features': [('lbl2data', 2), ('hlk2data', 1), ('hlk2lbl2data', 1)], 'num_labels': 1, 'num_metadata': 1, 'metadata_name': None, 'info_column_names': ['identifier', 'input_text'], 'use_tokenizer': True, 'tokenizer': 'bert-base-cased', 'tokenization_column': 'input_text', 'max_sequence_length': 32, 'padding': False, 'return_tensors': None, 'sep': '->', 'prompt_func': None, 'pad_side': 'right', 'drop': True, 'ret_t': True, 'in_place': True, 'collapse': True, 'device': 'cpu', 'inp': 'data', 'targ': 'lbl2data', 'ptr': 'lbl2data_data2ptr', 'n_lbl_samples': None, 'data_info_keys': None, 'lbl_info_keys': None, 'n_slbl_samples': 1, 'main_oversample': False, 'n_data_meta_samples': 1, 'n_lbl_meta_samples': 1, 'meta_info_keys': None, 'meta_oversample': False}\n"
     ]
    }
   ],
   "source": [
    "print(config['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5990f-6000-4473-8cda-53f73b2938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'return_tensors':'pt', 'padding':True}\n",
    "\n",
    "for k,v in params.items():\n",
    "    config['parameters'][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e74e2e-1321-4405-af56-b86805bdfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock.from_cfg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136c3ae-b689-479f-95d7-565c102b56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.dset.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c88bf-3b9b-4e04-a1a4-9afc5ab02b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  7302,  1204,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([  930, 27329]),\n",
       " 'lbl2data_data2ptr': tensor([1, 1]),\n",
       " 'lbl2data_identifier': ['Abstract_Window_Toolkit', 'Geography_of_Africa'],\n",
       " 'lbl2data_input_text': ['Abstract Window Toolkit', 'Geography of Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,   138,  4832, 15017, 24769,  6466, 10493,  2875,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plnk2data_idx': tensor([   762, 202927]),\n",
       " 'plnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_idx': tensor([   762, 202927]),\n",
       " 'lnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_identifier': ['Category:Free_computer_libraries',\n",
       "  'Category:Geography_of_the_Indian_Ocean'],\n",
       " 'lnk2data_input_text': ['Free computer libraries',\n",
       "  'Geography of the Indian Ocean'],\n",
       " 'lnk2data_input_ids': tensor([[  101,  4299,  2775,  9818,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 20678,  1104,  1103,  1890,  4879,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'lnk2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'lnk2data_attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'plnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'plnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'lnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_identifier': [],\n",
       " 'lnk2lbl_input_text': [],\n",
       " 'lnk2lbl_input_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_token_type_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_attention_mask': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_data2ptr': tensor([0, 0])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5f296-1e5c-464a-9db5-fbdc4df0a272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
