{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646426c7-93f9-4c40-adeb-7b04b74501a2",
   "metadata": {},
   "source": [
    "# SData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d09450-d08f-4b8c-988c-547430250692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d26b05-a557-4b6f-9500-f668a8d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57087ab-f73b-47e5-bf51-494691acb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, inspect, numpy as np, scipy.sparse as sp, inspect\n",
    "from typing import Callable, Optional, Union, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BatchEncoding\n",
    "from itertools import chain\n",
    "\n",
    "from xcai.core import Filterer, Info\n",
    "from xcai.data import MainXCData, NegXCData, MetaXCData\n",
    "from xcai.data import BaseXCDataset, MainXCDataset, MetaXCDataset, XCDataset\n",
    "from xcai.data import MetaXCDatasets, BaseXCDataBlock, XCDataBlock\n",
    "from xcai.data import _read_sparse_file\n",
    "from xcai.graph.operations import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from plum import dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "752b92f9-98ac-4b2c-9c35-cc889d59b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7eb4-9022-4c05-abb9-3c056fb00cc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef80bda6-674a-4597-9dfd-fafa01f330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dir = '/Users/suchith720/Projects/data/(mapped)LF-WikiSeeAlsoTitles-320K'\n",
    "data_cfg = {\n",
    "    'info_column_names': ['identifier', 'input_text'],\n",
    "    'use_tokenizer': True,\n",
    "    'tokenizer': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
    "    'tokenization_column': 'input_text',\n",
    "    'main_max_data_sequence_length': 32,\n",
    "    'main_max_lbl_sequence_length': 32,\n",
    "    'meta_max_sequence_length': 32,\n",
    "    'padding': True,\n",
    "    'return_tensors': 'pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb62f1c-176a-4adf-8d0b-ce003a7d7a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065e4bbf-0c93-4c30-9d96-e27a10822624",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = {\n",
    "    'data_lbl': f'{dset_dir}/trn_X_Y.txt',\n",
    "    'data_info': f'{dset_dir}/raw_data/train.raw.txt',\n",
    "    'lbl_info': f'{dset_dir}/raw_data/label.raw.txt',\n",
    "    'data_lbl_filterer': f'{dset_dir}/filter_labels_train.txt',\n",
    "}\n",
    "train_data = MainXCData.from_file(**train_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b215cd0-9c9b-4e0d-8c0e-87c4e881c0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedea812-39f8-47b5-bdc4-930da3e8d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cfg = {\n",
    "    'data_neg': f'{dset_dir}/category_trn_X_Y.txt',\n",
    "    'neg_info': f'{dset_dir}/raw_data/category.raw.txt',\n",
    "}\n",
    "neg_data = NegXCData.from_file(**neg_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e79417-89dc-481e-bd06-e8c0c16a2989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a79c584-be9a-48bc-a740-242d6168bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cfg = {\n",
    "    'prefix': 'hlk',\n",
    "    'data_meta': f'{dset_dir}/category_trn_X_Y.txt',\n",
    "    'lbl_meta': f'{dset_dir}/category_lbl_X_Y.txt',\n",
    "    'neg_meta': f'{dset_dir}/category_lbl_X_Y.txt',\n",
    "    'meta_info': f'{dset_dir}/raw_data/category.raw.txt',\n",
    "}\n",
    "meta_data = MetaXCData.from_file(**meta_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5bde94-7cbb-41ea-b840-a95df0a62ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data['neg_meta'] = meta_data['neg_meta'].T @ meta_data['neg_meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732b773-1e53-4a77-a2a9-55b3bce1789c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7bd28b7-d91c-4c41-bafa-9bcec00ff6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identity_collate_fn(batch): return BatchEncoding(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68132be5-746c-461c-93d6-8bfd71a0b546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ffa1dd6-c726-4fac-b1b9-f8dac0df01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sampler:\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(idxs:List, remove:Optional[float]=None, replace:Optional[float]=None):\n",
    "        remove_mask, replace_mask = list(), list()\n",
    "        for idx in idxs:\n",
    "            if remove is not None:\n",
    "                if np.random.rand() < remove:\n",
    "                    remove_mask.append([1]*len(idx))\n",
    "                    if replace is not None:\n",
    "                        replace_mask.append([0]*len(idx))\n",
    "                else:\n",
    "                    remove_mask.append([0]*len(idx))\n",
    "                    if replace is not None: \n",
    "                        replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))\n",
    "            elif replace is not None:\n",
    "                replace_mask.append([1]*len(idx) if np.random.rand() < replace else [0]*len(idx))\n",
    "        return remove_mask, replace_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def prune_indices_and_scores(output:Dict, prefix:str, data_lbl_indices:List, data_lbl_scores:List, \n",
    "                                 indices:List, num_samples:Optional[int]=None, use_distribution:Optional[bool]=False,\n",
    "                                 return_scores:Optional[bool]=False, dtype=torch.int64):\n",
    "        entity = prefix.split('2')[-1]\n",
    "        output[f'p{prefix}_idx'] = [data_lbl_indices[idx] for idx in indices]\n",
    "        scores = [data_lbl_scores[idx] for idx in indices] if use_distribution or return_scores else None\n",
    "        \n",
    "        if num_samples:\n",
    "            if scores is None:\n",
    "                output[f'p{prefix}_idx'] = [[o[i] for i in np.random.permutation(len(o))[:num_samples]] for o in output[f'p{prefix}_idx']]\n",
    "            else:\n",
    "                idxs, sc = list(), list()\n",
    "                for p,q in zip(output[f'p{prefix}_idx'], scores):\n",
    "                    assert len(p) == len(q)\n",
    "                    rnd_idx = np.random.permutation(len(p))[:num_samples]\n",
    "                    idxs.append([p[i] for i in rnd_idx])\n",
    "                    sc.append([q[i] for i in rnd_idx])\n",
    "                output[f'p{prefix}_idx'], scores = idxs, sc\n",
    "                \n",
    "        output[f'p{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'p{prefix}_idx']], dtype=dtype)\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_indices_and_scores(indices:List, scores:Optional[List]=None, num_samples:Optional[int]=1, \n",
    "                                  oversample:Optional[bool]=False, use_distribution:Optional[bool]=False, \n",
    "                                  return_scores:Optional[bool]=False):\n",
    "        if use_distribution and scores is None:\n",
    "            raise ValueError(f'`scores` cannot be empty when `use_distribution` is set.')\n",
    "        \n",
    "        s_indices, s_scores = [], []\n",
    "        for k in range(len(indices)):\n",
    "            probs = scores[k] if use_distribution else None\n",
    "            size = num_samples if oversample else min(num_samples, len(indices[k]))\n",
    "            \n",
    "            rnd_idx = np.random.choice(len(indices[k]), size=size, p=probs, replace=oversample) if len(indices[k]) else []\n",
    "\n",
    "            s_indices.append([indices[k][i] for i in rnd_idx])\n",
    "            if return_scores:\n",
    "                assert len(indices[k]) == len(scores[k]), f'Length of indices({len(indices[k])}) and scores({(len(scores[k]))}) should be equal.'\n",
    "                s_scores.append([scores[k][i] for i in rnd_idx])\n",
    "\n",
    "        return s_indices, s_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def get_info(prefix:str, idxs:List, info:Dict, info_keys:List):\n",
    "        output = dict()\n",
    "        for k,v in info.items():\n",
    "            if k in info_keys:\n",
    "                if isinstance(v, np.ndarray) or isinstance(v, torch.Tensor):\n",
    "                    o = v[idxs]\n",
    "                    if isinstance(o, np.ndarray): o = torch.from_numpy(o)\n",
    "                    output[f'{prefix}_{k}'] = o\n",
    "                else:\n",
    "                    output[f'{prefix}_{k}'] = [v[idx] for idx in idxs]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_items(\n",
    "        prefix:str,\n",
    "        data_lbl_indices:List,\n",
    "        \n",
    "        indices:List, \n",
    "        num_samples:int, \n",
    "        num_sampler_samples:int, \n",
    "        oversample:bool, \n",
    "                      \n",
    "        info:Dict, \n",
    "        info_keys:List,\n",
    "        \n",
    "        use_distribution:Optional[bool]=False, \n",
    "        data_lbl_scores:Optional[List]=None, \n",
    "                      \n",
    "        dropout_remove:Optional[float]=None, \n",
    "        dropout_replace:Optional[float]=None, \n",
    "        return_scores:Optional[bool]=False,\n",
    "        dtype=torch.int64,\n",
    "    ):\n",
    "        output, entity = dict(), prefix.split('2')[-1]\n",
    "            \n",
    "        scores = Sampler.prune_indices_and_scores(output, prefix, data_lbl_indices, data_lbl_scores, indices, \n",
    "                                                  num_samples, use_distribution, return_scores, dtype=dtype)\n",
    "        \n",
    "        output[f'{prefix}_idx'], scores = Sampler.sample_indices_and_scores(output[f'p{prefix}_idx'], scores, \n",
    "                                                                            num_sampler_samples, oversample, \n",
    "                                                                            use_distribution, return_scores)\n",
    "        if return_scores:\n",
    "            output[f'{prefix}_scores'] = torch.tensor(list(chain(*scores)), dtype=torch.float32)\n",
    "\n",
    "        output[f'{prefix}_{entity}2ptr'] = torch.tensor([len(o) for o in output[f'{prefix}_idx']], dtype=dtype)\n",
    "        output[f'{prefix}_idx'] = torch.tensor(list(chain(*output[f'{prefix}_idx'])), dtype=dtype)\n",
    "        output[f'p{prefix}_idx'] = torch.tensor(list(chain(*output[f'p{prefix}_idx'])), dtype=dtype)\n",
    "        \n",
    "        if info is not None:\n",
    "            output.update(Sampler.get_info(prefix, output[f'{prefix}_idx'], info, info_keys))\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df2464-461a-4389-99d6-23abf6d6eafa",
   "metadata": {},
   "source": [
    "## SDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26266ee9-e981-41e3-b485-69ddd17d60af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SMainXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a0d1d23-3b05-4516-b48d-bd81ef6299b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMainXCDataset(MainXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_slbl_samples:Optional[int]=1,\n",
    "        n_sneg_samples:Optional[int]=1,\n",
    "        main_oversample:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_slbl_samples,main_oversample,n_sneg_samples')\n",
    "        \n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = {'data_idx': torch.tensor(idxs, dtype=torch.int64)}\n",
    "        x.update(self.get_info('data', idxs, self.data_info, self.data_info_keys))\n",
    "        if self.data_lbl is not None:\n",
    "            prefix = 'lbl2data'\n",
    "            o = Sampler.extract_items(prefix, self.curr_data_lbl, idxs, self.n_lbl_samples, self.n_slbl_samples, \n",
    "                                      self.main_oversample, self.lbl_info, self.lbl_info_keys, self.use_main_distribution, \n",
    "                                      self.data_lbl_scores, return_scores=self.return_scores)\n",
    "            x.update(o)\n",
    "        if self.data_neg is not None:\n",
    "            prefix = 'neg2data'\n",
    "            o = Sampler.extract_items(prefix, self.curr_data_neg, idxs, self.n_neg_samples, self.n_sneg_samples, \n",
    "                                      self.main_oversample, self.neg_info, self.lbl_info_keys, self.use_main_distribution, \n",
    "                                      self.data_neg_scores, return_scores=self.return_scores)\n",
    "            x.update(o)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32d6c-ecb1-4af8-a070-ebb82b5c11d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d1d1a90-f3ae-44b2-bd20-1866331886ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main = SMainXCDataset(**train_data, **neg_data, n_slbl_samples=2, n_sneg_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c38e-a864-4000-8ead-c0c9531e8fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3dba53-bb5d-4494-a123-3a3b38a71532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  6207,  2102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([23961,   930,  1470, 27329]),\n",
       " 'lbl2data_data2ptr': tensor([2, 2]),\n",
       " 'lbl2data_identifier': ['Applet',\n",
       "  'Abstract_Window_Toolkit',\n",
       "  'List_of_national_parks_in_Africa',\n",
       "  'Geography_of_Africa'],\n",
       " 'lbl2data_input_text': ['Applet',\n",
       "  'Abstract Window Toolkit',\n",
       "  'List of national parks in Africa',\n",
       "  'Geography of Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,  6207,  2102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 10061,  3332,  6994, 23615,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2120,  6328,  1999,  3088,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'pneg2data_idx': tensor([  1058, 147261, 149012,  85726]),\n",
       " 'pneg2data_data2ptr': tensor([3, 1]),\n",
       " 'neg2data_idx': tensor([  1058, 149012,  85726]),\n",
       " 'neg2data_data2ptr': tensor([2, 1]),\n",
       " 'neg2data_identifier': ['Category:Technology_neologisms',\n",
       "  'Category:Component-based_software_engineering',\n",
       "  'Category:Geography_of_Africa'],\n",
       " 'neg2data_input_text': ['Technology neologisms',\n",
       "  'Component-based software engineering',\n",
       "  'Geography of Africa'],\n",
       " 'neg2data_input_ids': tensor([[  101,  2974,  9253, 21197, 22556,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  6922,  1011,  2241,  4007,  3330,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'neg2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_main.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b7144-619b-454b-977c-e17f6b32a321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a8ce8c-3933-4bdf-834c-cb38acfc615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main.oversample = True\n",
    "train_main.n_slbl_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63c977-2561-4b86-b0bb-8b88339fce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    train_main.__getitems__([1,2,3,4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8db7cbd3-cd4e-4f08-b4f6-4e26db6185ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_main, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86d3aac4-38ba-4853-927d-dfe26083b701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pneg2data_idx', 'pneg2data_data2ptr', 'neg2data_idx', 'neg2data_data2ptr', 'neg2data_identifier', 'neg2data_input_text', 'neg2data_input_ids', 'neg2data_attention_mask'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740280f-5924-435c-8f1d-ef1d4b1a837c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138603c1-ccec-4fc9-91d5-4470d4ff8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(data_cfg['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc2eef-e8f7-47ad-becc-30e910f9bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['plbl2data_data2ptr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57589218-fc9d-44c3-aea3-60992791b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.batch_decode(batch['data_input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ff3e2-a50b-41e3-a53f-be9cb818a402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SMetaXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4215992-e99e-4fc3-b20c-4b639f547517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMetaXCDataset(MetaXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_sdata_meta_samples:Optional[int]=1,\n",
    "        n_slbl_meta_samples:Optional[int]=1,\n",
    "        n_sneg_meta_samples:Optional[int]=1,\n",
    "        meta_oversample:Optional[bool]=False,\n",
    "        meta_dropout_remove:Optional[float]=None,\n",
    "        meta_dropout_replace:Optional[float]=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_sdata_meta_samples,n_slbl_meta_samples,n_sneg_meta_samples,meta_oversample')\n",
    "        store_attr('meta_dropout_remove,meta_dropout_replace')\n",
    "        \n",
    "    def get_data_meta(self, idxs:List):\n",
    "        x, prefix = dict(), f'{self.prefix}2data'\n",
    "        o = Sampler.extract_items(prefix, self.curr_data_meta, idxs, self.n_data_meta_samples, self.n_sdata_meta_samples, \n",
    "                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, \n",
    "                                  self.data_meta_scores, dropout_remove=self.meta_dropout_remove, \n",
    "                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "        \n",
    "    def get_lbl_meta(self, idxs:List):\n",
    "        if self.curr_lbl_meta is None: return {}\n",
    "        x, prefix = dict(), f'{self.prefix}2lbl'\n",
    "        o = Sampler.extract_items(prefix, self.curr_lbl_meta, idxs, self.n_lbl_meta_samples, self.n_slbl_meta_samples, \n",
    "                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, \n",
    "                                  self.lbl_meta_scores, dropout_remove=self.meta_dropout_remove, \n",
    "                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "\n",
    "    def get_neg_meta(self, idxs:List):\n",
    "        if self.curr_neg_meta is None: return {}\n",
    "        x, prefix = dict(), f'{self.prefix}2neg'\n",
    "        o = Sampler.extract_items(prefix, self.curr_neg_meta, idxs, self.n_neg_meta_samples, self.n_sneg_meta_samples, \n",
    "                                  self.meta_oversample, self.meta_info, self.meta_info_keys, self.use_meta_distribution, \n",
    "                                  self.neg_meta_scores, dropout_remove=self.meta_dropout_remove, \n",
    "                                  dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b40080-e07e-4d00-929f-77247cf2f1d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f456f40c-89f4-445f-b3c2-1f74ed54b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = SMetaXCDataset(**meta_data, n_sdata_meta_samples=2, n_slbl_meta_samples=2, n_sneg_meta_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "182a9b58-8edd-4dd6-b78b-5d0df121bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.meta_oversample = True\n",
    "train_meta.n_sdata_meta_samples = 3\n",
    "train_meta.n_slbl_meta_samples = 3\n",
    "train_meta.n_sneg_meta_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7b0ba-5789-4852-98b3-f28041b31402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d418f48e-3278-4f39-9f6d-62c4f408c00f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phlk2data_idx': tensor([  1058, 147261, 149012,  85726]),\n",
       " 'phlk2data_data2ptr': tensor([3, 1]),\n",
       " 'hlk2data_idx': tensor([  1058, 147261,   1058,  85726,  85726,  85726]),\n",
       " 'hlk2data_data2ptr': tensor([3, 3]),\n",
       " 'hlk2data_identifier': ['Category:Technology_neologisms',\n",
       "  'Category:Java_(programming_language)_libraries',\n",
       "  'Category:Technology_neologisms',\n",
       "  'Category:Geography_of_Africa',\n",
       "  'Category:Geography_of_Africa',\n",
       "  'Category:Geography_of_Africa'],\n",
       " 'hlk2data_input_text': ['Technology neologisms',\n",
       "  'Java (programming language) libraries',\n",
       "  'Technology neologisms',\n",
       "  'Geography of Africa',\n",
       "  'Geography of Africa',\n",
       "  'Geography of Africa'],\n",
       " 'hlk2data_input_ids': tensor([[  101,  2974,  9253, 21197, 22556,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  9262,  1006,  4730,  2653,  1007,  8860,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2974,  9253, 21197, 22556,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'hlk2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.get_data_meta([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73b58370-cc74-4605-9932-acc737beea93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phlk2lbl_idx': tensor([3384]),\n",
       " 'phlk2lbl_lbl2ptr': tensor([1, 0]),\n",
       " 'hlk2lbl_idx': tensor([3384, 3384, 3384]),\n",
       " 'hlk2lbl_lbl2ptr': tensor([3, 0]),\n",
       " 'hlk2lbl_identifier': ['Category:Animation_techniques',\n",
       "  'Category:Animation_techniques',\n",
       "  'Category:Animation_techniques'],\n",
       " 'hlk2lbl_input_text': ['Animation techniques',\n",
       "  'Animation techniques',\n",
       "  'Animation techniques'],\n",
       " 'hlk2lbl_input_ids': tensor([[ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [ 101, 7284, 5461,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0]]),\n",
       " 'hlk2lbl_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.get_lbl_meta([101, 201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7157858c-716c-4fc1-86e3-d10862667316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phlk2neg_idx': tensor([   101,    104,    118,    656,  48312,  51371,  53545,  53675,  54031,\n",
       "          63968,  64922,  65253,  65395,  65791,  65856,  66293,  66294,  66296,\n",
       "          66522,  68379,  68382,  69598,  70849,  70851,  70852,  73260,  76668,\n",
       "          79564,  79565,  80980,  86645,  86665,  88452,  90196,  94879,  94880,\n",
       "          94884,  94887,  94897,  94901,  94905,  94909,  98481, 102111, 103164,\n",
       "         105478, 105568, 107366, 107723, 108591, 108772, 108908, 110880, 111414,\n",
       "         112168, 112309, 115799, 121654, 127576, 127997, 128161, 130413, 130468,\n",
       "         133543, 134761, 156509, 205071, 208270, 208316, 213596, 213597, 213598,\n",
       "         213599, 213600, 213601, 213602, 213603, 213604, 213605, 213606, 213607,\n",
       "         213608, 213609, 213610, 213611, 213612, 213613, 213614, 213615, 213616,\n",
       "         213617, 213618, 213619, 213620, 213621, 267378, 315385, 346751, 346756,\n",
       "         346760, 352651, 371882, 371888, 371889, 371890, 371891, 371892, 371893,\n",
       "         397187, 398363, 398364, 398365, 398366, 398367, 398368, 398369, 398370,\n",
       "         398371, 508949, 535438, 535439,    199,    200,    201,    202,    203,\n",
       "            204,  47104,  67138,  77529,  77531, 109400, 109914, 163680, 205328,\n",
       "         284011, 401562]),\n",
       " 'phlk2neg_neg2ptr': tensor([121,  16]),\n",
       " 'hlk2neg_idx': tensor([ 70852, 105478, 213615, 109400, 163680, 163680]),\n",
       " 'hlk2neg_neg2ptr': tensor([3, 3]),\n",
       " 'hlk2neg_identifier': ['Category:Recipients_of_the_Order_of_the_Red_Banner',\n",
       "  'Category:Recipients_of_the_Order_of_Polonia_Restituta_(1944â\\x80\\x9389)',\n",
       "  'Category:Recipients_of_the_Order_of_the_Gold_Lion_of_the_House_of_Nassau',\n",
       "  'Category:Heritage_sites_in_British_Columbia',\n",
       "  'Category:Canyons_and_gorges_of_British_Columbia',\n",
       "  'Category:Canyons_and_gorges_of_British_Columbia'],\n",
       " 'hlk2neg_input_text': ['Recipients of the Order of the Red Banner',\n",
       "  'Recipients of the Order of Polonia Restituta (1944â\\x80\\x9389)',\n",
       "  'Recipients of the Order of the Gold Lion of the House of Nassau',\n",
       "  'Heritage sites in British Columbia',\n",
       "  'Canyons and gorges of British Columbia',\n",
       "  'Canyons and gorges of British Columbia'],\n",
       " 'hlk2neg_input_ids': tensor([[  101, 15991,  1997,  1996,  2344,  1997,  1996,  2417,  9484,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 15991,  1997,  1996,  2344,  1997, 11037,  6200,  2717,  4183,\n",
       "          13210,  1006,  3646,  2050,  2620,  2683,  1007,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 15991,  1997,  1996,  2344,  1997,  1996,  2751,  7006,  1997,\n",
       "           1996,  2160,  1997, 14646,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  4348,  4573,  1999,  2329,  3996,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  8399,  2015,  1998, 14980,  2015,  1997,  2329,  3996,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  8399,  2015,  1998, 14980,  2015,  1997,  2329,  3996,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'hlk2neg_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.get_neg_meta([101, 201])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c371b7-6fd2-4561-be7a-790ce4695f59",
   "metadata": {},
   "source": [
    "### `SXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0e4cf79-14dc-4228-b00b-b1d0f22be966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataset(XCDataset):\n",
    "\n",
    "    def __init__(self, data:SMainXCDataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], SMetaXCDataset)})\n",
    "        self._verify_inputs()\n",
    "        \n",
    "    @classmethod\n",
    "    @delegates(SMainXCDataset.from_file)\n",
    "    def from_file(cls, **kwargs):\n",
    "        data = SMainXCDataset.from_file(**kwargs)\n",
    "        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}\n",
    "\n",
    "        meta = dict()\n",
    "        for k,v in meta_kwargs.items():\n",
    "            input_kwargs = {p:q.get(k,None) if isinstance(q, dict) else q for p,q in kwargs.items()}\n",
    "            for o in v: input_kwargs.pop(o, None)\n",
    "            meta[k] = SMetaXCDataset.from_file(**v, **input_kwargs)  \n",
    "        # meta = {k:SMetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}\n",
    "        \n",
    "        return cls(data, **meta)\n",
    "        \n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = self.data.__getitems__(idxs)\n",
    "        if self.n_meta:\n",
    "            for meta in self.meta.values():\n",
    "                x.update(meta.get_data_meta(idxs))\n",
    "                if self.n_lbl:\n",
    "                    z = meta.get_lbl_meta(x['lbl2data_idx'])\n",
    "                    if len(z):\n",
    "                        z[f'{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                        z[f'p{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                    x.update(z)\n",
    "                if self.n_neg:\n",
    "                    z = meta.get_neg_meta(x['neg2data_idx'])\n",
    "                    if len(z):\n",
    "                        z[f'{meta.prefix}2neg_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2neg_neg2ptr'].split_with_sizes(x[f'neg2data_data2ptr'].tolist())])\n",
    "                        z[f'p{meta.prefix}2neg_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2neg_neg2ptr'].split_with_sizes(x[f'neg2data_data2ptr'].tolist())])\n",
    "                    x.update(z)\n",
    "        return x\n",
    "\n",
    "    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, \n",
    "                             topk:Optional[int]=10, **kwargs):\n",
    "        data_lbl = Graph.threshold_on_degree(self.data.data_lbl, thresh=thresh)\n",
    "        data_meta, lbl_meta = Graph.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk, \n",
    "                                                   do_normalize=True)\n",
    "        \n",
    "        self.meta['ohm_meta'] = SMetaXCDataset(prefix='ohm', data_meta=data_meta, lbl_meta=lbl_meta, \n",
    "                                               meta_info=dset.data.lbl_info, **kwargs)\n",
    "        \n",
    "    def get_data_lbl_random_walk_metadata(self, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, \n",
    "                                          prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, \n",
    "                                          degree_thresh=20, **kwargs):\n",
    "        data_meta, lbl_meta = Operations.get_random_walk_matrix(self.data.data_lbl, batch_size, walk_to, \n",
    "                                                                prob_reset, topk_thresh, degree_thresh)\n",
    "        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_meta, lbl_meta=lbl_meta,\n",
    "                                               meta_info=self.data.lbl_info, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6bffa-f910-49d2-8768-aaf3cdf3307e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc7f4e65-ec24-4ccd-b549-b294710f355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = SXCDataset(train_main, hlk_meta=train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34af6a17-4677-4432-b1e1-5b20ea85bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = train_dset.__getitems__([100, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc11f7-0ff7-43f7-9c20-81e8dd38a0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ebfa06c-8951-4807-b596-274c7a7ca2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2698e0e-046a-495b-b33f-0537a16f8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_idx',\n",
       " 'data_identifier',\n",
       " 'data_input_text',\n",
       " 'data_input_ids',\n",
       " 'data_attention_mask',\n",
       " 'plbl2data_idx',\n",
       " 'plbl2data_data2ptr',\n",
       " 'lbl2data_idx',\n",
       " 'lbl2data_data2ptr',\n",
       " 'lbl2data_identifier',\n",
       " 'lbl2data_input_text',\n",
       " 'lbl2data_input_ids',\n",
       " 'lbl2data_attention_mask',\n",
       " 'pneg2data_idx',\n",
       " 'pneg2data_data2ptr',\n",
       " 'neg2data_idx',\n",
       " 'neg2data_data2ptr',\n",
       " 'neg2data_identifier',\n",
       " 'neg2data_input_text',\n",
       " 'neg2data_input_ids',\n",
       " 'neg2data_attention_mask',\n",
       " 'phlk2data_idx',\n",
       " 'phlk2data_data2ptr',\n",
       " 'hlk2data_idx',\n",
       " 'hlk2data_data2ptr',\n",
       " 'hlk2data_identifier',\n",
       " 'hlk2data_input_text',\n",
       " 'hlk2data_input_ids',\n",
       " 'hlk2data_attention_mask',\n",
       " 'phlk2lbl_idx',\n",
       " 'phlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_idx',\n",
       " 'hlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_identifier',\n",
       " 'hlk2lbl_input_text',\n",
       " 'hlk2lbl_input_ids',\n",
       " 'hlk2lbl_attention_mask',\n",
       " 'hlk2lbl_data2ptr',\n",
       " 'phlk2lbl_data2ptr',\n",
       " 'phlk2neg_idx',\n",
       " 'phlk2neg_neg2ptr',\n",
       " 'hlk2neg_idx',\n",
       " 'hlk2neg_neg2ptr',\n",
       " 'hlk2neg_identifier',\n",
       " 'hlk2neg_input_text',\n",
       " 'hlk2neg_input_ids',\n",
       " 'hlk2neg_attention_mask',\n",
       " 'hlk2neg_data2ptr',\n",
       " 'phlk2neg_data2ptr']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76066769-9ad8-4517-a8f9-bc3959551919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c19d34-89af-44ba-b37b-35f542c1017d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SBaseXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40685068-05c1-47f3-b185-c6384266800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SBaseXCDataBlock(BaseXCDataBlock):\n",
    "    \n",
    "    @classmethod\n",
    "    @delegates(SXCDataset.from_file)\n",
    "    def from_file(cls, collate_fn:Callable=identity_collate_fn, **kwargs):\n",
    "        return cls(SXCDataset.from_file(**kwargs), collate_fn, **kwargs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f5540-8c21-425f-a4e9-15fa9df61d1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31ab7517-43b9-4f21-bdaa-a03c50f48aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_block = SBaseXCDataBlock(train_dset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cdac-4a66-4595-a633-a215b6669d93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09b24a62-8db4-4d46-88bf-3af0cc0ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataBlock(XCDataBlock):\n",
    "\n",
    "    @staticmethod\n",
    "    def inference_dset(data_info:Dict, data_lbl:sp.csr_matrix, lbl_info:Dict, data_lbl_filterer, \n",
    "                       **kwargs):\n",
    "        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)\n",
    "        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)\n",
    "        data_lbl[x_idx, y_idx] = 1\n",
    "        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])\n",
    "    \n",
    "        pred_dset = SXCDataset(SMainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,\n",
    "                                              data_lbl_filterer=data_lbl_filterer, **kwargs))\n",
    "        return pred_dset\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cfg(\n",
    "        cls, \n",
    "        cfg:Union[str,Dict],\n",
    "        collate_fn:Optional[Callable]=identity_collate_fn,\n",
    "        valid_pct:Optional[float]=0.2,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)\n",
    "\n",
    "        blocks = dict()\n",
    "        for split in ['train', 'valid', 'test', 'label']:\n",
    "            \n",
    "            if split in cfg['path']:\n",
    "                \n",
    "                params = cfg['parameters'].copy()\n",
    "                params.update(kwargs)\n",
    "                \n",
    "                if split != 'train': params['meta_dropout_remove'], params['meta_dropout_replace'] = None, None\n",
    "\n",
    "                if split != 'train' and 'train' in blocks:\n",
    "                    if 'lbl_info' not in cfg['path'][split]:\n",
    "                        cfg['path'][split]['lbl_info'] = blocks['train'].dset.data.lbl_info\n",
    "    \n",
    "                    if blocks['train'].dset.meta is not None:\n",
    "                        for meta_name in blocks['train'].dset.meta:\n",
    "                            if meta_name in cfg['path'][split] and 'meta_info' not in cfg['path'][split][meta_name]:\n",
    "                                cfg['path'][split][meta_name]['meta_info'] = blocks['train'].dset.meta[meta_name].meta_info\n",
    "                                \n",
    "                blocks[split] = SBaseXCDataBlock.from_file(**cfg['path'][split], **params, collate_fn=collate_fn)\n",
    "                \n",
    "        return cls(**blocks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa16f7-4370-450b-b43f-0e3a5f4ea481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed004a61-5ede-490a-85a5-5771fd33a53a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a21f-1de2-4b1f-ac65-eb401210d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.config import WIKISEEALSOTITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c260e-a02a-438a-a81c-c537523584cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock(train=train_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8194-6733-4938-a9cb-c278b4985a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_dset = block.linker_dset('hlk_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d91ba-9a68-4761-91d0-a5bdaf2ad782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194342b4-4515-4315-84e1-45124cd72afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f1099-f0ad-4df9-8ff5-2a5090685b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f3aaa-822f-48b6-9ed8-cf068d6cf539",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = WIKISEEALSOTITLES('/home/scai/phd/aiz218323/Projects/XC/data')['data_lnk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10505c9f-603a-440b-9f21-be93c4614210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transform_type': 'xc', 'smp_features': [('lbl2data', 1, 2), ('hlk2data', 1, 1), ('hlk2lbl2data', 2, 1)], 'pad_token': 0, 'oversample': False, 'sampling_features': [('lbl2data', 2), ('hlk2data', 1), ('hlk2lbl2data', 1)], 'num_labels': 1, 'num_metadata': 1, 'metadata_name': None, 'info_column_names': ['identifier', 'input_text'], 'use_tokenizer': True, 'tokenizer': 'bert-base-cased', 'tokenization_column': 'input_text', 'max_sequence_length': 32, 'padding': False, 'return_tensors': None, 'sep': '->', 'prompt_func': None, 'pad_side': 'right', 'drop': True, 'ret_t': True, 'in_place': True, 'collapse': True, 'device': 'cpu', 'inp': 'data', 'targ': 'lbl2data', 'ptr': 'lbl2data_data2ptr', 'n_lbl_samples': None, 'data_info_keys': None, 'lbl_info_keys': None, 'n_slbl_samples': 1, 'main_oversample': False, 'n_data_meta_samples': 1, 'n_lbl_meta_samples': 1, 'meta_info_keys': None, 'meta_oversample': False}\n"
     ]
    }
   ],
   "source": [
    "print(config['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5990f-6000-4473-8cda-53f73b2938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'return_tensors':'pt', 'padding':True}\n",
    "\n",
    "for k,v in params.items():\n",
    "    config['parameters'][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e74e2e-1321-4405-af56-b86805bdfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock.from_cfg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136c3ae-b689-479f-95d7-565c102b56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.dset.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c88bf-3b9b-4e04-a1a4-9afc5ab02b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  7302,  1204,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([  930, 27329]),\n",
       " 'lbl2data_data2ptr': tensor([1, 1]),\n",
       " 'lbl2data_identifier': ['Abstract_Window_Toolkit', 'Geography_of_Africa'],\n",
       " 'lbl2data_input_text': ['Abstract Window Toolkit', 'Geography of Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,   138,  4832, 15017, 24769,  6466, 10493,  2875,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plnk2data_idx': tensor([   762, 202927]),\n",
       " 'plnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_idx': tensor([   762, 202927]),\n",
       " 'lnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_identifier': ['Category:Free_computer_libraries',\n",
       "  'Category:Geography_of_the_Indian_Ocean'],\n",
       " 'lnk2data_input_text': ['Free computer libraries',\n",
       "  'Geography of the Indian Ocean'],\n",
       " 'lnk2data_input_ids': tensor([[  101,  4299,  2775,  9818,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 20678,  1104,  1103,  1890,  4879,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'lnk2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'lnk2data_attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'plnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'plnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'lnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_identifier': [],\n",
       " 'lnk2lbl_input_text': [],\n",
       " 'lnk2lbl_input_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_token_type_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_attention_mask': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_data2ptr': tensor([0, 0])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5f296-1e5c-464a-9db5-fbdc4df0a272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
