{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646426c7-93f9-4c40-adeb-7b04b74501a2",
   "metadata": {},
   "source": [
    "# SData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d09450-d08f-4b8c-988c-547430250692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d26b05-a557-4b6f-9500-f668a8d57b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a57087ab-f73b-47e5-bf51-494691acb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, inspect, numpy as np, scipy.sparse as sp\n",
    "from typing import Callable, Optional, Union, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from xcai.core import Filterer, Info\n",
    "from xcai.data import MainXCData, MetaXCData\n",
    "from xcai.data import BaseXCDataset, MainXCDataset, MetaXCDataset, MetaXCDatasets\n",
    "from xcai.data import BaseXCDataBlock\n",
    "from xcai.data import _read_sparse_file\n",
    "from xcai.graph.operations import *\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import *\n",
    "from plum import dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "752b92f9-98ac-4b2c-9c35-cc889d59b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7eb4-9022-4c05-abb9-3c056fb00cc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef80bda6-674a-4597-9dfd-fafa01f330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dir = '/Users/suchith720/Projects/data/(mapped)LF-WikiSeeAlsoTitles-320K'\n",
    "data_cfg = {\n",
    "    'info_column_names': ['identifier', 'input_text'],\n",
    "    'use_tokenizer': True,\n",
    "    'tokenizer': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
    "    'tokenization_column': 'input_text',\n",
    "    'main_max_data_sequence_length': 32,\n",
    "    'main_max_lbl_sequence_length': 32,\n",
    "    'meta_max_sequence_length': 32,\n",
    "    'padding': True,\n",
    "    'return_tensors': 'pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb62f1c-176a-4adf-8d0b-ce003a7d7a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "065e4bbf-0c93-4c30-9d96-e27a10822624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_cfg = {\n",
    "    'data_lbl': f'{dset_dir}/trn_X_Y.txt',\n",
    "    'data_info': f'{dset_dir}/raw_data/train.raw.txt',\n",
    "    'lbl_info': f'{dset_dir}/raw_data/label.raw.txt',\n",
    "    'data_lbl_filterer': f'{dset_dir}/filter_labels_train.txt',\n",
    "}\n",
    "train_data = MainXCData.from_file(**train_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ec403-3c17-4102-8816-c3f8a4d77b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a79c584-be9a-48bc-a740-242d6168bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scai/phd/aiz218323/.local/lib/python3.10/site-packages/xclib-0.97-py3.10-linux-x86_64.egg/xclib/data/data_utils.py:263: UserWarning: Header mis-match from inferred shape!\n",
      "  warnings.warn(\"Header mis-match from inferred shape!\")\n"
     ]
    }
   ],
   "source": [
    "meta_cfg = {\n",
    "    'prefix': 'hlk',\n",
    "    'data_meta': f'{dset_dir}/hyper_link_trn_X_Y.txt',\n",
    "    'lbl_meta': f'{dset_dir}/hyper_link_lbl_X_Y.txt',\n",
    "    'meta_info': f'{dset_dir}/raw_data/hyper_link.raw.txt',\n",
    "}\n",
    "meta_data = MetaXCData.from_file(**meta_cfg, **data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bd28b7-d91c-4c41-bafa-9bcec00ff6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identity_collate_fn(batch): return BatchEncoding(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df2464-461a-4389-99d6-23abf6d6eafa",
   "metadata": {},
   "source": [
    "## SDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26266ee9-e981-41e3-b485-69ddd17d60af",
   "metadata": {},
   "source": [
    "### `SMainXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0d1d23-3b05-4516-b48d-bd81ef6299b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMainXCDataset(MainXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_slbl_samples:Optional[int]=1,\n",
    "        main_oversample:Optional[bool]=False,\n",
    "        use_main_distribution:Optional[bool]=False,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_slbl_samples,main_oversample,use_main_distribution,return_scores')\n",
    "        \n",
    "        self.data_lbl_scores = None\n",
    "        if use_main_distribution or return_scores: self._store_scores()\n",
    "\n",
    "    def _get_dataset(\n",
    "        self, \n",
    "        data_info:Dict, \n",
    "        data_lbl:Optional[sp.csr_matrix]=None, \n",
    "        lbl_info:Optional[Dict]=None, \n",
    "        data_lbl_filterer:Optional[Union[sp.csr_matrix,np.array]]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        n_lbl_samples = kwargs.get('n_lbl_samples') if 'n_lbl_samples' in kwargs else self.n_lbl_samples\n",
    "        data_info_keys = kwargs.get('data_info_keys') if 'data_info_keys' in kwargs else self.data_info_keys\n",
    "        lbl_info_keys = kwargs.get('lbl_info_keys') if 'lbl_info_keys' in kwargs else self.lbl_info_keys\n",
    "        n_slbl_samples = kwargs.get('n_slbl_samples') if 'n_slbl_samples' in kwargs else self.n_slbl_samples\n",
    "        main_oversample = kwargs.get('main_oversample') if 'main_oversample' in kwargs else self.main_oversample\n",
    "        use_main_distribution = kwargs.get('use_main_distribution') if 'use_main_distribution' in kwargs else self.use_main_distribution\n",
    "        return_scores = kwargs.get('return_scores') if 'return_scores' in kwargs else self.return_scores\n",
    "        \n",
    "        return SMainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info, data_lbl_filterer=data_lbl_filterer, \n",
    "                              n_lbl_samples=n_lbl_samples, data_info_keys=data_info_keys, lbl_info_keys=lbl_info_keys, \n",
    "                              n_slbl_samples=n_slbl_samples, main_oversample=main_oversample, use_main_distribution=use_main_distribution, \n",
    "                              return_scores=return_scores)\n",
    "        \n",
    "    def _store_scores(self):\n",
    "        if self.data_lbl is not None:\n",
    "            if self.use_main_distribution:\n",
    "                data_lbl = self.data_lbl / (self.data_lbl.sum(axis=1) + 1e-9)\n",
    "                data_lbl = data_lbl.tocsr()\n",
    "            else:\n",
    "                data_lbl = self.data_lbl\n",
    "            self.data_lbl_scores = [o.data.tolist() for o in data_lbl]\n",
    "            \n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = {'data_idx': torch.tensor(idxs, dtype=torch.int64)}\n",
    "        x.update(self.get_info('data', idxs, self.data_info, self.data_info_keys))\n",
    "        if self.data_lbl is not None:\n",
    "            prefix = 'lbl2data'\n",
    "            o = self.extract_items(prefix, self.curr_data_lbl, idxs, self.n_lbl_samples, self.n_slbl_samples, self.main_oversample, \n",
    "                                   self.lbl_info, self.lbl_info_keys, self.use_main_distribution, self.data_lbl_scores, \n",
    "                                   return_scores=self.return_scores)\n",
    "            x.update(o)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(MainXCData.from_file)\n",
    "    def from_file(\n",
    "        cls, \n",
    "        n_lbl_samples:Optional[int]=None,\n",
    "        data_info_keys:Optional[List]=None,\n",
    "        lbl_info_keys:Optional[List]=None,\n",
    "        n_slbl_samples:Optional[int]=1,\n",
    "        main_oversample:Optional[bool]=False,\n",
    "        use_main_distribution:Optional[bool]=False,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return cls(**MainXCData.from_file(**kwargs), n_lbl_samples=n_lbl_samples, data_info_keys=data_info_keys, \n",
    "                   lbl_info_keys=lbl_info_keys, n_slbl_samples=n_slbl_samples, main_oversample=main_oversample, \n",
    "                   use_main_distribution=use_main_distribution, return_scores=return_scores)\n",
    "\n",
    "    def _getitems(cls, idxs:List):\n",
    "        return SMainXCDataset(\n",
    "            data_info={k:v[idxs] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[idx] for idx in idxs] for k,v in cls.data_info.items()}, \n",
    "            data_lbl=cls.data_lbl[idxs] if cls.data_lbl is not None else None, \n",
    "            lbl_info=cls.lbl_info, \n",
    "            data_lbl_filterer=Filterer.sample(cls.data_lbl_filterer, sz=cls.data_lbl.shape, idx=idxs) if cls.data_lbl_filterer is not None else None,\n",
    "            n_lbl_samples=cls.n_lbl_samples,\n",
    "            data_info_keys=cls.data_info_keys,\n",
    "            lbl_info_keys=cls.lbl_info_keys,\n",
    "            n_slbl_samples=cls.n_slbl_samples,\n",
    "            main_oversample=cls.main_oversample,\n",
    "            use_main_distribution=cls.use_main_distribution,\n",
    "            return_scores=cls.return_scores,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf32d6c-ecb1-4af8-a070-ebb82b5c11d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d1a90-f3ae-44b2-bd20-1866331886ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main = SMainXCDataset(**train_data, n_slbl_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dba53-bb5d-4494-a123-3a3b38a71532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  6207,  2102,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 10505,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([ 927,  928, 1471, 1470]),\n",
       " 'lbl2data_data2ptr': tensor([2, 2]),\n",
       " 'lbl2data_identifier': ['Application_posture',\n",
       "  'Bookmarklet',\n",
       "  'Outline_of_Africa',\n",
       "  'List_of_national_parks_in_Africa'],\n",
       " 'lbl2data_input_text': ['Application posture',\n",
       "  'Bookmarklet',\n",
       "  'Outline of Africa',\n",
       "  'List of national parks in Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,  4646, 16819,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2338, 10665,  7485,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12685,  1997,  3088,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2120,  6328,  1999,  3088,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_main.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b7144-619b-454b-977c-e17f6b32a321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8ce8c-3933-4bdf-834c-cb38acfc615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main.oversample = True\n",
    "train_main.n_slbl_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63c977-2561-4b86-b0bb-8b88339fce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    import pdb; pdb.set_trace()\n",
    "    train_main.__getitems__([1,2,3,4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7cbd3-cd4e-4f08-b4f6-4e26db6185ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_main, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3aac4-38ba-4853-927d-dfe26083b701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'data_identifier': ['Anarchism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Academy_Awards',\n",
       "  'International_Atomic_Time',\n",
       "  'Altruism',\n",
       "  'Allan_Dwan',\n",
       "  'Anthropology',\n",
       "  'Agricultural_science',\n",
       "  'Animation'],\n",
       " 'data_input_text': ['Anarchism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Academy Awards',\n",
       "  'International Atomic Time',\n",
       "  'Altruism',\n",
       "  'Allan Dwan',\n",
       "  'Anthropology',\n",
       "  'Agricultural science',\n",
       "  'Animation'],\n",
       " 'data_input_ids': tensor([[  101,  9617, 11140,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 19465,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 17484,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2914,  2982,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2248,  9593,  2051,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12456,  6820,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  8926,  1040,  7447,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12795,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4910,  2671,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  7284,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([    0,     1,     2,     3,     9, 26766,    12,    13,    14,    15,\n",
       "            16,    17,    18, 56258,    19,    20,    21,    22,    23,    24,\n",
       "            25,    26,    27,    28,    29,    30,    31,    32,    33,    34,\n",
       "            35,    36,    37,    38,    39,    40,    41,    42, 10243,    45,\n",
       "            48,    49,    50,    51,    52,    53,    54,    55,    56,    57,\n",
       "            58,    59,    60,    61,    62,    63,    64,    65,    66,    67,\n",
       "            68,    69,    70, 81953,   101,   102,   103,   104,   105]),\n",
       " 'plbl2data_data2ptr': tensor([ 3,  1,  2,  4,  4, 25,  1, 14, 10,  5]),\n",
       " 'lbl2data_idx': tensor([    1,     0,     3, 26766,     9,    12,    13,    16,    17, 10243,\n",
       "            21,    45,    56,    59,    64,    62,   101,   104]),\n",
       " 'lbl2data_data2ptr': tensor([2, 1, 2, 2, 2, 2, 1, 2, 2, 2]),\n",
       " 'lbl2data_identifier': ['Libertarian_socialism',\n",
       "  'Antinomianism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Conimbricenses',\n",
       "  'List_of_film_awards',\n",
       "  'List_of_Academy_Award_records',\n",
       "  'Clock_synchronization',\n",
       "  'Network_Time_Protocol',\n",
       "  'Altruism',\n",
       "  'Earning_to_give',\n",
       "  'Canadian_pioneers_in_early_Hollywood',\n",
       "  'Memetics',\n",
       "  'Prehistoric_medicine',\n",
       "  'Agroecology',\n",
       "  'Agricultural_sciences_basic_topics',\n",
       "  '12_basic_principles_of_animation',\n",
       "  'List_of_motion_picture_topics'],\n",
       " 'lbl2data_input_text': ['Libertarian socialism',\n",
       "  'Antinomianism',\n",
       "  'Autism',\n",
       "  'Aristotle',\n",
       "  'Conimbricenses',\n",
       "  'List of film awards',\n",
       "  'List of Academy Award records',\n",
       "  'Clock synchronization',\n",
       "  'Network Time Protocol',\n",
       "  'Altruism',\n",
       "  'Earning to give',\n",
       "  'Canadian pioneers in early Hollywood',\n",
       "  'Memetics',\n",
       "  'Prehistoric medicine',\n",
       "  'Agroecology',\n",
       "  'Agricultural sciences basic topics',\n",
       "  '12 basic principles of animation',\n",
       "  'List of motion picture topics'],\n",
       " 'lbl2data_input_ids': tensor([[  101, 19297, 14649,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3424,  3630, 20924,  2964,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 19465,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 17484,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  9530,  5714, 23736, 19023,  2229,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2143,  2982,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  2914,  2400,  2636,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  5119, 26351,  8093, 10698,  9276,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2897,  2051,  8778,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12456,  6820,  2964,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  7414,  2000,  2507,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  3010, 13200,  1999,  2220,  5365,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2033, 11368,  6558,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 14491,  4200,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 12943,  3217,  8586,  6779,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  4910,  4163,  3937,  7832,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2260,  3937,  6481,  1997,  7284,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  2862,  1997,  4367,  3861,  7832,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138603c1-ccec-4fc9-91d5-4470d4ff8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(data_cfg['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc2eef-e8f7-47ad-becc-30e910f9bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['plbl2data_data2ptr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57589218-fc9d-44c3-aea3-60992791b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.batch_decode(batch['data_input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ff3e2-a50b-41e3-a53f-be9cb818a402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SMetaXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4215992-e99e-4fc3-b20c-4b639f547517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMetaXCDataset(MetaXCDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_sdata_meta_samples:Optional[int]=1,\n",
    "        n_slbl_meta_samples:Optional[int]=1,\n",
    "        meta_oversample:Optional[bool]=False,\n",
    "        use_meta_distribution:Optional[bool]=False,\n",
    "        meta_dropout_remove:Optional[float]=None,\n",
    "        meta_dropout_replace:Optional[float]=None,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        store_attr('n_sdata_meta_samples,n_slbl_meta_samples,meta_oversample,use_meta_distribution')\n",
    "        store_attr('meta_dropout_remove,meta_dropout_replace,return_scores')\n",
    "\n",
    "        self.data_meta_scores, self.lbl_meta_scores = None, None\n",
    "        if use_meta_distribution or return_scores: self._store_scores()\n",
    "\n",
    "    def _store_scores(self):\n",
    "        if self.data_meta is not None:\n",
    "            if self.use_meta_distribution:\n",
    "                data_meta = self.data_meta / (self.data_meta.sum(axis=1) + 1e-9)\n",
    "                data_meta = data_meta.tocsr()\n",
    "            else:\n",
    "                data_meta = self.data_meta\n",
    "            self.data_meta_scores = [o.data.tolist() for o in data_meta]\n",
    "            \n",
    "        if self.lbl_meta is not None:\n",
    "            if self.use_meta_distribution:\n",
    "                lbl_meta = self.lbl_meta / (self.lbl_meta.sum(axis=1) + 1e-9)\n",
    "                lbl_meta = lbl_meta.tocsr()\n",
    "            else:\n",
    "                lbl_meta = self.lbl_meta\n",
    "            self.lbl_meta_scores = [o.data.tolist() for o in lbl_meta]\n",
    "\n",
    "    def _getitems(self, idxs:List):\n",
    "        return SMetaXCDataset(prefix=self.prefix, data_meta=self.data_meta[idxs], lbl_meta=self.lbl_meta, meta_info=self.meta_info, \n",
    "                              n_data_meta_samples=self.n_data_meta_samples, n_lbl_meta_samples=self.n_lbl_meta_samples, \n",
    "                              meta_info_keys=self.meta_info_keys, n_sdata_meta_samples=self.n_sdata_meta_samples, \n",
    "                              n_slbl_meta_samples=self.n_slbl_meta_samples, meta_oversample=self.meta_oversample, \n",
    "                              use_meta_distribution=self.use_meta_distribution, meta_dropout_remove=self.meta_dropout_remove, \n",
    "                              meta_dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "\n",
    "    def _sample_meta_items(self, idxs:List):\n",
    "        assert max(idxs) < self.n_meta, f\"indices should be less than {self.n_meta}\"\n",
    "        meta_info = {k: [v[i] for i in idxs] for k,v in self.meta_info.items()}\n",
    "        return SMetaXCDataset(prefix=self.prefix, data_meta=self.data_meta[:, idxs], lbl_meta=self.lbl_meta[:, idxs], meta_info=meta_info, \n",
    "                              n_data_meta_samples=self.n_data_meta_samples, n_lbl_meta_samples=self.n_lbl_meta_samples,\n",
    "                              meta_info_keys=self.meta_info_keys, n_sdata_meta_samples=self.n_sdata_meta_samples, \n",
    "                              n_slbl_meta_samples=self.n_slbl_meta_samples, meta_oversample=self.meta_oversample, \n",
    "                              use_meta_distribution=self.use_meta_distribution, meta_dropout_remove=self.meta_dropout_remove, \n",
    "                              meta_dropout_replace=self.meta_dropout_replace, return_scores=self.return_scores)\n",
    "        \n",
    "    @classmethod\n",
    "    @delegates(MetaXCData.from_file)\n",
    "    def from_file(\n",
    "        cls, \n",
    "        n_data_meta_samples:Optional[int]=None, \n",
    "        n_lbl_meta_samples:Optional[int]=None, \n",
    "        meta_info_keys:Optional[List]=None,\n",
    "        n_sdata_meta_samples:Optional[int]=1,\n",
    "        n_slbl_meta_samples:Optional[int]=1,\n",
    "        meta_oversample:Optional[bool]=False,\n",
    "        use_meta_distribution:Optional[bool]=False,\n",
    "        meta_dropout_remove:Optional[float]=None,\n",
    "        meta_dropout_replace:Optional[float]=None,\n",
    "        return_scores:Optional[bool]=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return cls(**MetaXCData.from_file(**kwargs), n_data_meta_samples=n_data_meta_samples, n_lbl_meta_samples=n_lbl_meta_samples, \n",
    "                   meta_info_keys=meta_info_keys, n_sdata_meta_samples=n_sdata_meta_samples, n_slbl_meta_samples=n_slbl_meta_samples,\n",
    "                   meta_oversample=meta_oversample, use_meta_distribution=use_meta_distribution, meta_dropout_remove=meta_dropout_remove, \n",
    "                   meta_dropout_replace=meta_dropout_replace, return_scores=return_scores)\n",
    "\n",
    "    def get_data_meta(self, idxs:List):\n",
    "        x, prefix = dict(), f'{self.prefix}2data'\n",
    "        o = self.extract_items(prefix, self.curr_data_meta, idxs, self.n_data_meta_samples, self.n_sdata_meta_samples, self.meta_oversample, \n",
    "                               self.meta_info, self.meta_info_keys, self.use_meta_distribution, self.data_meta_scores, \n",
    "                               dropout_remove=self.meta_dropout_remove, dropout_replace=self.meta_dropout_replace, \n",
    "                               return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "        \n",
    "    def get_lbl_meta(self, idxs:List):\n",
    "        x, prefix = dict(), f'{self.prefix}2lbl'\n",
    "        o = self.extract_items(prefix, self.curr_lbl_meta, idxs, self.n_lbl_meta_samples, self.n_slbl_meta_samples, self.meta_oversample, \n",
    "                               self.meta_info, self.meta_info_keys, self.use_meta_distribution, self.lbl_meta_scores, \n",
    "                               dropout_remove=self.meta_dropout_remove, dropout_replace=self.meta_dropout_replace, \n",
    "                               return_scores=self.return_scores)\n",
    "        x.update(o)\n",
    "        return x\n",
    "\n",
    "    def _verify_inputs(cls):\n",
    "        cls.n_data,cls.n_meta = cls.data_meta.shape[0],cls.data_meta.shape[1]\n",
    "        \n",
    "        if cls.lbl_meta is not None:\n",
    "            cls.n_lbl = cls.lbl_meta.shape[0]\n",
    "            if cls.lbl_meta.shape[1] != cls.n_meta:\n",
    "                raise ValueError(f'`lbl_meta`({cls.lbl_meta.shape[1]}) should have same number of columns as `data_meta`({cls.n_meta}).')\n",
    "    \n",
    "        if cls.meta_info is not None:\n",
    "            n_meta = cls._verify_info(cls.meta_info)\n",
    "            if n_meta != cls.n_meta:\n",
    "                raise ValueError(f'`meta_info`({n_meta}) should have same number of entries as number of columns of `data_meta`({cls.n_meta})')\n",
    "            if cls.meta_info_keys is None: cls.meta_info_keys = list(cls.meta_info.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b40080-e07e-4d00-929f-77247cf2f1d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456f40c-89f4-445f-b3c2-1f74ed54b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = SMetaXCDataset(**meta_data, n_sdata_meta_samples=2, n_slbl_meta_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a9b58-8edd-4dd6-b78b-5d0df121bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.meta_oversample = True\n",
    "train_meta.n_sdata_meta_samples = 3\n",
    "train_meta.n_slbl_meta_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418f48e-3278-4f39-9f6d-62c4f408c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.get_data_meta([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b58370-cc74-4605-9932-acc737beea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta.get_lbl_meta([101, 201])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c371b7-6fd2-4561-be7a-790ce4695f59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SXCDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0e4cf79-14dc-4228-b00b-b1d0f22be966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataset(BaseXCDataset):\n",
    "\n",
    "    def __init__(self, data:SMainXCDataset, **kwargs):\n",
    "        super().__init__()\n",
    "        self.data, self.meta = data, MetaXCDatasets({k:kwargs[k] for k in self.get_meta_args(**kwargs) if isinstance(kwargs[k], SMetaXCDataset)})\n",
    "        self._verify_inputs()\n",
    "\n",
    "    def _getitems(self, idxs:List):\n",
    "        return SXCDataset(self.data._getitems(idxs), **{k:meta._getitems(idxs) for k,meta in self.meta.items()})\n",
    "        \n",
    "    @classmethod\n",
    "    @delegates(SMainXCDataset.from_file)\n",
    "    def from_file(cls, **kwargs):\n",
    "        data = SMainXCDataset.from_file(**kwargs)\n",
    "        meta_kwargs = {o:kwargs.pop(o) for o in cls.get_meta_args(**kwargs)}\n",
    "        meta = {k:SMetaXCDataset.from_file(**v, **kwargs) for k,v in meta_kwargs.items()}\n",
    "        return cls(data, **meta)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_meta_args(**kwargs):\n",
    "        return [k for k in kwargs if re.match(r'.*_meta$', k)]\n",
    "\n",
    "    def _verify_inputs(self):\n",
    "        self.n_data, self.n_lbl = self.data.n_data, self.data.n_lbl\n",
    "        if len(self.meta):\n",
    "            self.n_meta = len(self.meta)\n",
    "            for meta in self.meta.values():\n",
    "                if meta.n_data != self.n_data: \n",
    "                    raise ValueError(f'`meta`({meta.n_data}) and `data`({self.n_data}) should have the same number of datapoints.')\n",
    "                if self.n_lbl is not None and meta.n_lbl != self.n_lbl: \n",
    "                    raise ValueError(f'`meta`({meta.n_lbl}) and `data`({self.n_lbl}) should have the same number of labels.')\n",
    "\n",
    "    def __getitems__(self, idxs:List):\n",
    "        x = self.data.__getitems__(idxs)\n",
    "        if self.n_meta:\n",
    "            for meta in self.meta.values():\n",
    "                x.update(meta.get_data_meta(idxs))\n",
    "                if self.n_lbl:\n",
    "                    z = meta.get_lbl_meta(x['lbl2data_idx'])\n",
    "                    z[f'{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                    z[f'p{meta.prefix}2lbl_data2ptr'] = torch.tensor([o.sum() for o in z[f'p{meta.prefix}2lbl_lbl2ptr'].split_with_sizes(x[f'lbl2data_data2ptr'].tolist())])\n",
    "                    x.update(z)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def lbl_info(self): return self.data.lbl_info\n",
    "\n",
    "    @property\n",
    "    def lbl_dset(self): return SMainXCDataset(data_info=self.data.lbl_info, n_lbl_samples=self.data.n_lbl_samples, \n",
    "                                               data_info_keys=self.data.data_info_keys, lbl_info_keys=self.data.lbl_info_keys, \n",
    "                                               n_slbl_samples=self.data.n_slbl_samples, main_oversample=self.data.main_oversample, \n",
    "                                               use_main_distribution=self.data.use_main_distribution, return_scores=self.data.return_scores)\n",
    "        \n",
    "    def lbl_meta_dset(self, meta_name):\n",
    "        m = self.meta[f'{meta_name}_meta']\n",
    "        return SMetaXCDataset(prefix=m.prefix, data_meta=m.lbl_meta, lbl_meta=m.lbl_meta, meta_info=m.meta_info, \n",
    "                              n_data_meta_samples=m.n_data_meta_samples, n_lbl_meta_samples=m.n_lbl_meta_samples, \n",
    "                              meta_info_keys=m.meta_info_keys, n_sdata_meta_samples=m.n_sdata_meta_samples, \n",
    "                              n_slbl_meta_samples=m.n_slbl_meta_samples, meta_oversample=m.meta_oversample, \n",
    "                              use_meta_distribution=m.use_meta_distribution, meta_dropout_remove=m.meta_dropout_remove, \n",
    "                              meta_dropout_replace=m.meta_dropout_replace, return_scores=m.return_scores)\n",
    "        \n",
    "    @property\n",
    "    def data_info(self): return self.data.data_info\n",
    "\n",
    "    @property\n",
    "    def data_dset(self): return SMainXCDataset(data_info=self.data.data_info, n_lbl_samples=self.data.n_lbl_samples, \n",
    "                                               data_info_keys=self.data.data_info_keys, lbl_info_keys=self.data.lbl_info_keys, \n",
    "                                               n_slbl_samples=self.data.n_slbl_samples, main_oversample=self.data.main_oversample, \n",
    "                                               use_main_distribution=self.data.use_main_distribution, return_scores=self.data.return_scores)\n",
    "        \n",
    "    def data_meta_dset(self, meta_name):\n",
    "        return self.meta[f'{meta_name}_meta']\n",
    "        \n",
    "    def one_batch(self, bsz:Optional[int]=10, seed:Optional[int]=None):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        idxs = list(torch.randperm(len(self)).numpy())[:bsz]\n",
    "        return [self[idx] for idx in idxs]\n",
    "\n",
    "    def get_one_hop_metadata(self, batch_size:Optional[int]=1024, thresh:Optional[int]=10, topk:Optional[int]=10, **kwargs):\n",
    "        data_lbl = Graph.threshold_on_degree(self.data.data_lbl, thresh=thresh)\n",
    "        data_meta, lbl_meta = Graph.one_hop_matrix(data_lbl, batch_size=batch_size, topk=topk, do_normalize=True)\n",
    "        self.meta['ohm_meta'] = SMetaXCDataset(prefix='ohm', data_meta=data_meta, lbl_meta=lbl_meta, \n",
    "                                               meta_info=self.data.lbl_info, **kwargs)\n",
    "\n",
    "    def get_random_walk_metadata(self, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, prob_reset:Optional[float]=0.8, \n",
    "                                 topk_thresh:Optional[int]=10, degree_thresh=20, **kwargs):\n",
    "        data_meta = perform_random_walk(data_lbl, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                        n_hops=1, thresh=degree_thresh, topk=topk_thresh, do_normalize=True)\n",
    "        lbl_meta = perform_random_walk(data_lbl.transpose().tocsr(), batch_size=batch_size, walk_to=walk_to, \n",
    "                                       prob_reset=prob_reset, n_hops=2, thresh=degree_thresh, topk=topk_thresh, do_normalize=True)\n",
    "        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_meta, lbl_meta=lbl_meta,\n",
    "                                               meta_info=self.data.lbl_info, **kwargs)\n",
    "\n",
    "    def get_random_walk_with_matrices_metadata(self, meta_name:str, batch_size:Optional[int]=1024, walk_to:Optional[int]=100, \n",
    "                                               prob_reset:Optional[float]=0.8, topk_thresh:Optional[int]=10, data_degree_thresh=20, \n",
    "                                               lbl_degree_thresh=20, **kwargs):\n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "        data_meta, lbl_meta = self.meta[f'{meta_name}_meta'].data_meta, self.meta[f'{meta_name}_meta'].lbl_meta\n",
    "        data_rnw = perform_random_walk_with_matrices(data_meta, lbl_meta, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                                     n_hops=2, data_thresh=data_degree_thresh, lbl_thresh=lbl_degree_thresh, \n",
    "                                                     topk=topk_thresh, do_normalize=True)\n",
    "        lbl_rnw = perform_random_walk_with_matrices(lbl_meta, data_meta, batch_size=batch_size, walk_to=walk_to, prob_reset=prob_reset, \n",
    "                                                    n_hops=3, data_thresh=data_degree_thresh, lbl_thresh=lbl_degree_thresh, \n",
    "                                                    topk=topk_thresh, do_normalize=True)\n",
    "        self.meta['rnw_meta'] = SMetaXCDataset(prefix='rnw', data_meta=data_rnw, lbl_meta=lbl_rnw, meta_info=self.data.lbl_info, **kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def combine_info(info_1:Dict, info_2:Dict, pad_token:int=0):\n",
    "        comb_info = dict()\n",
    "        for k,v in info_1.items():\n",
    "            if isinstance(v, tuple) or isinstance(v, list): comb_info[k] = v + info_2[k]\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                n_data = v.shape[0] + info_2[k].shape[0]\n",
    "                seq_len = max(v.shape[1], info_2[k].shape[1]) \n",
    "                \n",
    "                if k == 'input_ids': \n",
    "                    info = torch.full((n_data, seq_len), pad_token, dtype=v.dtype)\n",
    "                elif k == 'attention_mask': \n",
    "                    info = torch.full((n_data, seq_len), 0, dtype=v.dtype)\n",
    "                    \n",
    "                info[:v.shape[0], :v.shape[1]] = v\n",
    "                info[v.shape[0]:, :info_2[k].shape[1]] = info_2[k]\n",
    "                \n",
    "                comb_info[k] = info\n",
    "        return comb_info\n",
    "\n",
    "    def _get_main_dataset(\n",
    "        self,\n",
    "        data_info:Dict, \n",
    "        data_lbl:Optional[sp.csr_matrix]=None, \n",
    "        lbl_info:Optional[Dict]=None, \n",
    "        data_lbl_filterer:Optional[Union[sp.csr_matrix,np.array]]=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        dset = self.data._get_dataset(data_info, data_lbl, lbl_info, data_lbl_filterer, **kwargs)\n",
    "        return SXCDataset(dset)\n",
    "        \n",
    "    def combine_lbl_and_meta(self, meta_name:str, pad_token:int=0, p_data=0.5, **kwargs): \n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "            \n",
    "        data_lbl = self.data.data_lbl\n",
    "        data_lbl = data_lbl.multiply(1/(data_lbl.getnnz(axis=1).reshape(-1, 1) + 1e-9))\n",
    "        data_lbl = data_lbl.tocsr() * p_data\n",
    "        \n",
    "        data_meta = self.meta[f'{meta_name}_meta'].data_meta\n",
    "        data_meta = data_meta.multiply(1/(data_meta.getnnz(axis=1).reshape(-1, 1) + 1e-9))\n",
    "        data_meta = data_meta.tocsr() * (1 - p_data)\n",
    "        \n",
    "        lbl_info = self.data.lbl_info\n",
    "        meta_info = self.meta[f'{meta_name}_meta'].meta_info\n",
    "\n",
    "        comb_info = self.combine_info(lbl_info, meta_info, pad_token)\n",
    "        \n",
    "        return self._get_main_dataset(self.data.data_info, sp.hstack([data_lbl, data_meta]), comb_info, \n",
    "                                      self.data.data_lbl_filterer, **kwargs)\n",
    "\n",
    "    def combine_data_and_meta(self, meta_name:str, pad_token:int=0, **kwargs):\n",
    "        if f'{meta_name}_meta' not in self.meta: raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "        \n",
    "        data_lbl, meta_lbl = self.data.data_lbl, self.meta[f'{meta_name}_meta'].lbl_meta.transpose().tocsr()\n",
    "        assert data_lbl.shape[1] == meta_lbl.shape[1], f\"Incompatible metadata shape: {meta_lbl.shape}\"\n",
    "\n",
    "        data_info = self.data.data_info\n",
    "        meta_info = self.meta[f'{meta_name}_meta'].meta_info\n",
    "        comb_info = self.combine_info(data_info, meta_info, pad_token)\n",
    "\n",
    "        dset = self._get_main_dataset(comb_info, sp.vstack([data_lbl, meta_lbl]), self.data.lbl_info, \n",
    "                                      self.data.data_lbl_filterer, **kwargs)\n",
    "        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]\n",
    "        return dset._getitems(valid_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_combined_data_and_meta(dset, meta_lbl:sp.csr_matrix, meta_info:Dict, pad_token:int=0, **kwargs):    \n",
    "        data_lbl = dset.data.data_lbl\n",
    "        assert data_lbl.shape[1] == meta_lbl.shape[1], f\"Incompatible metadata shape: {meta_lbl.shape}\"\n",
    "        \n",
    "        data_info = dset.data.data_info\n",
    "        comb_info = dset.combine_info(data_info, meta_info, pad_token)\n",
    "        \n",
    "        dset = dset._get_main_dataset(comb_info, sp.vstack([data_lbl, meta_lbl]), dset.data.lbl_info, \n",
    "                                      dset.data.data_lbl_filterer, **kwargs)\n",
    "        valid_idx = np.where(dset.data.data_lbl.getnnz(axis=1) > 0)[0]\n",
    "        return dset._getitems(valid_idx)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6bffa-f910-49d2-8768-aaf3cdf3307e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f4e65-ec24-4ccd-b549-b294710f355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = SXCDataset(train_main, hlk_meta=train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af6a17-4677-4432-b1e1-5b20ea85bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = train_dset.__getitems__([100, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfa06c-8951-4807-b596-274c7a7ca2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=10, collate_fn=identity_collate_fn)\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2698e0e-046a-495b-b33f-0537a16f8af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_idx',\n",
       " 'data_identifier',\n",
       " 'data_input_text',\n",
       " 'data_input_ids',\n",
       " 'data_token_type_ids',\n",
       " 'data_attention_mask',\n",
       " 'plbl2data_idx',\n",
       " 'plbl2data_data2ptr',\n",
       " 'lbl2data_idx',\n",
       " 'lbl2data_data2ptr',\n",
       " 'lbl2data_identifier',\n",
       " 'lbl2data_input_text',\n",
       " 'lbl2data_input_ids',\n",
       " 'lbl2data_token_type_ids',\n",
       " 'lbl2data_attention_mask',\n",
       " 'phlk2data_idx',\n",
       " 'phlk2data_data2ptr',\n",
       " 'hlk2data_idx',\n",
       " 'hlk2data_data2ptr',\n",
       " 'hlk2data_identifier',\n",
       " 'hlk2data_input_text',\n",
       " 'hlk2data_input_ids',\n",
       " 'hlk2data_token_type_ids',\n",
       " 'hlk2data_attention_mask',\n",
       " 'phlk2lbl_idx',\n",
       " 'phlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_idx',\n",
       " 'hlk2lbl_lbl2ptr',\n",
       " 'hlk2lbl_identifier',\n",
       " 'hlk2lbl_input_text',\n",
       " 'hlk2lbl_input_ids',\n",
       " 'hlk2lbl_token_type_ids',\n",
       " 'hlk2lbl_attention_mask',\n",
       " 'hlk2lbl_data2ptr',\n",
       " 'phlk2lbl_data2ptr']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76066769-9ad8-4517-a8f9-bc3959551919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c19d34-89af-44ba-b37b-35f542c1017d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `SBaseXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40685068-05c1-47f3-b185-c6384266800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SBaseXCDataBlock(BaseXCDataBlock):\n",
    "\n",
    "    @delegates(DataLoader.__init__)\n",
    "    def __init__(\n",
    "        self, \n",
    "        dset:SXCDataset, \n",
    "        collate_fn:Optional[Callable]=identity_collate_fn,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.dset, self.dl_kwargs, self.collate_fn = dset, self._get_dl_kwargs(**kwargs), collate_fn\n",
    "        self.dl = DataLoader(dset, collate_fn=collate_fn, **self.dl_kwargs) if collate_fn is not None else None\n",
    "\n",
    "    def _get_dl_kwargs(self, **kwargs):\n",
    "        dl_params = inspect.signature(DataLoader.__init__).parameters\n",
    "        return {k:v for k,v in kwargs.items() if k in dl_params}\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(SXCDataset.from_file)\n",
    "    def from_file(cls, collate_fn:Callable=identity_collate_fn, **kwargs):\n",
    "        return SBaseXCDataBlock(SXCDataset.from_file(**kwargs), collate_fn, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dset)    \n",
    "    \n",
    "    def _getitems(self, idxs:List):\n",
    "        return SBaseXCDataBlock(self.dset._getitems(idxs), collate_fn=self.collate_fn, **self.dl_kwargs)\n",
    "\n",
    "    @property\n",
    "    def bsz(self): return self.dl.batch_size\n",
    "\n",
    "    @bsz.setter\n",
    "    def bsz(self, v):\n",
    "        self.dl_kwargs['batch_size'] = v\n",
    "        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None\n",
    "\n",
    "    @property\n",
    "    def data_lbl_filterer(self): return self.dset.data.data_lbl_filterer\n",
    "\n",
    "    @data_lbl_filterer.setter\n",
    "    def data_lbl_filterer(self, val): self.dset.data.data_lbl_filterer = val\n",
    "\n",
    "    @dispatch\n",
    "    def one_batch(self):\n",
    "        return next(iter(self.dl))\n",
    "\n",
    "    @dispatch\n",
    "    def one_batch(self, bsz:int):\n",
    "        self.dl_kwargs['batch_size'] = bsz\n",
    "        self.dl = DataLoader(self.dset, collate_fn=self.collate_fn, **self.dl_kwargs) if self.collate_fn is not None else None\n",
    "        return next(iter(self.dl))\n",
    "        \n",
    "    def filterer(cls, train:'SBaseXCDataBlock', valid:'SBaseXCDataBlock', fld:Optional[str]='identifier'):\n",
    "        train_info, valid_info, lbl_info = train.dset.data.data_info, valid.dset.data.data_info, train.dset.data.lbl_info\n",
    "        if fld not in train_info: raise ValueError(f'`{fld}` not in `data_info`')\n",
    "            \n",
    "        train.data_lbl_filterer, valid_filterer = Filterer.generate(train_info[fld], valid_info[fld], lbl_info[fld], \n",
    "                                                                    train.dset.data.data_lbl, valid.dset.data.data_lbl)\n",
    "        _, valid_filterer, idx = Filterer.prune(valid.dset.data.data_lbl, valid_filterer)\n",
    "        \n",
    "        valid = valid._getitems(idx)\n",
    "        valid.data_lbl_filterer = valid_filterer\n",
    "        \n",
    "        return train, valid\n",
    "        \n",
    "    def splitter(cls, valid_pct:Optional[float]=0.2, seed=None):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rnd_idx = list(torch.randperm(len(cls)).numpy())\n",
    "        cut = int(valid_pct * len(cls))\n",
    "        train, valid = cls._getitems(rnd_idx[cut:]), cls._getitems(rnd_idx[:cut])\n",
    "        if cls.data_lbl_filterer is None: return train, valid\n",
    "        else: return cls.filterer(train, valid)\n",
    "\n",
    "    def sample(cls, pct:Optional[float]=0.2, n:Optional[int]=None, seed=None):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rnd_idx = list(torch.randperm(len(cls)).numpy())\n",
    "        cut = int(pct * len(cls)) if n is None else max(1, n)\n",
    "        return cls._getitems(rnd_idx[:cut])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f5540-8c21-425f-a4e9-15fa9df61d1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab7517-43b9-4f21-bdaa-a03c50f48aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_block = SBaseXCDataBlock(train_dset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cdac-4a66-4595-a633-a215b6669d93",
   "metadata": {},
   "source": [
    "### `SXCDataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09b24a62-8db4-4d46-88bf-3af0cc0ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SXCDataBlock:\n",
    "\n",
    "    def __init__(self, train:SBaseXCDataBlock=None, valid:SBaseXCDataBlock=None, test:SBaseXCDataBlock=None):\n",
    "        self.train, self.valid, self.test = train, valid, test\n",
    "\n",
    "    @staticmethod\n",
    "    def load_cfg(fname):\n",
    "        with open(fname, 'r') as f: return json.load(f)\n",
    "\n",
    "    def sample_info(self, info, idx):\n",
    "        return {k: v[idx] if isinstance(v, torch.Tensor) or isinstance(v, np.ndarray) else [v[i] for i in idx] for k,v in info.items()}\n",
    "\n",
    "    def linker_dset(self, meta_name:str, remove_empty:Optional[bool]=True):\n",
    "        if meta_name not in self.train.dset.meta:\n",
    "            raise ValueError(f'Invalid metadata: {meta_name}')\n",
    "\n",
    "        train_meta = self.train.dset.meta[meta_name].data_meta\n",
    "        if remove_empty:\n",
    "            train_idx = np.where(train_meta.getnnz(axis=1) > 0)[0]\n",
    "            meta_idx = np.where(train_meta.getnnz(axis=0) > 0)[0]\n",
    "            train_meta = train_meta[train_idx][:, meta_idx].tocsr()\n",
    "        \n",
    "        train_info = self.train.dset.data.data_info\n",
    "        meta_info = self.train.dset.meta[meta_name].meta_info\n",
    "\n",
    "        if remove_empty:\n",
    "            train_info = self.sample_info(train_info, train_idx)\n",
    "            meta_info = self.sample_info(meta_info, meta_idx)\n",
    "\n",
    "        collate_fn = self.train.collate_fn\n",
    "        train_dset = SBaseXCDataBlock(SXCDataset(SMainXCDataset(data_info=train_info, data_lbl=train_meta, lbl_info=meta_info)), \n",
    "                                      collate_fn=collate_fn)\n",
    "        \n",
    "        if self.test is not None:\n",
    "            test_meta = self.test.dset.meta[meta_name].data_meta\n",
    "            if remove_empty:\n",
    "                test_meta = test_meta[:, meta_idx].tocsr()\n",
    "                test_idx = np.where(test_meta.getnnz(axis=1) > 0)[0]\n",
    "                test_meta = test_meta[test_idx].tocsr()\n",
    "    \n",
    "            test_info = self.test.dset.data.data_info\n",
    "            if remove_empty: test_info = self.sample_info(test_info, test_idx)\n",
    "    \n",
    "            test_dset = SBaseXCDataBlock(SXCDataset(SMainXCDataset(data_info=test_info, data_lbl=test_meta, lbl_info=meta_info)), \n",
    "                                         collate_fn=collate_fn)\n",
    "            return SXCDataBlock(train=train_dset, test=test_dset)\n",
    "        \n",
    "        return SXCDataBlock(train=train_dset)\n",
    "\n",
    "    @staticmethod\n",
    "    def inference_dset(data_info:Dict, data_lbl:sp.csr_matrix, lbl_info:Dict, data_lbl_filterer, **kwargs):\n",
    "        x_idx = np.where(data_lbl.getnnz(axis=1) == 0)[0].reshape(-1,1)\n",
    "        y_idx = np.zeros((len(x_idx),1), dtype=np.int64)\n",
    "        data_lbl[x_idx, y_idx] = 1\n",
    "        data_lbl_filterer = np.hstack([x_idx, y_idx]) if data_lbl_filterer is None else np.vstack([np.hstack([x_idx, y_idx]), data_lbl_filterer])\n",
    "    \n",
    "        pred_dset = SXCDataset(SMainXCDataset(data_info=data_info, data_lbl=data_lbl, lbl_info=lbl_info,\n",
    "                                              data_lbl_filterer=data_lbl_filterer, **kwargs))\n",
    "        return pred_dset\n",
    "\n",
    "    @property\n",
    "    def lbl_info(self): return self.test.dset.data.lbl_info if self.train is None else self.train.dset.data.lbl_info\n",
    "\n",
    "    @property\n",
    "    def lbl_dset(self): return SMainXCDataset(data_info=self.train.dset.data.lbl_info)\n",
    "\n",
    "    @property\n",
    "    def n_lbl(self): return self.test.dset.n_lbl if self.train is None else self.train.dset.n_lbl\n",
    "\n",
    "    @property\n",
    "    def collator(self): return self.test.collate_fn if self.train is None else self.train.collate_fn\n",
    "        \n",
    "    @classmethod\n",
    "    def from_cfg(\n",
    "        cls, \n",
    "        cfg:Union[str,Dict],\n",
    "        collate_fn:Optional[Callable]=identity_collate_fn,\n",
    "        valid_pct:Optional[float]=0.2,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if isinstance(cfg, str): cfg = cls.load_cfg(cfg)\n",
    "\n",
    "        blocks = dict()\n",
    "        for o in ['train', 'valid', 'test']:\n",
    "            if o in cfg['path']:\n",
    "                params = cfg['parameters'].copy()\n",
    "                params.update(kwargs)\n",
    "                if o != 'train': \n",
    "                    params['meta_dropout_remove'], params['meta_dropout_replace'] = None, None\n",
    "                blocks[o] = SBaseXCDataBlock.from_file(**cfg['path'][o], **params, collate_fn=collate_fn)\n",
    "                \n",
    "        return cls(**blocks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed004a61-5ede-490a-85a5-5771fd33a53a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3a21f-1de2-4b1f-ac65-eb401210d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.config import WIKISEEALSOTITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c260e-a02a-438a-a81c-c537523584cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock(train=train_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8194-6733-4938-a9cb-c278b4985a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_dset = block.linker_dset('hlk_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d91ba-9a68-4761-91d0-a5bdaf2ad782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194342b4-4515-4315-84e1-45124cd72afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(block.train.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f1099-f0ad-4df9-8ff5-2a5090685b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f3aaa-822f-48b6-9ed8-cf068d6cf539",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = WIKISEEALSOTITLES('/home/scai/phd/aiz218323/Projects/XC/data')['data_lnk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10505c9f-603a-440b-9f21-be93c4614210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transform_type': 'xc', 'smp_features': [('lbl2data', 1, 2), ('hlk2data', 1, 1), ('hlk2lbl2data', 2, 1)], 'pad_token': 0, 'oversample': False, 'sampling_features': [('lbl2data', 2), ('hlk2data', 1), ('hlk2lbl2data', 1)], 'num_labels': 1, 'num_metadata': 1, 'metadata_name': None, 'info_column_names': ['identifier', 'input_text'], 'use_tokenizer': True, 'tokenizer': 'bert-base-cased', 'tokenization_column': 'input_text', 'max_sequence_length': 32, 'padding': False, 'return_tensors': None, 'sep': '->', 'prompt_func': None, 'pad_side': 'right', 'drop': True, 'ret_t': True, 'in_place': True, 'collapse': True, 'device': 'cpu', 'inp': 'data', 'targ': 'lbl2data', 'ptr': 'lbl2data_data2ptr', 'n_lbl_samples': None, 'data_info_keys': None, 'lbl_info_keys': None, 'n_slbl_samples': 1, 'main_oversample': False, 'n_data_meta_samples': 1, 'n_lbl_meta_samples': 1, 'meta_info_keys': None, 'meta_oversample': False}\n"
     ]
    }
   ],
   "source": [
    "print(config['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5990f-6000-4473-8cda-53f73b2938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'return_tensors':'pt', 'padding':True}\n",
    "\n",
    "for k,v in params.items():\n",
    "    config['parameters'][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e74e2e-1321-4405-af56-b86805bdfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = SXCDataBlock.from_cfg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136c3ae-b689-479f-95d7-565c102b56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.dset.__getitems__([100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c88bf-3b9b-4e04-a1a4-9afc5ab02b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_idx': tensor([100, 200]),\n",
       " 'data_identifier': ['Applet', 'Geography_of_Africa'],\n",
       " 'data_input_text': ['Applet', 'Geography of Africa'],\n",
       " 'data_input_ids': tensor([[  101,  7302,  1204,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'data_attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plbl2data_idx': tensor([  927,   928,   929,   930, 23961,  1470,  1471, 27329]),\n",
       " 'plbl2data_data2ptr': tensor([5, 3]),\n",
       " 'lbl2data_idx': tensor([  930, 27329]),\n",
       " 'lbl2data_data2ptr': tensor([1, 1]),\n",
       " 'lbl2data_identifier': ['Abstract_Window_Toolkit', 'Geography_of_Africa'],\n",
       " 'lbl2data_input_text': ['Abstract Window Toolkit', 'Geography of Africa'],\n",
       " 'lbl2data_input_ids': tensor([[  101,   138,  4832, 15017, 24769,  6466, 10493,  2875,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101, 20678,  1104,  2201,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'lbl2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'lbl2data_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'plnk2data_idx': tensor([   762, 202927]),\n",
       " 'plnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_idx': tensor([   762, 202927]),\n",
       " 'lnk2data_data2ptr': tensor([1, 1]),\n",
       " 'lnk2data_identifier': ['Category:Free_computer_libraries',\n",
       "  'Category:Geography_of_the_Indian_Ocean'],\n",
       " 'lnk2data_input_text': ['Free computer libraries',\n",
       "  'Geography of the Indian Ocean'],\n",
       " 'lnk2data_input_ids': tensor([[  101,  4299,  2775,  9818,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 20678,  1104,  1103,  1890,  4879,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'lnk2data_token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'lnk2data_attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " 'plnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'plnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_idx': tensor([], dtype=torch.int64),\n",
       " 'lnk2lbl_lbl2ptr': tensor([0, 0]),\n",
       " 'lnk2lbl_identifier': [],\n",
       " 'lnk2lbl_input_text': [],\n",
       " 'lnk2lbl_input_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_token_type_ids': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_attention_mask': tensor([], size=(0, 29), dtype=torch.int64),\n",
       " 'lnk2lbl_data2ptr': tensor([0, 0])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5f296-1e5c-464a-9db5-fbdc4df0a272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
