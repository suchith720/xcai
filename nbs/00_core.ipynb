{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd, numpy as np, logging, sys, re, os, torch, json, inspect\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy import sparse\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from scipy import sparse\n",
    "from IPython.display import display\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase, BatchEncoding\n",
    "from typing import List, Dict, Union, Optional, Any, Callable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from fastcore.dispatch import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Info`: LOADS METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_data(x:Dict, n:Optional[int]=10, seed:Optional[int]=None):\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        display(pd.DataFrame(x).sample(n, random_state=seed))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Info():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokz, self.info = None, None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_txt(fname:str, sep:Optional[str]='->', enc:Optional[str]='latin-1'):\n",
    "        with open(fname, encoding=enc) as f:\n",
    "            info = [o[:-1] for o in f]\n",
    "        return list(zip(*[(o,) if sep is None else o.split(sep, maxsplit=1) for o in info]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_csv(fname:str):\n",
    "        df = pd.read_csv(fname).fillna('')\n",
    "        return [df[c].tolist() for c in df.columns]\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_file(fname:str, sep:Optional[str]='->', enc:Optional[str]='latin-1'):\n",
    "        if fname.endswith(\".txt\"): \n",
    "            return Info._read_txt(fname, sep=sep, enc=enc)\n",
    "        elif fname.endswith(\".csv\"): \n",
    "            return Info._read_csv(fname)\n",
    "        else: \n",
    "            raise ValueError(f\"Invalid filename: {fname}.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_info(fname:str, sep:Optional[str]='->', info_column_names:Optional[List]=None, enc:Optional[str]='latin-1'):\n",
    "        info = Info._read_file(fname, sep=sep, enc=enc)\n",
    "        info_column_names = list(range(len(info))) if info_column_names is None else info_column_names\n",
    "        if len(info_column_names) != len(info): raise ValueError(f'`info_column_names` and `info` should have same number of elements.')\n",
    "        return {p:q for p,q in zip(info_column_names, info)}\n",
    "\n",
    "    def read_info(self, fname:Optional[str], sep:Optional[str]='->', info_column_names:Optional[List]=None, enc:Optional[str]='latin-1'):\n",
    "        self.info = Info._read_info(fname, sep, info_column_names, enc)\n",
    "        return self.info\n",
    "    \n",
    "    def tokenize(self, tokenization_column:Union[int, str], tokenizer:Union[str, PreTrainedTokenizerBase], \n",
    "                 max_sequence_length:Optional[int]=None, padding:Optional[bool]=True, return_tensors:Optional[str]=None, \n",
    "                 prompt_func:Optional[Callable]=None):\n",
    "        if self.tokz is None: self.tokz = tokenizer if isinstance(tokenizer, PreTrainedTokenizerBase) else AutoTokenizer.from_pretrained(tokenizer)\n",
    "        tokenization_column = list(self.info.keys())[0] if tokenization_column is None else tokenization_column\n",
    "        if tokenization_column is None: logging.info(f'`tokenization_column` not given as input, so value set to {tokenization_column}.')\n",
    "        if tokenization_column not in self.info: raise ValueError(f'`{tokenization_column}` is invalid `tokenization_column` value.')\n",
    "\n",
    "        tokenization_text = self.info[tokenization_column] if prompt_func is None else [prompt_func(t) for t in self.info[tokenization_column]]\n",
    "        self.info.update(self.tokz(tokenization_text, truncation=True, max_length=max_sequence_length, \n",
    "                                   padding=padding, return_tensors=return_tensors))\n",
    "        \n",
    "        return self.info\n",
    "\n",
    "    def show_data(self, n:Optional[int]=10, seed:Optional[int]=None):\n",
    "        with pd.option_context('display.max_colwidth', None):\n",
    "            display(pd.DataFrame(self.info).sample(n, random_state=seed))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.info is None: return 0\n",
    "        n_info = [len(v) for v in self.info.values()]\n",
    "        if len(n_info) == 0: raise ValueError('`info` cannot be empty.')\n",
    "        if not np.all([o == n_info[0] for o in n_info]): raise ValueError('`info` should contain features with same length.')\n",
    "        return n_info[0]\n",
    "\n",
    "    @classmethod\n",
    "    def from_txt(cls, \n",
    "                 fname:str, \n",
    "                 sep:Optional[str]='->', \n",
    "                 info_column_names:Optional[List]=None, \n",
    "                 enc:Optional[str]='latin-1',\n",
    "                 use_tokenizer:Optional[bool]=False,\n",
    "                 tokenizer:Optional[Union[str,PreTrainedTokenizerBase]]=None,\n",
    "                 tokenization_column:Optional[str]=None,\n",
    "                 max_sequence_length:Optional[int]=None,\n",
    "                 padding:Optional[bool]=True,\n",
    "                 return_tensors:Optional[str]=None,\n",
    "                 prompt_func:Optional[Callable]=None,\n",
    "                 **kwargs):\n",
    "        self = cls()\n",
    "        self.info = self.read_info(fname, sep, info_column_names, enc)\n",
    "        if use_tokenizer: \n",
    "            self.tokenize(tokenization_column, tokenizer, max_sequence_length, padding=padding, return_tensors=return_tensors, \n",
    "                          prompt_func=prompt_func)\n",
    "        return self.info\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'/home/scai/phd/aiz218323/scratch/datasets/benchmarks/(mapped)LF-WikiSeeAlsoTitles-320K/raw_data/train.raw.txt'\n",
    "cols = ['identifier', 'input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'/home/scai/phd/aiz218323/scratch/datasets/wikipedia/20250123/LF-WikiSeeAlsoTitles-320K/raw_data/train.old.raw.csv'\n",
    "cols = ['identifier', 'input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_func(text):\n",
    "    prompt = f\"\"\"\n",
    "    Given the title of a wikipedia article and the corresponding categories of that article on wikipedia,\n",
    "    your task is to predict the titles of all articles which are likely to be listed in the see also section of the\n",
    "    mentioned article. Output the coma separated list of titles of the articles in the see also section of the\n",
    "    given article.\n",
    "    ### Input : \n",
    "    ### Title : {text}\n",
    "    \n",
    "    #### Task Output :\n",
    "    #### Predicted title :\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\n",
    "tokz.add_special_tokens({\"pad_token\": \"<PAD>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Info.from_txt(fname, info_column_names=cols, use_tokenizer=True, tokenizer=tokz, tokenization_column=cols[1], \n",
    "                     max_sequence_length=128, padding=True, return_tensors='pt', prompt_func=prompt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>\\n    Given the title of a wikipedia article and the corresponding categories of that article on wikipedia,\\n    your task is to predict the titles of all articles which are likely to be listed in the see also section of the\\n    mentioned article. Output the coma separated list of titles of the articles in the see also section of the\\n    given article.\\n    ### Input : \\n    ### Title : Thunderstorm\\n    \\n    #### Task Output :\\n    #### Predicted title :\\n    <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.decode(info['input_ids'][10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>input_text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431204</th>\n",
       "      <td>Certificate_of_Formula_Compliance</td>\n",
       "      <td>Certificate of Formula Compliance</td>\n",
       "      <td>[101, 8196, 1035, 1997, 1035, 5675, 1035, 12646, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651497</th>\n",
       "      <td>2017_Sunwolves_season</td>\n",
       "      <td>2017 Sunwolves season</td>\n",
       "      <td>[101, 2418, 1035, 3103, 12155, 20899, 1035, 2161, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208048</th>\n",
       "      <td>Hadza_people</td>\n",
       "      <td>Hadza people</td>\n",
       "      <td>[101, 2018, 4143, 1035, 2111, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179912</th>\n",
       "      <td>The_Lost_City_(2005_film)</td>\n",
       "      <td>The Lost City (2005 film)</td>\n",
       "      <td>[101, 1996, 1035, 2439, 1035, 2103, 1035, 1006, 2384, 1035, 2143, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130941</th>\n",
       "      <td>Testability</td>\n",
       "      <td>Testability</td>\n",
       "      <td>[101, 3231, 8010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               identifier                         input_text  \\\n",
       "431204  Certificate_of_Formula_Compliance  Certificate of Formula Compliance   \n",
       "651497              2017_Sunwolves_season              2017 Sunwolves season   \n",
       "208048                       Hadza_people                       Hadza people   \n",
       "179912          The_Lost_City_(2005_film)          The Lost City (2005 film)   \n",
       "130941                        Testability                        Testability   \n",
       "\n",
       "                                                                                                                                    input_ids  \\\n",
       "431204             [101, 8196, 1035, 1997, 1035, 5675, 1035, 12646, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "651497            [101, 2418, 1035, 3103, 12155, 20899, 1035, 2161, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "208048                       [101, 2018, 4143, 1035, 2111, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "179912  [101, 1996, 1035, 2439, 1035, 2103, 1035, 1006, 2384, 1035, 2143, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "130941                             [101, 3231, 8010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                          token_type_ids  \\\n",
       "431204  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "651497  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "208048  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "179912  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "130941  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                          attention_mask  \n",
       "431204  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "651497  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "208048  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "179912  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "130941  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = f'/home/scai/phd/aiz218323/scratch/datasets/benchmarks/(mapped)LF-WikiSeeAlsoTitles-320K/raw_data/train.raw.txt'\n",
    "cols = ['identifier', 'input_text']\n",
    "\n",
    "info = Info.from_txt(fname, sep='->', info_column_names=cols, use_tokenizer=True, tokenizer='bert-base-uncased', \n",
    "                     tokenization_column=cols[0], max_sequence_length=32)\n",
    "show_data(info, n=5, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>input_text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431204</th>\n",
       "      <td>Certificate_of_Formula_Compliance</td>\n",
       "      <td>Certificate of Formula Compliance</td>\n",
       "      <td>[101, 8196, 1035, 1997, 1035, 5675, 1035, 12646, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651497</th>\n",
       "      <td>2017_Sunwolves_season</td>\n",
       "      <td>2017 Sunwolves season</td>\n",
       "      <td>[101, 2418, 1035, 3103, 12155, 20899, 1035, 2161, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208048</th>\n",
       "      <td>Hadza_people</td>\n",
       "      <td>Hadza people</td>\n",
       "      <td>[101, 2018, 4143, 1035, 2111, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179912</th>\n",
       "      <td>The_Lost_City_(2005_film)</td>\n",
       "      <td>The Lost City (2005 film)</td>\n",
       "      <td>[101, 1996, 1035, 2439, 1035, 2103, 1035, 1006, 2384, 1035, 2143, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130941</th>\n",
       "      <td>Testability</td>\n",
       "      <td>Testability</td>\n",
       "      <td>[101, 3231, 8010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               identifier                         input_text  \\\n",
       "431204  Certificate_of_Formula_Compliance  Certificate of Formula Compliance   \n",
       "651497              2017_Sunwolves_season              2017 Sunwolves season   \n",
       "208048                       Hadza_people                       Hadza people   \n",
       "179912          The_Lost_City_(2005_film)          The Lost City (2005 film)   \n",
       "130941                        Testability                        Testability   \n",
       "\n",
       "                                                                                                                                    input_ids  \\\n",
       "431204             [101, 8196, 1035, 1997, 1035, 5675, 1035, 12646, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "651497            [101, 2418, 1035, 3103, 12155, 20899, 1035, 2161, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "208048                       [101, 2018, 4143, 1035, 2111, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "179912  [101, 1996, 1035, 2439, 1035, 2103, 1035, 1006, 2384, 1035, 2143, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "130941                             [101, 3231, 8010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                          token_type_ids  \\\n",
       "431204  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "651497  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "208048  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "179912  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "130941  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                                                          attention_mask  \n",
       "431204  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "651497  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "208048  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "179912  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "130941  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = f'/home/scai/phd/aiz218323/scratch/datasets/wikipedia/20250123/LF-WikiSeeAlsoTitles-320K/raw_data/train.old.raw.csv'\n",
    "cols = ['identifier', 'input_text']\n",
    "\n",
    "info = Info.from_txt(fname, sep=None, info_column_names=cols, use_tokenizer=True, tokenizer='bert-base-uncased', \n",
    "                     tokenization_column=cols[0], max_sequence_length=32)\n",
    "show_data(info, n=5, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = Info()\n",
    "_ = info.read_info(fname, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247245</th>\n",
       "      <td>20_minutes_(Switzerland)</td>\n",
       "      <td>20 minutes (Switzerland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>Isoelectric_point</td>\n",
       "      <td>Isoelectric point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538866</th>\n",
       "      <td>Moldova_in_the_Eurovision_Song_Contest_2013</td>\n",
       "      <td>Moldova in the Eurovision Song Contest 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100310</th>\n",
       "      <td>Lemhi_Pass</td>\n",
       "      <td>Lemhi Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639082</th>\n",
       "      <td>Order_of_precedence_in_Kelantan</td>\n",
       "      <td>Order of precedence in Kelantan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510129</th>\n",
       "      <td>RTS,S</td>\n",
       "      <td>RTS,S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216809</th>\n",
       "      <td>Geering_(automobile)</td>\n",
       "      <td>Geering (automobile)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569292</th>\n",
       "      <td>Sole_Survivor_(2013_film)</td>\n",
       "      <td>Sole Survivor (2013 film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175818</th>\n",
       "      <td>Puerto_Rico_National_Cemetery</td>\n",
       "      <td>Puerto Rico National Cemetery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615747</th>\n",
       "      <td>Anna_IllÃ©s</td>\n",
       "      <td>Anna IllÃ©s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         identifier  \\\n",
       "247245                     20_minutes_(Switzerland)   \n",
       "2305                              Isoelectric_point   \n",
       "538866  Moldova_in_the_Eurovision_Song_Contest_2013   \n",
       "100310                                   Lemhi_Pass   \n",
       "639082              Order_of_precedence_in_Kelantan   \n",
       "510129                                        RTS,S   \n",
       "216809                         Geering_(automobile)   \n",
       "569292                    Sole_Survivor_(2013_film)   \n",
       "175818                Puerto_Rico_National_Cemetery   \n",
       "615747                                  Anna_IllÃ©s   \n",
       "\n",
       "                                         input_text  \n",
       "247245                     20 minutes (Switzerland)  \n",
       "2305                              Isoelectric point  \n",
       "538866  Moldova in the Eurovision Song Contest 2013  \n",
       "100310                                   Lemhi Pass  \n",
       "639082              Order of precedence in Kelantan  \n",
       "510129                                        RTS,S  \n",
       "216809                         Geering (automobile)  \n",
       "569292                    Sole Survivor (2013 film)  \n",
       "175818                Puerto Rico National Cemetery  \n",
       "615747                                  Anna IllÃ©s  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info.show_data(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693082"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Filterer`: XC FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Filterer:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_filter(fname:str):\n",
    "        if fname is not None and os.path.exists(fname): return np.loadtxt(fname, dtype=np.int64)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate(train_id:List, test_id:List, lbl_id:List, train_lbl:sparse.csr_matrix, test_lbl:sparse.csr_matrix):\n",
    "        _, train_idx, lbl2train_idx = np.intersect1d(train_id, lbl_id, return_indices=True)\n",
    "        train_lbl_filterer = np.vstack([train_idx, lbl2train_idx]).T\n",
    "        \n",
    "        _, test_idx, lbl2test_idx = np.intersect1d(test_id, lbl_id, return_indices=True)\n",
    "        test_lbl_filterer = np.vstack([test_idx, lbl2test_idx]).T\n",
    "        \n",
    "        train_udx, train_udx2idx = np.unique(train_idx, return_index=True)\n",
    "        lbl2test_udx, lbl2test_udx2idx = np.unique(lbl2test_idx, return_index=True)\n",
    "        \n",
    "        _test_lbl_filterer = train_lbl[train_udx][:, lbl2test_udx].T\n",
    "        \n",
    "        rows, cols = _test_lbl_filterer.nonzero()\n",
    "        test_idx = test_idx[lbl2test_udx2idx[rows]]\n",
    "        lbl2test_idx = lbl2train_idx[train_udx2idx[cols]]\n",
    "        \n",
    "        _test_lbl_filterer = np.vstack([test_idx, lbl2test_idx]).T\n",
    "        test_lbl_filterer = np.vstack([test_lbl_filterer, _test_lbl_filterer])\n",
    "    \n",
    "        return train_lbl_filterer, test_lbl_filterer\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(f:np.array, sz:tuple, idx:List):\n",
    "        f = sparse.coo_matrix((np.full(f.shape[0],1), (f[:, 0], f[:, 1])), shape=sz).tocsr()\n",
    "        f = f[idx].tocoo()\n",
    "        return np.vstack([f.row, f.col]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def prune(data:sparse.csr_matrix, data_filterer:np.array):\n",
    "        data = data.copy()\n",
    "        data[data_filterer[:,0], data_filterer[:,1]] = 0\n",
    "        data.eliminate_zeros()\n",
    "        \n",
    "        idx = np.where(data.getnnz(axis=1) > 0)[0]\n",
    "        return data[idx], Filterer.sample(data_filterer, data.shape, idx), idx\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(data:sparse.csr_matrix, data_filterer:np.array):\n",
    "        data[data_filterer[:,0], data_filterer[:,1]] = 0\n",
    "        data.eliminate_zeros()\n",
    "        return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token `IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tok_sparse(tokens:List, n_cols:Optional[int]=None):\n",
    "    n_toks = torch.tensor([len(tok) for tok in tokens])\n",
    "    tok_ptr = torch.concat([torch.zeros((1,), dtype=torch.long), n_toks.cumsum(dim=0)])\n",
    "    toks = torch.tensor(list(chain(*tokens)))\n",
    "    tok_cnt = torch.full((toks.shape[0],), 1, dtype=torch.long)\n",
    "    m = sparse.csr_matrix((tok_cnt, toks, tok_ptr)) if n_cols is None else sparse.csr_matrix((tok_cnt, toks, tok_ptr), shape=(len(n_toks), n_cols))\n",
    "    m.sum_duplicates()\n",
    "    return m\n",
    "\n",
    "def compute_inv_doc_freq(inputs:sparse.csr_matrix):\n",
    "    n_docs = inputs.shape[0]\n",
    "    doc_freq = torch.tensor(inputs.getnnz(axis=0))\n",
    "    return torch.log((n_docs+1)/(doc_freq+1))+1\n",
    "\n",
    "def get_tok_idf(dset:Dataset, field:Optional[str]='data_input_ids', n_cols:Optional[int]=None):\n",
    "    toks = [list(chain(*dset[i][field])) for i in tqdm(range(len(dset)))]\n",
    "    tok_sparse = get_tok_sparse(toks, n_cols=n_cols)\n",
    "    return compute_inv_doc_freq(tok_sparse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_batch(m, b, m_args=None):\n",
    "    m_kwargs = inspect.signature(m.forward).parameters\n",
    "    return BatchEncoding({k:v for k,v in b.items() if k in m_kwargs or (m_args is not None and k in m_args)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def store_attr(names=None, self=None, but='', cast=False, store_args=None, is_none=True, **attrs):\n",
    "    fr = sys._getframe(1)\n",
    "    args = argnames(fr, True)\n",
    "    if self: args = ('self', *args)\n",
    "    else: self = fr.f_locals[args[0]]\n",
    "    if store_args is None: store_args = not hasattr(self,'__slots__')\n",
    "    if store_args and not hasattr(self, '__stored_args__'): self.__stored_args__ = {}\n",
    "    anno = annotations(self) if cast else {}\n",
    "    if names and isinstance(names,str): names = re.split(', *', names)\n",
    "    ns = names if names is not None else getattr(self, '__slots__', args[1:])\n",
    "    added = {n:fr.f_locals[n] for n in ns}\n",
    "    attrs = {**attrs, **added}\n",
    "    if isinstance(but,str): but = re.split(', *', but)\n",
    "    attrs = {k:v for k,v in attrs.items() if k not in but}\n",
    "    return _store_attr(self, anno, is_none, **attrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _store_attr(self, anno, is_none, **attrs):\n",
    "    stored = getattr(self, '__stored_args__', None)\n",
    "    for n,v in attrs.items():\n",
    "        if n in anno: v = anno[n](v)\n",
    "        if is_none or v is not None: setattr(self, n, v)\n",
    "        if stored is not None: stored[n] = v\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_attr(x, attr:str):\n",
    "    for a in attr.split('.'): x = getattr(x, a)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sorted_metric(keys:List, order:Optional[Dict]=None):\n",
    "    order = {o.split('@')[0]:i for i,o in enumerate(keys)}\n",
    "    def _suffix(x): return (int(x.split('_')[0]) , x.split('_')[1]) if '_' in x else (int(x),)\n",
    "    def _key_fn(x): return order[x[0]], _suffix(x[1])\n",
    "    def key_fn(x): return _key_fn(x.split('@'))\n",
    "    return sorted(keys, key=key_fn)\n",
    "\n",
    "def display_metric(metrics, remove_prefix:Optional[bool]=True, order:Optional[List]=None, scale:Optional[int]=100.0):\n",
    "    metrics = {k.split('_', maxsplit=1)[1]:v for k,v in metrics.items()} if remove_prefix else metrics\n",
    "    metric_keys, other_keys = sorted_metric([k for k in metrics if '@' in k], order), [k for k in metrics if '@' not in k]\n",
    "    \n",
    "    from IPython.display import display\n",
    "    with pd.option_context('display.precision',4,'display.max_colwidth',None,'display.max_columns',None):\n",
    "        metric,other = pd.DataFrame([metrics])[metric_keys]*scale, pd.DataFrame([metrics])[other_keys]\n",
    "        display(pd.concat([metric, other], axis=1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tensor_statistics(x:torch.Tensor):\n",
    "    c = ['mean', 'std', '25', '50', '75']\n",
    "    s = torch.cat([x.float().mean(dim=0, keepdim=True), \n",
    "                   x.float().std(dim=0, keepdim=True),\n",
    "                   torch.quantile(x.float(), torch.tensor([0.25, 0.5, 0.75]))])\n",
    "    return pd.DataFrame([s.tolist()], columns=c)\n",
    "\n",
    "def total_recall(inp_idx:torch.Tensor, n_inp:torch.Tensor, targ:sparse.csr_matrix, filterer:sparse.csr_matrix):\n",
    "    val, ptr = torch.ones(len(inp_idx)), torch.cat([torch.zeros(1, dtype=torch.int64), n_inp.cumsum(0)])\n",
    "    inp = sparse.csr_matrix((val,inp_idx,ptr), shape=targ.shape); inp.sum_duplicates(); inp.data[:] = 1\n",
    "    if filterer is not None: inp, targ = Filterer.apply(inp, filterer), Filterer.apply(targ, filterer)\n",
    "    sc = inp.multiply(targ)/(targ.getnnz(axis=1)[:, None]*targ.shape[0])\n",
    "    return sc.sum(), sc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_best_model(mdir:str, pat:Optional[str]=r'^checkpoint-(\\d+)'):\n",
    "    nm = sorted([int(re.match(pat, o).group(1)) for o in os.listdir(mdir) if re.match(pat, o)])[-1]\n",
    "    fname = f'{mdir}/checkpoint-{nm}/trainer_state.json'\n",
    "    with open(fname, 'r') as file: mname = json.load(file)['best_model_checkpoint']\n",
    "    return f'{mdir}/checkpoint-{nm}' if mname is None else mname\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_output_sparse(pred_idx, pred_ptr, pred_score, targ_idx, targ_ptr, n_lbl):\n",
    "    n_data = pred_ptr.shape[0]\n",
    "    \n",
    "    pred_ptr = torch.cat([torch.zeros((1,), dtype=torch.long), pred_ptr.cumsum(dim=0)])\n",
    "    \n",
    "    targ_ptr = torch.cat([torch.zeros((1,), dtype=torch.long), targ_ptr.cumsum(dim=0)])\n",
    "    targ_score = torch.ones((targ_idx.shape[0],), dtype=torch.long)\n",
    "    \n",
    "    pred = sparse.csr_matrix((pred_score,pred_idx,pred_ptr), shape=(n_data, n_lbl))\n",
    "    targ = sparse.csr_matrix((targ_score,targ_idx,targ_ptr), shape=(n_data, n_lbl))\n",
    "    return pred, targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_output(data_lbl, pred_lbl):\n",
    "    output = {\n",
    "        'targ_idx': torch.tensor(data_lbl.indices),\n",
    "        'targ_ptr': torch.tensor([q-p for p,q in zip(data_lbl.indptr, data_lbl.indptr[1:])]),\n",
    "        'pred_idx': torch.tensor(pred_lbl.indices),\n",
    "        'pred_ptr': torch.tensor([q-p for p,q in zip(pred_lbl.indptr, pred_lbl.indptr[1:])]),\n",
    "        'pred_score': torch.tensor(pred_lbl.data),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_config(fname, key):\n",
    "    with open(fname, 'r') as file:\n",
    "        return json.load(file)[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ScoreFusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScoreFusion():\n",
    "    \n",
    "    def __init__(self, prop:Optional[np.array]=None, max_depth:Optional[int]=7):\n",
    "        self.clf, self.prop = DecisionTreeClassifier(max_depth=max_depth), prop\n",
    "        \n",
    "    def sample(self, \n",
    "               score_a:sparse.csr_matrix, \n",
    "               score_b:sparse.csr_matrix, \n",
    "               targ:Optional[sparse.csr_matrix]=None, \n",
    "               n_samples:Optional[int]=None):\n",
    "        if n_samples is not None and n_samples > 0 and n_samples < score_a.shape[0]:\n",
    "            rnd_idx = np.random.permutation(score_a.shape[0])[:n_samples]\n",
    "            score_a, score_b = score_a[rnd_idx], score_b[rnd_idx]\n",
    "            targ = targ if targ is None else targ[rnd_idx]\n",
    "        return score_a, score_b, targ\n",
    "    \n",
    "    def get_inp(self, row_idx, col_idx, score_a, score_b):\n",
    "        inp = [np.array(o[row_idx,col_idx]).reshape(-1, 1) for o in [score_a, score_b]]\n",
    "        if self.prop is not None: inp.append(self.prop[col_idx].reshape(-1,1))\n",
    "        return np.hstack(inp)\n",
    "    \n",
    "    def prepare_inputs(self, score_a:sparse.csr_matrix, score_b:sparse.csr_matrix, \n",
    "                       targ:Optional[sparse.csr_matrix]=None, n_samples:Optional[int]=None):\n",
    "        if n_samples is not None:\n",
    "            score_a, score_b, targ = self.sample(score_a, score_b, targ, n_samples)\n",
    "        row_idx, col_idx = score_b.nonzero()\n",
    "        inp = self.get_inp(row_idx, col_idx, score_a, score_b)\n",
    "        targ = targ if targ is None else np.array(targ[row_idx, col_idx]).ravel()\n",
    "        return inp, targ, (row_idx,col_idx)\n",
    "        \n",
    "    def fit(self, score_a:sparse.csr_matrix, score_b:sparse.csr_matrix, targ:Optional[sparse.csr_matrix]=None, \n",
    "            n_samples:Optional[int]=None):\n",
    "        score, targ, _ = self.prepare_inputs(score_a, score_b, targ, n_samples)\n",
    "        self.clf.fit(score, targ)\n",
    "\n",
    "    def predict(self, score_a:sparse.csr_matrix, score_b:sparse.csr_matrix, beta:Optional[int]=1):\n",
    "        inp, _, (row_idx, col_idx) = self.prepare_inputs(score_a, score_b)\n",
    "        res = score_b.copy()\n",
    "        res[row_idx,col_idx] = self.clf.predict_proba(inp)[:, 1]\n",
    "        return beta*(res+score_a)+score_b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Sparse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def retain_randk(matrix:sparse.csr_matrix, topk:Optional[int]=3):\n",
    "    data, indices, indptr = [], [], np.zeros_like(matrix.indptr)\n",
    "    for i,row in tqdm(enumerate(matrix), total=matrix.shape[0]):\n",
    "        if row.nnz > 0:\n",
    "            idx = np.random.randint(row.nnz, size=topk)\n",
    "            ind, d = row.indices[idx], row.data[idx]\n",
    "            indptr[i+1] = indptr[i] + topk\n",
    "            indices.append(ind); data.append(d)\n",
    "        else:\n",
    "            indptr[i+1] = indptr[i]\n",
    "    \n",
    "    data = np.hstack(data)\n",
    "    indices = np.hstack(indices)\n",
    "    \n",
    "    o = sparse.csr_matrix((data, indices, indptr), dtype=matrix.dtype)\n",
    "    o.sort_indices()\n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Robustness Analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_topk(data_lbl, topk=5):\n",
    "    data,indices,indptr = [],[],[0]\n",
    "    for i,j in tqdm(zip(data_lbl.indptr, data_lbl.indptr[1:]), total=data_lbl.shape[0]):\n",
    "        idx = np.random.permutation(j-i)[:topk]\n",
    "        data.append(data_lbl.data[i:j][idx])\n",
    "        indices.append(data_lbl.indices[i:j][idx])\n",
    "        indptr.append(indptr[-1]+len(idx))\n",
    "    data = np.hstack(data)\n",
    "    indices = np.hstack(indices)\n",
    "    indptr = np.array(indptr)\n",
    "\n",
    "    o = sparse.csr_matrix((data,indices,indptr), shape=data_lbl.shape, dtype=np.float32)\n",
    "    o.sort_indices()\n",
    "    o.eliminate_zeros()\n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def robustness_analysis(block, meta_name:str, analysis_type:str='missing', pct:float=0.5, topk:int=3):\n",
    "    data_meta = random_topk(block.test.dset.meta[f'{meta_name}_meta'].data_meta, topk=topk)\n",
    "    \n",
    "    if analysis_type == 'noise':\n",
    "        mask = np.random.rand(len(data_meta.data)) < pct\n",
    "        data_meta.indices[mask] = np.random.randint(block.n_lbl, size=mask.sum())\n",
    "        data_meta.sort_indices()\n",
    "    elif analysis_type == 'missing':\n",
    "        data_meta.data[np.random.rand(len(data_meta.data)) < pct] = 0\n",
    "        data_meta.eliminate_zeros()\n",
    "    else:\n",
    "        raise ValueError(f'Invalid `analysis_type`: {analysis_type}.')\n",
    "        \n",
    "    lbl_meta = block.test.dset.meta[f'{meta_name}_meta'].lbl_meta\n",
    "    block.test.dset.meta[f'{meta_name}_meta'].update_meta_matrix(data_meta, lbl_meta)\n",
    "    return block\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
