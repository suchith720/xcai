{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9345ca-bde6-4b36-ad29-1d7af2d94d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.product_key.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354ca689-ef3d-4a25-88e9-5cf765406b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beae00f5-d327-4c05-92f8-61c3f2d91c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204e238c-c822-40da-b84f-c162a27ca922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math, numpy as np, torch\n",
    "\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from xcai.core import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d5e20b-b85a-4c50-ba5c-46ce051d6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.distributed._composable.fsdp import MixedPrecisionPolicy, fully_shard\n",
    "from torch.distributed.tensor.parallel import parallelize_module\n",
    "\n",
    "from xcai.models.product_key.colwise_embedding_bag import ColwiseEmbeddingBag, xFormerEmbeddingBag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5decc-b008-42ff-95ad-7892f35a77db",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003c6956-4d4f-4577-bc3a-a13aaaf9eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.main import *\n",
    "from xcai.models.modeling_utils import Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4253636e-f35e-41ba-a789-804da3028200",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/scratch/scai/phd/aiz218323/outputs/mogicX/03_ngame-for-wikiseealsotitles'\n",
    "\n",
    "data_dir = '/home/scai/phd/aiz218323/scratch/datasets/benchmarks/'\n",
    "config_file = 'wikiseealsotitles'\n",
    "config_key = 'data_meta'\n",
    "\n",
    "mname = 'sentence-transformers/msmarco-distilbert-base-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62cee75-44a1-406f-8aa4-ac93ffc175bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/scratch/scai/phd/aiz218323/datasets/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9264c05-46ca-4ddd-9b3a-94f40f00b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = f'{pkl_dir}/mogicX/wikiseealsotitles_data-meta_distilbert-base-uncased_sxc.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b077d7a6-59ea-4eda-ba63-e7127405844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = build_block(pkl_file, config_file, True, config_key, data_dir=data_dir, n_sdata_meta_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "655d22b4-ba13-4cf7-a326-5e8579b6d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "91cbb8dd-44a2-4f62-bde8-556cfbda62c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_idx', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_data2ptr', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'pcat2data_idx', 'pcat2data_data2ptr', 'cat2data_idx', 'cat2data_data2ptr', 'cat2data_identifier', 'cat2data_input_text', 'cat2data_input_ids', 'cat2data_attention_mask', 'pcat2lbl_idx', 'pcat2lbl_lbl2ptr', 'cat2lbl_idx', 'cat2lbl_lbl2ptr', 'cat2lbl_identifier', 'cat2lbl_input_text', 'cat2lbl_input_ids', 'cat2lbl_attention_mask', 'cat2lbl_data2ptr', 'pcat2lbl_data2ptr'])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cac5c8-b9ea-4d46-b3f0-c89948478a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea4ddd3-8de4-4e74-a97b-d77ede07bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "m = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebee34ff-a527-4fa1-8dcb-1434c2eec626",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(input_ids=batch['data_input_ids'], attention_mask=batch['data_attention_mask'])\n",
    "o = Pooling.mean_pooling(o.last_hidden_state, batch['data_attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "edd0284e-97d6-4d0d-8e86-79a7aded32f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d706c99-318a-4f9c-a71e-0a6120a02505",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `Query MLP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f19b0bb-9075-4b65-81f4-566f1db8a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QueryMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, heads, k_dim, sizes, bias=False, batchnorm=False):\n",
    "        super().__init__()\n",
    "        store_attr('input_dim,heads,k_dim,sizes')\n",
    "        assert sizes[0] == input_dim\n",
    "        assert sizes[-1] == (heads * k_dim)\n",
    "        self.mlp = QueryMLP.get_mlp(list(sizes), bias=bias, batchnorm=batchnorm)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mlp(sizes, bias=True, batchnorm=True):\n",
    "        \"\"\"\n",
    "        Generate a feedforward neural network.\n",
    "        \"\"\"\n",
    "        assert len(sizes) >= 2\n",
    "        pairs = [(sizes[i], sizes[i+1]) for i in range(len(sizes) - 1)]\n",
    "        \n",
    "        layers = []\n",
    "        for i, (dim_in, dim_out) in enumerate(pairs):\n",
    "            layers.append(nn.Linear(dim_in, dim_out, bias=bias))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(dim_out))\n",
    "            if i < len(pairs) - 1:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[-1] == self.input_dim\n",
    "        x = x.contiguous().view(-1, self.input_dim) if x.dim() > 2 else x\n",
    "        bs = len(x)\n",
    "        \n",
    "        o = self.mlp(x)\n",
    "\n",
    "        assert o.shape == (bs, self.heads * self.k_dim)\n",
    "        return o.view(bs * self.heads, self.k_dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632d741-bd6e-4282-830d-1b22510f9712",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f0df9bd-c29a-4228-8ff2-d0e39b89fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = QueryMLP(16, 2, 32, (16, 32, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "896269db-168f-4b2f-8fbb-347293428bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=32, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95b12845-c4d8-40fd-8365-8887690a0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f3fbe5b-da33-4d4d-8823-82a8a19c3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39aef79b-aff2-4c52-a54d-997693e4e03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aefda9-76ce-4900-9c89-97d56da6a6ea",
   "metadata": {},
   "source": [
    "## `Hashing Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db2b3e64-ced2-4880-9174-f141a2e65e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HashingMemory(nn.Module):\n",
    "\n",
    "    VALUES = None\n",
    "    EVAL_MEMORY = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim:int,\n",
    "        output_dim:int,\n",
    "        \n",
    "        value_fixed_lr:Optional[float] = 0.001,\n",
    "        \n",
    "        k_dim:Optional[int] = 512,\n",
    "        v_dim:Optional[int] = -1,\n",
    "        \n",
    "        num_heads:Optional[int] = 4,\n",
    "        num_knn:Optional[int] = 32,\n",
    "        \n",
    "        num_keys:Optional[int] = 1024,\n",
    "        use_query_bias:Optional[bool] = True,\n",
    "        use_query_batchnorm:Optional[bool] = False,\n",
    "        \n",
    "        input_dropout:Optional[float] = 0.0,\n",
    "        query_dropout:Optional[float] = 0.0,\n",
    "        value_dropout:Optional[float] = 0.0,\n",
    "\n",
    "        use_gate:Optional[bool] = False,\n",
    "        share_values:Optional[bool] = True,\n",
    "        swilu_projection:Optional[bool] = True,\n",
    "        use_peer_variant:Optional[bool] = False,  # Replaces the PK memory with the PEER variant (Parameter Efficient Expert Retrieval)\n",
    "    ):\n",
    "        # dropout\n",
    "        assert 0 <= input_dropout < 1\n",
    "        assert 0 <= query_dropout < 1\n",
    "        assert 0 <= value_dropout < 1\n",
    "        \n",
    "        store_attr('input_dropout,query_dropout,value_dropout')\n",
    "        \n",
    "        assert k_dim >= 2\n",
    "        assert k_dim % 2 == 0\n",
    "        assert num_heads >= 2\n",
    "\n",
    "        # PEER variant\n",
    "        assert not (\n",
    "            use_peer_variant and v_dim > 0\n",
    "        ), f\"Cannot use PEER variant with a value dimension different from the input dimension (v_dim=-1)\"\n",
    "\n",
    "        if use_query_batchnorm:\n",
    "            logger.warning(\n",
    "                \"WARNING: If you use batch normalization, be sure that you use batches of sentences with the same size at training time. Otherwise, the padding token will result in incorrect mean/variance estimations in the BatchNorm layer.\"\n",
    "            )\n",
    "\n",
    "        # initialize\n",
    "        super().__init__()\n",
    "        \n",
    "        store_attr('use_peer_variant,input_dim,output_dim,k_dim,num_heads,num_knn,share_values,value_fixed_lr')\n",
    "        self.num_keys, self.size = num_keys, num_keys**2\n",
    "        self.v_dim = v_dim if v_dim > 0 else output_dim\n",
    "\n",
    "        # values initialization\n",
    "        self.swilu_proj = swilu_projection\n",
    "        self.v_proj = v_dim > 0 or self.swilu_proj\n",
    "\n",
    "        # initialize keys\n",
    "        self.keys = nn.Parameter(\n",
    "            torch.empty(2 * self.num_heads * int(self.size**0.5), self.k_dim // 2)\n",
    "        )\n",
    "        \n",
    "        self.original = not self.share_values or HashingMemory.VALUES is None\n",
    "        \n",
    "        # initialize the values\n",
    "        if self.original:\n",
    "            if not self.use_peer_variant:  # PK\n",
    "                self.values = xFormerEmbeddingBag(self.size, self.v_dim)\n",
    "                HashingMemory.VALUES = self.values\n",
    "            else:  # PEER\n",
    "                self.values_u = nn.Embedding(self.size, self.v_dim)\n",
    "                self.values_v = nn.Embedding(self.size, self.v_dim)\n",
    "                HashingMemory.VALUES = self.values_u, self.values_v\n",
    "        else:\n",
    "            if not self.use_peer_variant:  # PK\n",
    "                self.values = None\n",
    "            else:  # PEER\n",
    "                self.values_u = None\n",
    "                self.values_v = None\n",
    "\n",
    "        if self.v_proj:\n",
    "            proj_input = v_dim\n",
    "            if self.swilu_proj and proj_input < 0: proj_input = output_dim\n",
    "            self.value_proj = torch.nn.Linear(proj_input, output_dim)\n",
    "            \n",
    "        if self.swilu_proj: self.swilu_projection = torch.nn.Linear(self.input_dim, proj_input)\n",
    "            \n",
    "        # gated memory\n",
    "        self.gating = torch.nn.Linear(input_dim, 1) if use_gate else None\n",
    "\n",
    "        # query network\n",
    "        l_sizes = (self.input_dim, self.num_heads * self.k_dim)\n",
    "\n",
    "        self.query_proj = QueryMLP(\n",
    "            self.input_dim,\n",
    "            self.num_heads,\n",
    "            self.k_dim,\n",
    "            l_sizes,\n",
    "            bias=use_query_bias,\n",
    "            batchnorm=use_query_batchnorm,\n",
    "        )\n",
    "\n",
    "    def mp_parallelize(self, mesh, model_args, distributed_args, param_dtype):\n",
    "        fsdp_config = dict(\n",
    "            mp_policy=(\n",
    "                MixedPrecisionPolicy(\n",
    "                    param_dtype=param_dtype,\n",
    "                    # reduce_dtype=torch.float32,\n",
    "                    reduce_dtype=torch.bfloat16,\n",
    "                )\n",
    "            ),\n",
    "            mesh=mesh[\"dp_replicate\"],\n",
    "        )\n",
    "        # parallelize the module\n",
    "        if distributed_args.memory_parallel_size > 1:\n",
    "            assert (\n",
    "                not self.use_peer_variant\n",
    "            ), f\"The PEER variant does not have a memory parallel implementation\"\n",
    "            if self.original:\n",
    "                layer_plan = {\"values\": ColwiseEmbeddingBag()}\n",
    "                parallelize_module(\n",
    "                    self,\n",
    "                    mesh[\"memory_parallel\"],\n",
    "                    layer_plan,\n",
    "                )\n",
    "\n",
    "        # share the parameters\n",
    "        if self.original:\n",
    "            if not self.use_peer_variant:\n",
    "                self.values = fully_shard(\n",
    "                    self.values, **fsdp_config, reshard_after_forward=False\n",
    "                )\n",
    "            else:\n",
    "                self.values_u = fully_shard(\n",
    "                    self.values_u, **fsdp_config, reshard_after_forward=False\n",
    "                )\n",
    "                self.values_v = fully_shard(\n",
    "                    self.values_v, **fsdp_config, reshard_after_forward=False\n",
    "                )\n",
    "        if self.mem_share_values and self.original:\n",
    "            if not self.use_peer_variant:\n",
    "                HashingMemory.VALUES = self.values\n",
    "            else:\n",
    "                HashingMemory.VALUES = self.values_u, self.values_v\n",
    "        if self.mem_share_values and not self.original:\n",
    "            if not self.use_peer_variant:\n",
    "                self.values = HashingMemory.VALUES\n",
    "            else:\n",
    "                self.values_u, self.values_v = HashingMemory.VALUES\n",
    "\n",
    "    def reset_parameters(self, init_std=None, factor=1.0):\n",
    "        # keys\n",
    "        bound = 1 / math.sqrt(self.k_dim)\n",
    "        nn.init.uniform_(self.keys, a=-bound, b=bound)\n",
    "        \n",
    "        # values\n",
    "        if self.original:\n",
    "            if not self.use_peer_variant:\n",
    "                nn.init.normal_(self.values.weight, mean=0, std=self.v_dim**-0.5)\n",
    "            else:\n",
    "                nn.init.normal_(self.values_u.weight, mean=0, std=self.v_dim**-0.5)\n",
    "                nn.init.normal_(self.values_v.weight, mean=0, std=self.v_dim**-0.5)\n",
    "                \n",
    "        # queries\n",
    "        nn.init.xavier_uniform_(self.query_proj.query_mlps[0].weight)\n",
    "        \n",
    "        # value projection\n",
    "        if self.v_proj:\n",
    "            nn.init.normal_(self.value_proj.weight, mean=0, std=self.output_dim**-0.5)\n",
    "        if self.swilu_proj:\n",
    "            nn.init.normal_(\n",
    "                self.swilu_projection.weight, mean=0, std=self.output_dim**-0.5\n",
    "            )\n",
    "            \n",
    "        # fixed learning rate:\n",
    "        if self.original:\n",
    "            if self.use_peer_variant:\n",
    "                for p in self.values_u.parameters():\n",
    "                    p.fixed_lr = self.value_fixed_lr\n",
    "                    p.pk_value_param = True\n",
    "                for p in self.values_v.parameters():\n",
    "                    p.fixed_lr = self.value_fixed_lr\n",
    "                    p.pk_value_param = True\n",
    "            else:\n",
    "                for p in self.values.parameters():\n",
    "                    p.fixed_lr = self.value_fixed_lr\n",
    "                    p.pk_value_param = True\n",
    "                    \n",
    "        if self.gating is not None:\n",
    "            nn.init.normal_(self.gating.weight, mean=0, std=self.input_dim**-0.5)\n",
    "\n",
    "    def get_indices(self, query, knn):\n",
    "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
    "        bs = len(query) // self.num_heads\n",
    "        query = query.view(-1, self.num_heads, self.k_dim)\n",
    "        half = self.k_dim // 2\n",
    "        \n",
    "        # keys : (heads, 2, n_keys, half)\n",
    "        # keys1 : (heads, n_keys, half)\n",
    "        keys = self.keys.view(self.num_heads, 2, -1, half)\n",
    "        keys1 = keys[:, 0, :, :]\n",
    "        keys2 = keys[:, 1, :, :]\n",
    "        n_keys = len(keys[0][0])\n",
    "\n",
    "        # split query for product quantization\n",
    "        q1 = query[:, :, :half]  # (bs, heads, half)\n",
    "        q2 = query[:, :, half:]  # (bs, heads, half)\n",
    "\n",
    "        # compute indices with associated scores\n",
    "        scores1 = torch.einsum(\n",
    "            \"blh, lkh->blk\", q1, keys1\n",
    "        )  # (bs , heads, n_keys ** 0.5)\n",
    "        scores2 = torch.einsum(\n",
    "            \"blh, lkh->blk\", q2, keys2\n",
    "        )  # (bs , heads, n_keys ** 0.5)\n",
    "\n",
    "        scores1, indices1 = scores1.topk(knn, dim=2, largest=True)  # (bs, heads, knn)\n",
    "        scores2, indices2 = scores2.topk(knn, dim=2, largest=True)  # (bs, heads, knn)\n",
    "\n",
    "        # cartesian product on best candidate keys\n",
    "        all_scores = (\n",
    "            scores1.view(bs, self.num_heads, knn, 1).expand(bs, self.num_heads, knn, knn)\n",
    "            + scores2.view(bs, self.num_heads, 1, knn).expand(bs, self.num_heads, knn, knn)\n",
    "        ).view(\n",
    "            bs, self.num_heads, -1\n",
    "        )  # (bs, heads, knn ** 2)\n",
    "        all_indices = (\n",
    "            indices1.view(bs, self.num_heads, knn, 1).expand(bs, self.num_heads, knn, knn)\n",
    "            * n_keys\n",
    "            + indices2.view(bs, self.num_heads, 1, knn).expand(bs, self.num_heads, knn, knn)\n",
    "        ).view(\n",
    "            bs, self.num_heads, -1\n",
    "        )  # (bs, heads, knn ** 2)\n",
    "\n",
    "        # select overall best scores and indices\n",
    "        scores, best_indices = torch.topk(\n",
    "            all_scores, k=knn, dim=2, largest=True, sorted=True\n",
    "        )  # (bs, heads, knn)\n",
    "        indices = all_indices.gather(2, best_indices)  # (bs, heads, knn)\n",
    "\n",
    "        # return scores with indices\n",
    "        assert scores.shape == indices.shape == (bs, self.num_heads, knn)\n",
    "        return scores.view(bs * self.num_heads, knn), indices.view(bs * self.num_heads, knn)\n",
    "\n",
    "    def get_scores(self, query, indices):\n",
    "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
    "        bs = len(query) // self.num_heads\n",
    "        query = query.view(-1, self.num_heads, self.k_dim)\n",
    "        half = self.k_dim // 2\n",
    "        \n",
    "        # keys : (heads, 2, n_keys, half)\n",
    "        # keys1 : (heads, n_keys, half)\n",
    "        keys = self.keys.view(self.num_heads, 2, -1, half)\n",
    "        keys1 = keys[:, 0, :, :]\n",
    "        keys2 = keys[:, 1, :, :]\n",
    "        n_keys = len(keys[0][0])\n",
    "\n",
    "        # split query for product quantization\n",
    "        q1 = query[:, :, :half]  # (bs, heads, half)\n",
    "        q2 = query[:, :, half:]  # (bs, heads, half)\n",
    "\n",
    "        # compute indices with associated scores\n",
    "        scores1 = torch.einsum(\n",
    "            \"blh, lkh->blk\", q1, keys1\n",
    "        )  # (bs , heads, num_keys ** 2)\n",
    "        scores2 = torch.einsum(\n",
    "            \"blh, lkh->blk\", q2, keys2\n",
    "        )  # (bs , heads, num_keys ** 2)\n",
    "\n",
    "        all_scores = (\n",
    "            scores1.view(bs, self.num_heads, self.num_keys, 1).expand(bs, self.num_heads, self.num_keys, self.num_keys)\n",
    "            + scores2.view(bs, self.num_heads, 1, self.num_keys).expand(bs, self.num_heads, self.num_keys, self.num_keys)\n",
    "        ).view(\n",
    "            bs, self.num_heads, -1\n",
    "        )  # (bs, heads, num_keys ** 2)\n",
    "\n",
    "        num_items = indices.size(1)\n",
    "        indices = indices.unsqueeze(1).expand(-1, all_scores.size(1), -1)\n",
    "        scores = all_scores.gather(dim=2, index=indices)\n",
    "\n",
    "        assert scores.shape == indices.shape == (bs, self.num_heads, num_items)\n",
    "        return scores.view(bs * self.num_heads, num_items), indices.contiguous().view(bs * self.num_heads, num_items)\n",
    "        \n",
    "    def forward(self, x, input_indices=None, use_self_linker=True, return_scores=False):\n",
    "        \"\"\"\n",
    "        Read from the memory.\n",
    "        \"\"\"\n",
    "        B, C = x.shape\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        assert x.shape[-1] == self.input_dim\n",
    "        prefix_shape = x.shape[:-1]\n",
    "\n",
    "        bs = np.prod(prefix_shape)\n",
    "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
    "        query = self.query_proj(x)  # (bs * heads, k_dim)\n",
    "        query = F.dropout(query, p=self.query_dropout, training=self.training)  # (bs * heads, k_dim)\n",
    "        assert query.shape == (bs * self.num_heads, self.k_dim)\n",
    "\n",
    "        scores, indices = None, None\n",
    "        \n",
    "        # get scores\n",
    "        if input_indices is not None: \n",
    "            scores, indices = self.get_scores(query, input_indices) # (bs * heads, num_items)\n",
    "        \n",
    "        # get indices\n",
    "        if use_self_linker:\n",
    "            sc, idx = self.get_indices(query, self.num_knn)  # (bs * heads, knn)\n",
    "            scores = sc if scores is None else torch.cat([scores, sc], dim=1) # (bs * heads, knn + num_items)\n",
    "            indices = idx if indices is None else torch.cat([indices, idx], dim=1) # (bs * heads, knn + num_items)\n",
    "\n",
    "        num_items = indices.size(1)\n",
    "        \n",
    "        # store indices / scores (eval mode only - for usage statistics)\n",
    "        if not self.training and HashingMemory.EVAL_MEMORY:\n",
    "            self.last_indices = indices.view(bs, self.num_heads, num_items).detach().cpu()\n",
    "            self.last_scores = scores.view(bs, self.num_heads, num_items).detach().cpu().float()\n",
    "\n",
    "        # re-scoring\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(scores)  # (bs * heads, num_items)\n",
    "\n",
    "        # merge heads / knn (since we sum heads)\n",
    "        indices = indices.view(bs, self.num_heads * num_items)  # (bs, heads * num_items)\n",
    "        scores = scores.view(bs, self.num_heads * num_items)  # (bs, heads * num_items)\n",
    "\n",
    "        if not self.use_peer_variant:\n",
    "            output = self.values(indices, scores)  # (bs, v_dim)\n",
    "            if self.v_proj and not self.swilu_proj:\n",
    "                output = self.value_proj(output)\n",
    "            if self.swilu_proj:\n",
    "                output = self.value_proj(output * F.silu(self.swilu_projection(input)))\n",
    "        else:\n",
    "            u = self.values_u(indices)\n",
    "            x = torch.einsum(\n",
    "                \"bh, blh->bl\", x, u\n",
    "            )  # (bs, v_dim) , (bs, heads * knn, v_dim) -> (bs, heads * knn)\n",
    "            x = F.gelu(x)  # This can be either GeLU or ReLU\n",
    "            v = self.values_v(indices)\n",
    "            x = x * scores  # (bs, heads * num_items)\n",
    "            output = torch.einsum(\n",
    "                \"bl, blh->bh\", x, v\n",
    "            )  # (bs, heads * num_items) , (bs, heads * num_items, v_dim) -> (bs, v_dim)\n",
    "\n",
    "        output = F.dropout(\n",
    "            output, p=self.value_dropout, training=self.training\n",
    "        )  # (bs, v_dim)\n",
    "\n",
    "        # reshape output\n",
    "        if len(prefix_shape) >= 2:\n",
    "            output = output.view(prefix_shape + (self.v_dim,))  # (..., v_dim)\n",
    "\n",
    "        if self.gating:\n",
    "            output = F.sigmoid(self.gating(input)) * output\n",
    "        output = output.view(B, C)\n",
    "        \n",
    "        return (output, scores, indices) if return_scores else output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f525b5-b348-4536-99a6-9241010d2129",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8db4db02-cec6-42bd-b474-5be0bdec81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xcai.models.cachew import BaseMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5d90e048-6613-42fa-88d9-45d5e5a648f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_keys = int(np.ceil(np.sqrt(block.train.dset.meta['cat_meta'].n_meta))); num_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e42141c3-b716-4d62-8a5a-91789fe021b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = HashingMemory(768, 768, num_keys=810, share_values=False, use_peer_variant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d62ab98-5a36-4062-9b62-a7e2500eba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_indices = BaseMemory.align_indices(batch['cat2data_idx'], batch['cat2data_data2ptr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f67f77ae-68cd-4426-bd4c-026e82a29605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = m(o, input_indices=input_indices, use_self_linker=True, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "295eefe7-a96e-40d2-885c-c98f2275fe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c67b5c-73d3-43fa-9e0b-51f6c7cb623e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a577806-d46e-46fc-be00-77646e8c4012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
