{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1071a98f-579b-47a9-8a19-e5d5ddd21282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.NVM0XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42e7ecc-b859-4656-9dcd-190548be8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c3e72c-aed5-4f0d-b6fa-865bc5df5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, re, inspect, pickle, os, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Mapping, Any, Union\n",
    "\n",
    "from transformers.activations import get_activation\n",
    "\n",
    "from fastcore.meta import *\n",
    "\n",
    "from xcai.core import store_attr\n",
    "from xcai.losses import MultiTriplet\n",
    "from xcai.models.modeling_nvembed import NVEmbedModel\n",
    "from xcai.models.modeling_utils import XCModelOutput, Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c515cb0-d9a4-414b-b755-eedf8231d1ea",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6f5abb-8e05-4390-9ac7-b0089260d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a56ec34-4246-4d19-9ea2-7eb03cccbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_nv-embed-v2_xcs.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b90b963-fa32-4db7-b2ba-b20948e57624",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10915572-9f1f-43a2-bf1e-42d430c1d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = block.train.one_batch(10)\n",
    "for i,batch in enumerate(block.train.dl):\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc4cada-39f1-4251-8259-a09548f53688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plbl2data_idx', 'plbl2data_data2ptr', 'lbl2data_idx', 'lbl2data_identifier', 'lbl2data_input_text', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'lbl2data_data2ptr', 'data_identifier', 'data_input_text', 'data_input_ids', 'data_attention_mask', 'data_idx'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a11d2-0009-488b-8806-63351697f55a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4d6682-5399-46f8-833e-67b27f89b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pooling:\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(data_embeds:torch.FloatTensor, data_attention_mask:torch.LongTensor):\n",
    "        data_attention_mask = data_attention_mask.unsqueeze(2).expand(data_embeds.size()).float()\n",
    "        return torch.sum(data_embeds * data_attention_mask, 1) / torch.clamp(data_attention_mask.sum(1), min=1e-9)\n",
    "\n",
    "    @staticmethod\n",
    "    def last_pooling(data_embeds:torch.FloatTensor, data_attention_mask:torch.LongTensor):\n",
    "        index = data_attention_mask.sum(dim=1) - 1\n",
    "        return data_embeds[torch.arange(data_embeds.size(0), device=data_embeds.device), index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1b5b58-2ee9-49ea-a2e8-9250d2b08c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RepresentationHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.projector = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = get_activation('relu')\n",
    "        \n",
    "        self.post_init()\n",
    "        \n",
    "    def post_init(self):\n",
    "        torch.nn.init.eye_(self.transform.weight)\n",
    "        torch.nn.init.eye_(self.projector.weight)\n",
    "        \n",
    "        torch.nn.init.zeros_(self.transform.bias)\n",
    "        torch.nn.init.zeros_(self.projector.bias)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.transform(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6394a-4e32-4d8b-9003-2f87083e337b",
   "metadata": {},
   "source": [
    "## `NVM009`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32298157-72b2-4a86-a77a-ea3f3215f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NVM009Encoder(NVEmbedModel):\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config)\n",
    "        self.dr_head = RepresentationHead(config)\n",
    "        \n",
    "    @delegates(NVEmbedModel.__call__)\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids:Optional[torch.Tensor]=None, \n",
    "        attention_mask:Optional[torch.Tensor]=None,\n",
    "        pool_mask: Optional[torch.Tensor]=None,\n",
    "        return_dict: bool=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        outputs = self.embedding_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        embeds = self.latent_attention_model(\n",
    "            outputs.last_hidden_state,\n",
    "            pool_mask,\n",
    "        )\n",
    "        rep = self.dr_head(embeds)\n",
    "        \n",
    "        return outputs, F.normalize(Pooling.mean_pooling(rep, attention_mask), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4dfbe3-af1f-4a7b-92e2-95aa4bde1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NVM009(NVEmbedModel):\n",
    "    use_generation,use_representation = False,True\n",
    "    _tied_weights_keys = [\"encoder.embedding_model,encoder.latent_attention_model\"]\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 bsz:Optional[int]=None,\n",
    "                 tn_targ:Optional[int]=None,\n",
    "                 margin:Optional[float]=0.3,\n",
    "                 tau:Optional[float]=0.1,\n",
    "                 apply_softmax:Optional[bool]=False,\n",
    "                 n_negatives:Optional[int]=5,\n",
    "                 use_encoder_parallel:Optional[bool]=True,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        store_attr('use_encoder_parallel')\n",
    "        self.encoder = NVM009Encoder(config)\n",
    "        self.loss_fn = MultiTriplet(bsz=bsz, tn_targ=tn_targ, margin=margin, n_negatives=n_negatives, tau=tau, \n",
    "                                    apply_softmax=apply_softmax, reduce='mean')\n",
    "        self.post_init()\n",
    "        self.remap_post_init()\n",
    "        \n",
    "    def init_dr_head(self):\n",
    "        self.encoder.dr_head.post_init()\n",
    "        \n",
    "    def remap_post_init(self):\n",
    "        self.embedding_model = self.encoder.embedding_model\n",
    "        self.latent_attention_model = self.encoder.latent_attention_model\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        data_input_ids:Optional[torch.Tensor]=None,\n",
    "        data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        lbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        lbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        lbl2data_input_ids:Optional[torch.Tensor]=None,\n",
    "        lbl2data_attention_mask:Optional[torch.Tensor]=None,\n",
    "        plbl2data_data2ptr:Optional[torch.Tensor]=None,\n",
    "        plbl2data_idx:Optional[torch.Tensor]=None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if self.use_encoder_parallel: \n",
    "            encoder = nn.DataParallel(module=self.encoder)\n",
    "        else: encoder = self.encoder\n",
    "        \n",
    "        data_o, data_repr = encoder(data_input_ids, data_attention_mask)\n",
    "        \n",
    "        loss, lbl2data_repr = None, None\n",
    "        if lbl2data_input_ids is not None:\n",
    "            lbl2data_o, lbl2data_repr = encoder(lbl2data_input_ids, lbl2data_attention_mask)\n",
    "            \n",
    "            loss = self.loss_fn(data_repr, lbl2data_repr, lbl2data_data2ptr, lbl2data_idx, \n",
    "                                plbl2data_data2ptr, plbl2data_idx, **kwargs)\n",
    "\n",
    "        if not return_dict:\n",
    "            o = (data_repr, lbl2data_repr)\n",
    "            return ((loss,) + o) if loss is not None else o\n",
    "\n",
    "        return XCModelOutput(\n",
    "            loss=loss,\n",
    "            data_repr=data_repr,\n",
    "            lbl2data_repr=lbl2data_repr,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ea8b2-9c4c-4d68-b273-76e00d7c1e54",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5cb93b-87f1-41b5-9218-8a618d14a64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36abb2b63a947578d72e415c15806cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of NVM009 were not initialized from the model checkpoint at nvidia/NV-Embed-v2 and are newly initialized: ['encoder.dr_head.layer_norm.bias', 'encoder.dr_head.layer_norm.weight', 'encoder.dr_head.projector.bias', 'encoder.dr_head.projector.weight', 'encoder.dr_head.transform.bias', 'encoder.dr_head.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = NVM009.from_pretrained('nvidia/NV-Embed-v2', bsz=1024, margin=0.3, tau=0.1, n_negatives=10, apply_softmax=True, \n",
    "                               use_encoder_parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f253e3-8812-46fc-b25e-8ac5408444ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_dr_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e255ec4e-2c67-43e9-b468-269ac3fb7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg_2/lib/python3.9/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "/scratch/scai/phd/aiz218323/Projects/xcai/xcai/losses.py:22: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(data_ptr, data_idx, scores, device=data_ptr.device)\n"
     ]
    }
   ],
   "source": [
    "o = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407385b3-7fdc-4de2-8603-fb85407536f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0271, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1810c9c-8480-49b5-a250-54a55dd5eeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68a9fa3e-ee65-43cc-a800-87a2c1c83d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ae497c-fa97-40ba-af00-91479bf2677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e38f604f-3d55-45c3-988d-f316c4b1c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0e7658e-76f1-4778-b0f4-038ffc3ee89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg_2/lib/python3.9/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "o = m(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7577a2bf-e86a-4fab-856c-5eace19df9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0271, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588271a3-bb43-469e-bd8a-fb7fd5c41503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e86045f0-936b-48e6-a350-6bae35545f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.embedding_model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.embedding_model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for n,p in m.named_parameters():\n",
    "    if p.requires_grad: print(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8695e40-b0ff-4faa-b35b-823cb31754e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
