{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91550c-45ce-484e-8c9e-1925bed97ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp torch_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8e8a9-3384-46e7-802a-f5b6d3b3beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51865b1b-1c83-47a0-92b5-7e6bbf39bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math, torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e807878-5f2e-4bab-b18b-ee879c13b17e",
   "metadata": {},
   "source": [
    "## CUDALongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecab9b4-2d21-4e27-a972-9e4c5983effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CUDALongTensor:\n",
    "    \"\"\"\n",
    "    A wrapper class for `torch.cuda.LongTensor`. When performing operations that are\n",
    "    currently not supported for `torch.cuda.LongTensor` (e.g `matmul`), it will\n",
    "    convert the underlying LongTensor into DoubleTensor and convert the computed\n",
    "    result back to a LongTensor. The computed result will be the same as the original\n",
    "    expected result.\n",
    "    \"\"\"\n",
    "\n",
    "    __BITS = torch.iinfo(torch.long).bits\n",
    "    __DEFAULT_NBLOCKS = 3\n",
    "    __BLOCK_SIZE = {3: None, 4: None}  # Number of bits per block\n",
    "    __INDICES = {3: [], 4: []}\n",
    "    __SHIFTS = {3: [], 4: []}\n",
    "    for nblocks in [3, 4]:\n",
    "        __BLOCK_SIZE[nblocks] = math.ceil(__BITS / nblocks)\n",
    "        for i in range(nblocks):\n",
    "            for j in range(nblocks):\n",
    "                if (i + j) * __BLOCK_SIZE[nblocks] >= __BITS:\n",
    "                    continue\n",
    "                idx = i * nblocks + j\n",
    "                __INDICES[nblocks].append(idx)\n",
    "                __SHIFTS[nblocks].append((i + j) * __BLOCK_SIZE[nblocks])\n",
    "                \n",
    "    @staticmethod\n",
    "    def __encode_as_fp64(x, num_blocks=3):\n",
    "        \"\"\"Converts a CUDALongTensor to an encoding of\n",
    "        torch.cuda.DoubleTensor that represent the same data.\n",
    "        \"\"\"\n",
    "        nb = num_blocks\n",
    "        bks = CUDALongTensor.__BLOCK_SIZE[num_blocks]\n",
    "\n",
    "        x_block = torch.stack(\n",
    "            [(x >> (bks * i)) & (2**bks - 1) for i in range(nb)]\n",
    "        )\n",
    "\n",
    "        return x_block.double()\n",
    "\n",
    "    @staticmethod\n",
    "    def __decode_as_int64(x, num_blocks=3):\n",
    "        \"\"\"Converts a CUDALongTensor encoded as torch.cuda.DoubleTensor\n",
    "        back to the CUDALongTensor it encodes\n",
    "        \"\"\"\n",
    "        x = x.long()\n",
    "\n",
    "        indices = CUDALongTensor.__INDICES[num_blocks]\n",
    "        shifts = CUDALongTensor.__SHIFTS[num_blocks]\n",
    "\n",
    "        indices = torch.tensor(indices, device=x.device)\n",
    "        shifts = torch.tensor(shifts, device=x.device)\n",
    "        shifts = shifts.view(-1, *([1] * (x.ndim - 1)))\n",
    "\n",
    "        result = torch.index_select(x, 0, indices)\n",
    "        result <<= shifts\n",
    "\n",
    "        return result.sum(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(x, y, *args, **kwargs):\n",
    "        if not x.is_cuda or not y.is_cuda: return x@y\n",
    "        \n",
    "        # Use 4 blocks if each dot product is 256 elements or larger to prevent overflow in the sum\n",
    "        nb = 3 if x.size(-1) < 256 else 4\n",
    "\n",
    "        # Prepend 1 to the dimension of x or y if it is 1-dimensional\n",
    "        remove_x, remove_y = False, False\n",
    "        if x.dim() == 1:\n",
    "            x = x.view(1, x.shape[0])\n",
    "            remove_x = True\n",
    "        if y.dim() == 1:\n",
    "            y = y.view(y.shape[0], 1)\n",
    "            remove_y = True\n",
    "\n",
    "        x_encoded = CUDALongTensor.__encode_as_fp64(x, nb)\n",
    "        y_encoded = CUDALongTensor.__encode_as_fp64(y, nb)\n",
    "\n",
    "        # Span x and y for cross multiplication\n",
    "        repeat_idx = [1] * (x_encoded.dim() - 1)\n",
    "        x_enc_span = x_encoded.repeat(nb, *repeat_idx)\n",
    "        y_enc_span = torch.repeat_interleave(y_encoded, repeats=nb, dim=0)\n",
    "\n",
    "        # Broadcasting\n",
    "        for _ in range(abs(x_enc_span.ndim - y_enc_span.ndim)):\n",
    "            if x_enc_span.ndim > y_enc_span.ndim:\n",
    "                y_enc_span.unsqueeze_(1)\n",
    "            else:\n",
    "                x_enc_span.unsqueeze_(1)\n",
    "\n",
    "        z_encoded = torch.matmul(x_enc_span, y_enc_span, *args, **kwargs)\n",
    "\n",
    "        if remove_x:\n",
    "            z_encoded.squeeze_(-2)\n",
    "        if remove_y:\n",
    "            z_encoded.squeeze_(-1)\n",
    "\n",
    "        return CUDALongTensor.__decode_as_int64(z_encoded, nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e5f8c-683e-4aed-ad1f-c40b2c63be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.ones(4, 4, dtype=torch.int64).triu()\n",
    "v = torch.randint(1, 10, size=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5dd3b-7d61-4f81-aac3-17a3dd425f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5, 14, 15]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDALongTensor.matmul(v, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2705aef-67c3-4474-a4af-a802e59fdbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5, 14, 15]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,v = u.to('cuda'), v.to('cuda')\n",
    "CUDALongTensor.matmul(v, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869e1ca-2a5e-4e13-8333-9ba1247e595e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
